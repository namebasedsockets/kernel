diff --git a/.gitignore b/.gitignore
index 869e1a3..1a7e610 100644
--- a/.gitignore
+++ b/.gitignore
@@ -25,6 +25,8 @@
 *.elf
 *.bin
 *.gz
+*~
+\#*#
 
 #
 # Top-level generic files
@@ -37,6 +39,8 @@ Module.markers
 Module.symvers
 !.gitignore
 !.mailmap
+/debian
+/build-um
 
 #
 # Generated include files
diff --git a/Makefile b/Makefile
index 71e98e9..b6297db 100644
--- a/Makefile
+++ b/Makefile
@@ -341,6 +341,7 @@ KBUILD_CPPFLAGS := -D__KERNEL__ $(LINUXINCLUDE)
 KBUILD_CFLAGS   := -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs \
 		   -fno-strict-aliasing -fno-common \
 		   -Werror-implicit-function-declaration
+
 KBUILD_AFLAGS   := -D__ASSEMBLY__
 
 # Read KERNELRELEASE from include/config/kernel.release (if it exists)
diff --git a/arch/um/.gitignore b/arch/um/.gitignore
new file mode 100644
index 0000000..c9219c7
--- /dev/null
+++ b/arch/um/.gitignore
@@ -0,0 +1,9 @@
+#Symlinks used by uml
+/Kconfig.arch
+/include/kern_constants.h
+/include/sysdep
+/include/uml-config.h
+/include/user_constants.h
+/kernel/config.c
+/kernel/config.tmp
+/os
diff --git a/arch/um/drivers/mconsole_kern.c b/arch/um/drivers/mconsole_kern.c
index 8f44ebb..b9c8573 100644
--- a/arch/um/drivers/mconsole_kern.c
+++ b/arch/um/drivers/mconsole_kern.c
@@ -793,7 +793,7 @@ static int __init mconsole_init(void)
 		return -1;
 	snprintf(mconsole_socket_name, sizeof(file), "%s", file);
 
-	sock = os_create_unix_socket(file, sizeof(file), 1);
+	sock = os_create_unix_socket(file, strlen(file), 1);
 	if (sock < 0) {
 		printk(KERN_ERR "Failed to initialize management console\n");
 		return 1;
diff --git a/arch/um/os-Linux/file.c b/arch/um/os-Linux/file.c
index b5afcfd..09ee348 100644
--- a/arch/um/os-Linux/file.c
+++ b/arch/um/os-Linux/file.c
@@ -521,7 +521,7 @@ int os_create_unix_socket(const char *file, int len, int close_on_exec)
 
 	addr.sun_family = AF_UNIX;
 
-	snprintf(addr.sun_path, len, "%s", file);
+	snprintf(addr.sun_path, sizeof(addr.sun_path), "%s", file);
 
 	err = bind(sock, (struct sockaddr *) &addr, sizeof(addr));
 	if (err < 0)
diff --git a/arch/um/os-Linux/mem.c b/arch/um/os-Linux/mem.c
index 93a11d7..e696144 100644
--- a/arch/um/os-Linux/mem.c
+++ b/arch/um/os-Linux/mem.c
@@ -10,6 +10,7 @@
 #include <errno.h>
 #include <fcntl.h>
 #include <string.h>
+#include <sys/stat.h>
 #include <sys/mman.h>
 #include <sys/param.h>
 #include "init.h"
diff --git a/arch/um/os-Linux/user_syms.c b/arch/um/os-Linux/user_syms.c
index 74f49bb..89b48a1 100644
--- a/arch/um/os-Linux/user_syms.c
+++ b/arch/um/os-Linux/user_syms.c
@@ -14,7 +14,6 @@
 #undef memset
 
 extern size_t strlen(const char *);
-extern void *memcpy(void *, const void *, size_t);
 extern void *memmove(void *, const void *, size_t);
 extern void *memset(void *, int, size_t);
 extern int printf(const char *, ...);
@@ -24,7 +23,11 @@ extern int printf(const char *, ...);
 EXPORT_SYMBOL(strstr);
 #endif
 
+#ifndef __x86_64__
+extern void *memcpy(void *, const void *, size_t);
 EXPORT_SYMBOL(memcpy);
+#endif
+
 EXPORT_SYMBOL(memmove);
 EXPORT_SYMBOL(memset);
 EXPORT_SYMBOL(printf);
diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
index 12c07c1..ada42b4 100644
--- a/include/asm-generic/bug.h
+++ b/include/asm-generic/bug.h
@@ -21,10 +21,10 @@ struct bug_entry {
 #endif	/* CONFIG_GENERIC_BUG */
 
 #ifndef HAVE_ARCH_BUG
-#define BUG() do { \
-	printk("BUG: failure at %s:%d/%s()!\n", __FILE__, __LINE__, __func__); \
-	panic("BUG!"); \
-} while (0)
+#define BUG() do {							\
+		printk(KERN_ERR "BUG: failure at %s:%d/%s()!\n", __FILE__, __LINE__, __FUNCTION__); \
+		panic("BUG!");						\
+	} while (0)
 #endif
 
 #ifndef HAVE_ARCH_BUG_ON
diff --git a/include/asm-um/.gitignore b/include/asm-um/.gitignore
new file mode 100644
index 0000000..0d795ef
--- /dev/null
+++ b/include/asm-um/.gitignore
@@ -0,0 +1,11 @@
+#symlinks used by uml arch
+/arch
+/archparam.h
+/elf.h
+/host_ldt.h
+/module.h
+/processor.h
+/ptrace.h
+/sigcontext.h
+/system.h
+/vm-flags.h
diff --git a/include/linux/aio.h b/include/linux/aio.h
index f6b8cf9..82df5df 100644
--- a/include/linux/aio.h
+++ b/include/linux/aio.h
@@ -5,6 +5,7 @@
 #include <linux/workqueue.h>
 #include <linux/aio_abi.h>
 #include <linux/uio.h>
+#include <linux/wait.h>
 
 #include <asm/atomic.h>
 
diff --git a/include/linux/in.h b/include/linux/in.h
index db458be..43b3fb2 100644
--- a/include/linux/in.h
+++ b/include/linux/in.h
@@ -28,6 +28,7 @@ enum {
   IPPROTO_IGMP = 2,		/* Internet Group Management Protocol	*/
   IPPROTO_IPIP = 4,		/* IPIP tunnels (older KA9Q tunnels use 94) */
   IPPROTO_TCP = 6,		/* Transmission Control Protocol	*/
+  IPPROTO_MTCPSUB = 7,          /* MTCP subflow                         */
   IPPROTO_EGP = 8,		/* Exterior Gateway Protocol		*/
   IPPROTO_PUP = 12,		/* PUP protocol				*/
   IPPROTO_UDP = 17,		/* User Datagram Protocol		*/
diff --git a/include/linux/list.h b/include/linux/list.h
index 969f6e9..6d3a9df 100644
--- a/include/linux/list.h
+++ b/include/linux/list.h
@@ -49,8 +49,8 @@ static inline void __list_add(struct list_head *new,
 }
 #else
 extern void __list_add(struct list_head *new,
-			      struct list_head *prev,
-			      struct list_head *next);
+		       struct list_head *prev,
+		       struct list_head *next);
 #endif
 
 /**
diff --git a/include/linux/netlink.h b/include/linux/netlink.h
index 9ff1b54..0ef5397 100644
--- a/include/linux/netlink.h
+++ b/include/linux/netlink.h
@@ -24,6 +24,7 @@
 /* leave room for NETLINK_DM (DM Events) */
 #define NETLINK_SCSITRANSPORT	18	/* SCSI Transports */
 #define NETLINK_ECRYPTFS	19
+#define NETLINK_PM              21      /* Communication with user space PM */
 
 #define MAX_LINKS 32		
 
diff --git a/include/linux/pm_netlink.h b/include/linux/pm_netlink.h
new file mode 100644
index 0000000..a4a4ff9
--- /dev/null
+++ b/include/linux/pm_netlink.h
@@ -0,0 +1,77 @@
+/*
+ *	Path Manager implementation - netlink communication with user space.
+ *
+ *	Author:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *
+ *      Support for the Generic Multipath Architecture (GMA).
+ *
+ *      date : June 2009
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#ifndef _LINUX_PM_NETLINK_H
+#define _LINUX_PM_NETLINK_H
+
+#ifndef __KERNEL__
+#include <sys/types.h>
+#include <netinet/in.h>
+#else
+#include <linux/in6.h>
+#include <asm/byteorder.h>
+#include <asm/types.h>
+#include <net/if_inet6.h>
+#endif /*__KERNEL__*/
+
+#include <linux/netlink.h>
+
+#define MAX_NL_PAYLOAD 50 
+
+/*The current version of libc has an old version of linux/netlink.h
+  with this field undefined.*/
+
+#ifndef NLMSG_MIN_TYPE
+#define NLMSG_MIN_TYPE 0x10
+#endif
+
+
+/*PM Multicast groups, currently only one.*/
+enum pm_nlgroups {
+	PMNLGRP_NONE,
+	PMNLGRP_DEFAULT,
+	__PMNLGRP_MAX
+};
+#define PMNLGRP_MAX (__PMNLGRP_MAX-1)
+
+
+enum pm_nl_type_t {	
+	PM_NL_PATHUPDATE = 1+NLMSG_MIN_TYPE, /*Announce new paths indices*/
+};
+
+/*Data part of the Netlink message for Netlink code PM_NL_PATHUPDATE
+  For the moment this is only supported for IPv6 */
+struct nl_ulid_pair {
+	struct in6_addr local;
+	struct in6_addr remote;
+	uint32_t        path_indices; /*bitmap of paths that can be used
+					 For example, if bit 3 is set, then
+					 3 is currently a valid path index
+					 that can be understood by a Path 
+					 Manager*/
+};
+
+#ifdef __KERNEL__ /*definitions for kernel space*/
+
+#include <net/netlink.h>
+
+extern struct sock *pmnl_sk;
+
+#endif /*__KERNEL__*/
+
+#endif /*_LINUX_PM_NETLINK_H*/
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 2725f4e..8d31e27 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -270,6 +270,11 @@ struct sk_buff {
 		struct  rtable		*rtable;
 	};
 	struct	sec_path	*sp;
+	unsigned int            path_index; /*Path index for multipath control*/
+	__u32                   path_mask; /*Mask of the path indices that
+					     have tried to send this skb*/
+	int                     count_dsn; /*TODEL*/
+	__u32                   data_seq; /*TODEL*/
 
 	/*
 	 * This is the control buffer. It is free to use for every
@@ -277,8 +282,7 @@ struct sk_buff {
 	 * want to keep them across layers you have to do a skb_clone()
 	 * first. This is owned by whoever has the skb queued ATM.
 	 */
-	char			cb[48];
-
+	char			cb[64];
 	unsigned int		len,
 				data_len;
 	__u16			mac_len,
diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index fe77e14..a916790 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -21,6 +21,8 @@
 #include <asm/byteorder.h>
 #include <linux/socket.h>
 
+struct multipath_pcb;
+
 struct tcphdr {
 	__be16	source;
 	__be16	dest;
@@ -177,6 +179,7 @@ struct tcp_md5sig {
 #include <net/sock.h>
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
+#include <linux/tcp_options.h>
 
 static inline struct tcphdr *tcp_hdr(const struct sk_buff *skb)
 {
@@ -204,26 +207,6 @@ struct tcp_sack_block {
 	u32	end_seq;
 };
 
-struct tcp_options_received {
-/*	PAWS/RTTM data	*/
-	long	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
-	u32	ts_recent;	/* Time stamp to echo next		*/
-	u32	rcv_tsval;	/* Time stamp value             	*/
-	u32	rcv_tsecr;	/* Time stamp echo reply        	*/
-	u16 	saw_tstamp : 1,	/* Saw TIMESTAMP on last packet		*/
-		tstamp_ok : 1,	/* TIMESTAMP seen on SYN packet		*/
-		dsack : 1,	/* D-SACK is scheduled			*/
-		wscale_ok : 1,	/* Wscale seen on SYN packet		*/
-		sack_ok : 4,	/* SACK seen on SYN packet		*/
-		snd_wscale : 4,	/* Window scaling received from sender	*/
-		rcv_wscale : 4;	/* Window scaling to send to receiver	*/
-/*	SACKs data	*/
-	u8	eff_sacks;	/* Size of SACK array to send with next packet */
-	u8	num_sacks;	/* Number of SACK blocks		*/
-	u16	user_mss;  	/* mss requested by user in ioctl */
-	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
-};
-
 /* This is the max number of SACKS that we'll generate and process. It's safe
  * to increse this, although since:
  *   size = TCPOLEN_SACK_BASE_ALIGNED (4) + n * TCPOLEN_SACK_PERBLOCK (8)
@@ -264,6 +247,55 @@ struct tcp_sock {
  */
  	u32	rcv_nxt;	/* What we want to receive next 	*/
 	u32	copied_seq;	/* Head of yet unread data		*/
+#ifdef CONFIG_MTCP
+	/*data for the scheduler*/
+	struct {
+		int	space;
+		u32	seq;
+		u32	time;
+		short   shift; /*Shift to apply to the space field. 
+				 It is increased when space bytes are
+				 flushed in less than a jiffie (can happen 
+				 with gigabit ethernet), so as to use a larger
+				 basis for bw computation.*/
+	} bw_est;
+	u32    cur_bw_est;
+
+	/*per subflow data, for tcp_recvmsg*/
+	u32     peek_seq;       /* Peek seq, for use by MTCP            */
+	u32     *seq;
+	u32     copied;
+	u32    map_data_seq; /*Those three fields record the current mapping*/
+	u16    map_data_len;
+	u32    map_subseq;
+/*isn: needed to translate abs to relative subflow seqnums*/
+	u32    snt_isn;
+	u32    rcv_isn;
+	unsigned long last_snd_probe;
+	unsigned long last_rcv_probe;
+#endif
+/*We keep these flags even if CONFIG_MTCP is not checked, because it allows
+  checking MTPC capability just by checking the mpc flag, rather than adding
+  ifdefs everywhere.*/
+	u8      mpc:1,          /* Other end is multipath capable       */
+		
+		wait_event_any_sk_released:1, /*1 if mtcp_wait_event_any_sk()
+						has released this sock, and
+						must thus lock it again,
+						so that to let everything
+						equal. This is important
+						because a new subsocket
+						can appear during we sleep.*/	
+		wait_data_bit_set:1, /*Similar to previous, for wait_data*/
+		push_frames:1, /*An other subsocket may liberate space in the
+				 sending window of this sock. Normally, a push
+				 is then done immediately, but if the socket is
+				 locked at that moment, push_frames is set, so
+				 that the push is done in the release_sock.*/
+		mss_too_low:1, /*mss for this sock is too low, just ignore*/
+		pf:1; /*Potentially Failed state: when this flag is set, we
+			stop using the subflow*/
+	
 	u32	rcv_wup;	/* rcv_nxt on last window update sent	*/
  	u32	snd_nxt;	/* Next sequence we send		*/
 
@@ -275,17 +307,20 @@ struct tcp_sock {
 	/* Data for direct copy to user */
 	struct {
 		struct sk_buff_head	prequeue;
+		int			memory;		
+#ifndef CONFIG_MTCP
 		struct task_struct	*task;
 		struct iovec		*iov;
-		int			memory;
-		int			len;
+		int                     len;
 #ifdef CONFIG_NET_DMA
+		int			copied;		
 		/* members for async copy */
 		struct dma_chan		*dma_chan;
 		int			wakeup;
 		struct dma_pinned_list	*pinned_list;
 		dma_cookie_t		dma_cookie;
 #endif
+#endif /*CONFIG_MTCP*/
 	} ucopy;
 
 	u32	snd_wl1;	/* Sequence for window update		*/
@@ -293,7 +328,7 @@ struct tcp_sock {
 	u32	max_window;	/* Maximal window ever seen from peer	*/
 	u32	mss_cache;	/* Cached effective mss, not including SACKS */
 
-	u32	window_clamp;	/* Maximal window to advertise		*/
+	u32     window_clamp;   /* Maximal window to advertise		*/
 	u32	rcv_ssthresh;	/* Current window clamp			*/
 
 	u32	frto_highmark;	/* snd_nxt when RTO occurred */
@@ -321,6 +356,7 @@ struct tcp_sock {
  *      Options received (usually on last packet, some only on SYN packets).
  */
 	struct tcp_options_received rx_opt;
+	struct multipath_options mopt;
 
 /*
  *	Slow start and congestion control (see also Nagle, and Karn & Partridge)
@@ -409,6 +445,18 @@ struct tcp_sock {
 #endif
 
 	int			linger2;
+	struct multipath_pcb    *mpcb;
+#ifdef CONFIG_MTCP
+	int                     path_index;
+	struct tcp_sock         *next; /*Next subflow socket*/
+#ifdef CONFIG_MTCP_PM
+	u32                     mtcp_loc_token;
+	uint8_t                 pending:1, /*One if this is a pending subsock
+					     (established, but not yet
+					     attached to the mpcb)*/
+		                slave_sk:1;
+#endif
+#endif
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
diff --git a/include/linux/tcp_options.h b/include/linux/tcp_options.h
new file mode 100644
index 0000000..a949045
--- /dev/null
+++ b/include/linux/tcp_options.h
@@ -0,0 +1,97 @@
+/*
+ * INET		An implementation of the TCP/IP protocol suite for the LINUX
+ *		operating system.  INET is implemented using the  BSD Socket
+ *		interface as the means of communication with the user level.
+ *
+ *		Definitions for the TCP module.
+ *
+ * Version:	@(#)tcp.h	1.0.5	05/23/93
+ *
+ * Authors:	Ross Biro
+ *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
+ *              S. Barré: Added this file, to recuperate a portion
+ *              of the previous tcp.h file, in order to support mtcp
+ *              includes interdependence.
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+
+#ifndef _TCP_OPTIONS_H
+#define _TCP_OPTIONS_H
+
+#include <linux/types.h>
+#include <net/mtcp_pm.h>
+
+#define OPTION_SACK_ADVERTISE	(1 << 0)
+#define OPTION_TS		(1 << 1)
+#define OPTION_MD5		(1 << 2)
+#define OPTION_MPC              (1 << 3)
+#define OPTION_TOKEN            (1 << 4)
+#define OPTION_DSN              (1 << 5)
+#define OPTION_ADDR             (1 << 6)
+#define OPTION_JOIN             (1 << 7)
+#define OPTION_DATA_ACK         (1 << 8)
+#define OPTION_DFIN             (1 << 9)
+
+struct tcp_out_options {
+	u16 options;		/* bit field of OPTION_* */
+	u8 ws;			/* window scale, 0 to disable */
+	u8 num_sack_blocks;	/* number of SACK blocks to include */
+	u16 mss;		/* 0 to disable */
+	__u32 tsval, tsecr;	/* need to include OPTION_TS */
+	__u32 data_seq;         /* data sequence number, for MPTCP */
+	__u32 data_ack;         /* data ack, for MPTCP */
+	__u16 data_len;         /* data level length, for MPTCP*/
+	__u32 sub_seq;          /* subflow seqnum, for MPTCP*/
+#ifdef CONFIG_MTCP_PM
+	__u32 token;            /* token for mptcp */
+	struct mtcp_loc4 *addr4;  /* v4 addresses for MPTCP */
+	int num_addr4;          /* Number of addresses v4, MPTCP*/
+	u8      addr_id;        /* address id */
+#endif
+};
+
+struct tcp_options_received {
+/*	PAWS/RTTM data	*/
+	long	ts_recent_stamp;/* Time we stored ts_recent (for aging) */
+	u32	ts_recent;	/* Time stamp to echo next		*/
+	u32	rcv_tsval;	/* Time stamp value             	*/
+	u32	rcv_tsecr;	/* Time stamp echo reply        	*/
+	u32 	saw_tstamp : 1,	/* Saw TIMESTAMP on last packet		*/
+		tstamp_ok : 1,	/* TIMESTAMP seen on SYN packet		*/
+		dsack : 1,	/* D-SACK is scheduled			*/
+		wscale_ok : 1,	/* Wscale seen on SYN packet		*/
+		sack_ok : 4,	/* SACK seen on SYN packet		*/
+		snd_wscale : 4,	/* Window scaling received from sender	*/
+		rcv_wscale : 4,	/* Window scaling to send to receiver	*/
+		saw_mpc : 1,    /* MPC option seen, for MPTCP */
+		saw_dfin :1;    /* DFIN option seen, for MPTCP */
+/*	SACKs data	*/
+	u8	eff_sacks;	/* Size of SACK array to send with next packet */
+	u8	num_sacks;	/* Number of SACK blocks		*/
+	u16	user_mss;  	/* mss requested by user in ioctl */
+	u16	mss_clamp;	/* Maximal mss, negotiated at connection setup */
+#ifdef CONFIG_MTCP_PM
+ 	u32     mtcp_rem_token; /* Remote token, for mptcp */
+	u32     rcv_isn; /*Needed to retrieve abs subflow seqnum from the
+			   relative version.*/	
+#endif
+};
+
+struct multipath_options {	
+#ifdef CONFIG_MTCP_PM
+	int    num_addr4; 
+	int    num_addr6;
+	struct mtcp_loc4 addr4[MTCP_MAX_ADDR];
+	struct mtcp_loc6 addr6[MTCP_MAX_ADDR];
+#endif
+	u8      list_rcvd:1, /*1 if IP list has been received (MTCP_PM)*/
+		dfin_rcvd:1;
+	u32     fin_dsn; /*DSN of the byte 
+			   FOLLOWING the Data FIN*/
+};
+
+#endif /*_TCP_OPTIONS_H*/
diff --git a/include/linux/tcp_probe.h b/include/linux/tcp_probe.h
new file mode 100644
index 0000000..2f7e8a3
--- /dev/null
+++ b/include/linux/tcp_probe.h
@@ -0,0 +1,23 @@
+#ifndef __TCP_PROBE_H__
+#define __TCP_PROBE_H__ 1
+
+#include <linux/tcp.h>
+
+struct tcpprobe_ops {
+	int (*rcv_established)(struct sock *sk, struct sk_buff *skb,
+			       struct tcphdr *th, unsigned len);
+	int (*transmit_skb)(struct sock *sk, struct sk_buff *skb, 
+			    int clone_it, gfp_t gfp_mask);
+	int (*logmsg)(struct sock *sk,char *msg, va_list args);
+};
+
+
+int register_probe(struct tcpprobe_ops* ops, unsigned char ipversion);
+int unregister_probe(struct tcpprobe_ops* ops, unsigned char ipversion);
+
+int tcpprobe_rcv_established(struct sock *sk, struct sk_buff *skb,
+			     struct tcphdr *th, unsigned len);
+int tcpprobe_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
+			  gfp_t gfp_mask);
+int tcpprobe_logmsg(struct sock *sk,char *fmt,...);
+#endif /*__TCP_PROBE_H__*/
diff --git a/include/linux/xfrm.h b/include/linux/xfrm.h
index 4bc1e6b..825791b 100644
--- a/include/linux/xfrm.h
+++ b/include/linux/xfrm.h
@@ -141,6 +141,7 @@ enum
 #define XFRM_MODE_BEET 4
 #define XFRM_MODE_MAX 5
 
+
 /* Netlink configuration messages.  */
 enum {
 	XFRM_MSG_BASE = 0x10,
diff --git a/include/net/dst.h b/include/net/dst.h
index 8a8b71e..a113a3b 100644
--- a/include/net/dst.h
+++ b/include/net/dst.h
@@ -85,6 +85,7 @@ struct dst_entry
 		struct rt6_info   *rt6_next;
 		struct dn_route  *dn_next;
 	};
+	unsigned int            path_index;
 };
 
 
diff --git a/include/net/flow.h b/include/net/flow.h
index b45a5e4..ce62c07 100644
--- a/include/net/flow.h
+++ b/include/net/flow.h
@@ -14,6 +14,7 @@ struct flowi {
 	int	oif;
 	int	iif;
 	__u32	mark;
+	unsigned int path_index;
 
 	union {
 		struct {
diff --git a/include/net/ipv6.h b/include/net/ipv6.h
index 6d5b58a..33fde45 100644
--- a/include/net/ipv6.h
+++ b/include/net/ipv6.h
@@ -39,6 +39,7 @@
 #define NEXTHDR_ICMP		58	/* ICMP for IPv6. */
 #define NEXTHDR_NONE		59	/* No next header */
 #define NEXTHDR_DEST		60	/* Destination options header. */
+
 #define NEXTHDR_MOBILITY	135	/* Mobility header. */
 
 #define NEXTHDR_MAX		255
@@ -605,6 +606,7 @@ static inline int snmp6_unregister_dev(struct inet6_dev *idev) { return 0; }
 extern ctl_table ipv6_route_table_template[];
 extern ctl_table ipv6_icmp_table_template[];
 
+
 extern struct ctl_table *ipv6_icmp_sysctl_init(struct net *net);
 extern struct ctl_table *ipv6_route_sysctl_init(struct net *net);
 extern int ipv6_sysctl_register(void);
diff --git a/include/net/mtcp.h b/include/net/mtcp.h
new file mode 100644
index 0000000..83a22f3
--- /dev/null
+++ b/include/net/mtcp.h
@@ -0,0 +1,358 @@
+/*
+ *	MTCP implementation
+ *
+ *	Authors:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *      Part of this code is inspired from an early version for linux 2.4 by
+ *      Costin Raiciu.
+ *
+ *      date : May 10
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#ifndef _MTCP_H
+#define _MTCP_H
+
+#include <linux/tcp_options.h>
+#include <linux/notifier.h>
+#include <linux/xfrm.h>
+#include <linux/aio.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/mutex.h>
+#include <linux/completion.h>
+#include <linux/skbuff.h>
+#include <linux/list.h>
+#include <linux/tcp.h>
+
+#include <net/request_sock.h>
+#include <net/mtcp_pm.h>
+
+#ifdef CONFIG_MTCP_DEBUG
+  # define mtcp_debug(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+  # define mtcp_debug(fmt,args...)
+#endif
+
+
+/*Default MSS for MPTCP
+  All subflows will be using that MSS. If any subflow has a lower MSS, it is
+  just not used.*/
+#define MPTCP_MSS 1400
+extern int sysctl_mptcp_mss;
+
+
+/*DEBUG - TODEL*/
+
+#define MTCP_DEBUG_OFO_QUEUE 0x1
+#define MTCP_DEBUG_QUEUE_SKB 0x2
+#define MTCP_DEBUG_CHECK_RCV_QUEUE 0x4
+#define MTCP_DEBUG_DATA_QUEUE 0x8
+#define MTCP_DEBUG_COPY_TO_IOVEC 0x10
+
+#ifdef MTCP_RCV_QUEUE_DEBUG
+struct mtcp_debug {
+	const char* func_name;
+	u32 seq;
+	int len;
+	int end; /*1 if this is the last debug info*/
+};
+
+void print_debug_array(void);
+void freeze_rcv_queue(struct sock *sk, const char *func_name);
+#endif
+
+extern struct proto mtcpsub_prot;
+
+#define MPCB_FLAG_SERVER_SIDE 	0   /* this mpcb belongs to a server side 
+				       connection.
+				       (obtained through a listen)*/
+#define MPCB_FLAG_FIN_ENQUEUED  1  /*A dfin has been enqueued on the meta-send
+				     queue.*/
+
+struct multipath_pcb {
+	struct tcp_sock           tp;
+
+	/*connection identifier*/
+	xfrm_address_t            remote_ulid, local_ulid;
+	__be16                    remote_port,local_port;
+	
+	/*list of sockets in this multipath connection*/
+	struct tcp_sock*          connection_list;
+	
+	/*Master socket, also part of the connection_list, this
+	  socket is the one that the application sees.*/
+	struct sock*              master_sk;
+	/*socket count in this connection*/
+	int                       cnt_subflows;    
+	int                       syn_sent;
+	int                       cnt_established;
+	int                       err;
+	
+	char                      done;
+	unsigned short            shutdown;
+	
+	struct {
+		struct task_struct	*task;
+		struct iovec		*iov;
+/*The length field is initialized by mtcp_recvmsg, and decremented by 
+  each subsocket separately, upon data reception. That's why each subsocket
+  must do the copies with appropriate locks.
+  Whenever a subsocket decrements this field, it must increment its 
+  tp->copied field, so that we can track later how many bytes have been
+  eaten by which subsocket.*/
+		int                     len;
+	} ucopy; /*Fields moved from tcp_sock struct to this one*/
+
+	struct multipath_options  received_options;
+	struct tcp_options_received tcp_opt;
+
+	struct sk_buff_head	  reinject_queue;
+	spinlock_t                lock;
+	struct mutex              mutex;
+	struct kref               kref;	
+	struct notifier_block     nb; /*For listening to PM events*/
+	unsigned long		  flags; /* atomic, for bits see 
+					    MPCB_FLAG_XXX */
+	u32                       noneligible; /*Path mask of temporarily
+						 non eligible
+						 subflows by the scheduler*/
+#ifdef CONFIG_MTCP_PM
+	struct list_head          collide_tk;
+	uint8_t                   addr_unsent; /* num of addrs not yet
+				                  sent to our peer */
+	
+	struct mtcp_loc4          addr4[MTCP_MAX_ADDR]; /*We need to store
+							  the set of local
+							  addresses, so 
+							  that we have 
+							  a stable view
+							  of the available
+							  addresses. 
+							  Playing with the
+							  addresses directly
+							  in the system
+							  would expose us
+							  to concurrency
+							  problems*/
+	int                       num_addr4; /*num of addresses actually
+					       stored above.*/
+	struct mtcp_loc6          addr6[MTCP_MAX_ADDR];
+	int                       num_addr6;
+
+	struct path4             *pa4;
+	int                       pa4_size;
+	struct path6             *pa6;
+	int                       pa6_size;
+
+	int                       next_unused_pi; /*Next pi to pick up
+						    in case a new path
+						    becomes available*/
+#endif
+};
+
+#define mpcb_from_tcpsock(tp) ((tp)->mpcb)
+#define is_master_sk(tp) (!(tp)->slave_sk)
+#define is_meta_tp(tp) ((tp)->mpcb && &(tp)->mpcb->tp==tp)
+#define is_meta_sk(sk) ((tcp_sk(sk))->mpcb && 			\
+			&(tcp_sk(sk))->mpcb->tp==tcp_sk(sk))
+#define is_dfin_seg(mpcb,skb) (mpcb->received_options.dfin_rcvd && \
+			       mpcb->received_options.fin_dsn==	   \
+			       TCP_SKB_CB(skb)->end_data_seq)
+
+/*Iterates overs all subflows*/
+#define mtcp_for_each_tp(mpcb,tp)			\
+	for ((tp)=(mpcb)->connection_list;(tp);(tp)=(tp)->next)
+
+/*Iterates over new subflows. prevnum is the number
+  of flows already known by the caller.
+  Note that prevnum is altered by this macro*/
+#define mtcp_for_each_newtp(mpcb,tp,prevnum)				\
+	for ((tp)=(mpcb)->connection_list,				\
+		     prevnum=(mpcb)->cnt_subflows-prevnum;		\
+	     prevnum;(tp)=(tp)->next,prevnum--)
+
+#define mtcp_for_each_sk(mpcb,sk,tp)					\
+	for ((sk)=(struct sock*)(mpcb)->connection_list,(tp)=tcp_sk(sk); \
+	     sk;							\
+	     sk=(struct sock*)tcp_sk(sk)->next,tp=tcp_sk(sk))
+
+#define mtcp_for_each_sk_safe(__mpcb,__sk,__temp)			\
+	for (__sk=(struct sock*)(__mpcb)->connection_list,		\
+		     __temp=(__sk)?(struct sock*)tcp_sk(__sk)->next:NULL; \
+	     __sk;							\
+	     __sk=__temp,						\
+		     __temp=(__sk)?(struct sock*)tcp_sk(__sk)->next:NULL)
+
+/*Returns 1 if any subflow meets the condition @cond
+  Else return 0. Moreover, if 1 is returned, sk points to the
+  first subsocket that verified the condition*/
+#define mtcp_test_any_sk(mpcb,sk,cond)			\
+	({int __ans=0; struct tcp_sock *__tp;		\
+		mtcp_for_each_sk(mpcb,sk,__tp) {	\
+			if (cond)  {			\
+				__ans=1;		\
+				break;			\
+			}				\
+		}					\
+		__ans;})				\
+	
+/*Idem here with tp in lieu of sk*/	
+#define mtcp_test_any_tp(mpcb,tp,cond)			\
+	({      int __ans=0;				\
+		mtcp_for_each_tp(mpcb,tp) {		\
+			if (cond) {			\
+				__ans=1;		\
+				break;			\
+			}				\
+		}					\
+		__ans;					\
+	})						\
+	
+#define mtcp_test_any_sk_tp(mpcb,sk,tp,cond)		\
+	({int __ans=0;					\
+		mtcp_for_each_sk(mpcb,sk,tp) {		\
+			if (cond) {			\
+				__ans=1;		\
+				break;			\
+			}				\
+		}					\
+		__ans;})				\
+	
+/*Returns 1 if all subflows meet the condition @cond
+  Else return 0. */
+#define mtcp_test_all_sk(mpcb,sk,cond)			\
+	({int __ans=1; struct tcp_sock *__tp;		\
+		mtcp_for_each_sk(mpcb,sk,__tp) {	\
+			if (!(cond)) {			\
+				__ans=0;		\
+				break;			\
+			}				\
+		}					\
+		__ans;})				\
+	
+/*Wait for event @__condition to happen on any subsocket, 
+  or __timeo to expire
+  This is the MPTCP equivalent of sk_wait_event */
+#define mtcp_wait_event_any_sk(__mpcb,__sk, __tp, __timeo, __condition)	\
+	({	int __rc;						\
+		mtcp_for_each_sk(__mpcb,__sk,__tp) {			\
+			release_sock(__sk);				\
+		}							\
+		__rc = mtcp_test_any_sk_tp(__mpcb,__sk,__tp,		\
+					   __condition);		\
+		if (!__rc)  						\
+			*(__timeo) = schedule_timeout(*(__timeo));	\
+		mtcp_for_each_sk(__mpcb,__sk,__tp)			\
+			lock_sock(__sk);				\
+		__rc = mtcp_test_any_sk_tp(__mpcb,__sk,__tp,		\
+					   __condition);		\
+		__rc;							\
+	})
+
+//#define DEBUG_PITOFLAG
+
+#ifdef DEBUG_PITOFLAG
+static inline int PI_TO_FLAG(int pi)
+{
+	BUG_ON(!pi);
+	return (1<<(pi-1));
+}
+#else
+#define PI_TO_FLAG(pi) (1<<(pi-1))
+#endif
+
+/*For debugging only. Verifies consistency between subsock seqnums
+  and metasock seqnums*/
+//#define MTCP_DEBUG_SEQNUMS 1
+#ifdef MTCP_DEBUG_SEQNUMS
+void mtcp_check_seqnums(struct multipath_pcb *mpcb, int before);
+#else
+#define mtcp_check_seqnums(mpcb, before)
+#endif
+
+//#define MTCP_DEBUG_PKTS_OUT 1
+
+#ifdef MTCP_DEBUG_PKTS_OUT
+int check_pkts_out(struct sock* sk);
+void check_send_head(struct sock *sk,int num);
+#else
+#define check_pkts_out(sk)
+#define check_send_head(sk,num)
+#endif
+
+static inline void mtcp_init_addr_list(struct multipath_options *mopt)
+{
+	mopt->list_rcvd=
+		mopt->num_addr4=
+		mopt->num_addr6=0;
+}
+
+/**
+ * This function is almost exactly the same as sk_wmem_free_skb.
+ * The only difference is that we call kfree_skb instead of __kfree_skb.
+ * This is important because a subsock may want to remove an skb,
+ * while the meta-sock still has a reference to it.
+ */
+static inline void mtcp_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
+{
+	skb_truesize_check(skb);
+	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
+	sk->sk_wmem_queued -= skb->truesize;
+	sk_mem_uncharge(sk, skb->truesize);
+	kfree_skb(skb);
+}
+
+int mtcp_wait_data(struct multipath_pcb *mpcb, struct sock *master_sk, 
+		   long *timeo,int flags);
+int mtcp_queue_skb(struct sock *sk,struct sk_buff *skb);
+void mtcp_ofo_queue(struct multipath_pcb *mpcb);
+int mtcp_check_rcv_queue(struct multipath_pcb *mpcb,struct msghdr *msg, 
+			 size_t *len, u32 *data_seq, int *copied, int flags);
+/*Possible return values from mtcp_queue_skb*/
+#define MTCP_EATEN 1 /*The skb has been (fully or partially) eaten by the app*/
+#define MTCP_QUEUED 2 /*The skb has been queued in the mpcb ofo queue*/
+
+struct multipath_pcb* mtcp_alloc_mpcb(struct sock *master_sk);
+void mtcp_ask_update(struct sock *sk);
+void mtcp_destroy_mpcb(struct multipath_pcb *mpcb);
+void mtcp_add_sock(struct multipath_pcb *mpcb,struct tcp_sock *tp);
+void mtcp_del_sock(struct multipath_pcb *mpcb, struct tcp_sock *tp);
+void mtcp_reset_options(struct multipath_options* mopt);
+void mtcp_update_metasocket(struct sock *sock);
+int mtcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
+		 size_t size);
+int mtcp_is_available(struct sock *sk);
+struct sock* get_available_subflow(struct multipath_pcb *mpcb, 
+				   struct sk_buff *skb, int *pf);
+void mtcp_reinject_data(struct sock *orig_sk);
+int mtcp_get_dataseq_mapping(struct tcp_sock *tp, struct sk_buff *skb);
+int mtcp_init_subsockets(struct multipath_pcb *mpcb, 
+			 uint32_t path_indices);
+int mtcpsub_get_port(struct sock *sk, unsigned short snum);
+void mtcp_update_window_clamp(struct multipath_pcb *mpcb);
+void mtcp_update_sndbuf(struct multipath_pcb *mpcb);
+void mtcp_update_dsn_ack(struct multipath_pcb *mpcb, u32 start, u32 end);
+int mtcpv6_init(void);
+void mpcb_get(struct multipath_pcb *mpcb);
+void mpcb_put(struct multipath_pcb *mpcb);
+void mtcp_data_ready(struct sock *sk);
+void mtcp_push_frames(struct sock *sk);
+int mtcp_v4_add_raddress(struct multipath_options *mopt,			
+			 struct in_addr *addr, u8 id);
+
+void verif_wqueues(struct multipath_pcb *mpcb);
+
+void mtcp_skb_entail(struct sock *sk, struct sk_buff *skb);
+struct sk_buff* mtcp_next_segment(struct sock *sk, int *reinject);
+void mpcb_release(struct kref* kref);
+void mtcp_clean_rtx_queue(struct sock *sk);
+void mtcp_send_fin(struct sock *mpcb_sk);
+void mtcp_close(struct sock *master_sk, long timeout);
+#endif /*_MTCP_H*/
diff --git a/include/net/mtcp_pm.h b/include/net/mtcp_pm.h
new file mode 100644
index 0000000..918410b
--- /dev/null
+++ b/include/net/mtcp_pm.h
@@ -0,0 +1,80 @@
+/*
+ *	MTCP PM implementation
+ *
+ *	Authors:
+ *      Sébastien Barré           <sebastien.barre@uclouvain.be>
+ *
+ *      date : March 09
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#ifndef _MTCP_PM_H
+#define _MTCP_PM_H
+
+#ifdef CONFIG_MTCP_PM
+
+#include <linux/list.h>
+#include <linux/jhash.h>
+#include <linux/types.h>
+#include <linux/in.h>
+#include <linux/in6.h>
+#include <linux/skbuff.h>
+
+#define MTCP_MAX_ADDR 3 /*Max number of local or remote addresses we can store*/
+
+struct multipath_pcb;
+
+struct mtcp_loc4 {
+	u8                id;
+	struct in_addr    addr;
+};
+
+struct mtcp_loc6 {
+	u8                 id;
+	struct in6_addr    addr;
+};
+
+struct path4 {
+	struct mtcp_loc4  loc; /*local address*/
+	struct mtcp_loc4  rem; /*remote address*/
+	int               path_index;
+};
+
+struct path6 {
+	struct mtcp_loc6  loc; /*local address*/
+	struct mtcp_loc6  rem; /*remote address*/
+	int               path_index;
+};
+
+struct mtcp_pm_ctx {
+	struct list_head    collide_token;
+
+/*token information*/
+	u32                 tk_local;
+
+	struct multipath_pcb *mpcb;
+};
+
+#define loc_token(mpcb)					\
+	(tcp_sk(mpcb->master_sk)->mtcp_loc_token)
+
+u32 mtcp_new_token(void);
+void mtcp_hash_insert(struct multipath_pcb *mpcb,u32 token);
+void mtcp_hash_remove(struct multipath_pcb *mpcb);
+struct multipath_pcb* mtcp_hash_find(u32 token);
+void mtcp_set_addresses(struct multipath_pcb *mpcb);
+void mtcp_update_patharray(struct multipath_pcb *mpcb);
+struct in_addr *mtcp_get_loc_addr(struct multipath_pcb *mpcb, int path_index);
+struct in_addr *mtcp_get_rem_addr(struct multipath_pcb *mpcb, int path_index);
+u8 mtcp_get_loc_addrid(struct multipath_pcb *mpcb, int path_index);
+int mtcp_lookup_join(struct sk_buff *skb);
+int mtcp_syn_recv_sock(struct sk_buff *skb);
+int mtcp_check_new_subflow(struct multipath_pcb *mpcb);
+void mtcp_pm_release(struct multipath_pcb *mpcb);
+#endif /* CONFIG_MTCP_PM */
+#endif /*_MTCP_PM_H*/
diff --git a/include/net/mtcp_v6.h b/include/net/mtcp_v6.h
new file mode 100644
index 0000000..70e12f3
--- /dev/null
+++ b/include/net/mtcp_v6.h
@@ -0,0 +1,84 @@
+/*
+ *	MTCP implementation
+ *      IPv6-related functions  
+ *
+ *	Authors:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *      date : June 09
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+
+#ifndef _MTCP_V6_H
+#define _MTCP_V6_H
+
+#include <net/inet6_hashtables.h>
+
+/*Lookup for socket, taking into account the path index*/
+
+struct sock *__mtcpv6_lookup_established(struct net *net,
+					 struct inet_hashinfo *hashinfo,
+					 const struct in6_addr *saddr,
+					 const __be16 sport,
+					 const struct in6_addr *daddr,
+					 const u16 hnum,
+					 const int dif, const int path_index);
+
+struct sock *mtcpv6_lookup_listener(struct net *net,
+				    struct inet_hashinfo *hashinfo, 
+				    const struct in6_addr *daddr,
+				    const unsigned short hnum, const int dif, 
+				    const int path_index);
+
+static inline struct sock *__mtcpv6_lookup(struct net *net,
+					   struct inet_hashinfo *hashinfo,
+					   const struct in6_addr *saddr,
+					   const __be16 sport,
+					   const struct in6_addr *daddr,
+					   const u16 hnum,
+					   const int dif, const int path_index)
+{
+	struct sock *sk = __mtcpv6_lookup_established(net, hashinfo, saddr,
+						      sport, daddr, hnum, dif,
+						      path_index);
+	if (sk)
+		return sk;
+	
+	/*For listening socket, we use the standard function, simply ignoring
+	  the path index, since no MTCP slave socket is listening. (we do never
+	  call listen on those kinds of sockets)*/
+	sk=mtcpv6_lookup_listener(net, hashinfo, daddr, hnum, dif,path_index);
+
+	return sk;
+}
+
+static inline struct sock *__mtcpv6_lookup_skb(struct inet_hashinfo *hashinfo,
+					       struct sk_buff *skb,
+					       const __be16 sport,
+					       const __be16 dport)
+{
+	struct sock *sk;
+	
+	if (unlikely(sk = skb_steal_sock(skb)))
+		return sk;
+	else return __mtcpv6_lookup(dev_net(skb->dst->dev), hashinfo,
+				    &ipv6_hdr(skb)->saddr, sport,
+				    &ipv6_hdr(skb)->daddr, ntohs(dport),
+				    inet6_iif(skb),skb->path_index);	
+}
+
+extern struct sock *mtcpv6_lookup(struct net *net, 
+				  struct inet_hashinfo *hashinfo,
+				  const struct in6_addr *saddr, 
+				  const __be16 sport,
+				  const struct in6_addr *daddr, 
+				  const __be16 dport,
+				  const int dif, const int path_index);
+
+#endif /* _MTCP_V6_H */
diff --git a/include/net/netevent.h b/include/net/netevent.h
index e82b7ba..d5bc234 100644
--- a/include/net/netevent.h
+++ b/include/net/netevent.h
@@ -12,6 +12,8 @@
  */
 #ifdef __KERNEL__
 
+#include <linux/in6.h>
+
 struct dst_entry;
 
 struct netevent_redirect {
@@ -19,10 +21,25 @@ struct netevent_redirect {
 	struct dst_entry *new;
 };
 
+/*For the moment this is only supported for IPv6
+  This indicates that new paths are available for the given 
+  local and remote ulids. */
+struct ulid_pair {
+	struct in6_addr* local;
+	struct in6_addr* remote;
+	uint32_t         path_indices; /*bitmap of paths that can be used
+					 For example, if bit 3 is set, then
+					 3 is currently a valid path index
+					 that can be understood by a Path 
+					 Manager*/
+};
+
 enum netevent_notif_type {
 	NETEVENT_NEIGH_UPDATE = 1, /* arg is struct neighbour ptr */
 	NETEVENT_PMTU_UPDATE,	   /* arg is struct dst_entry ptr */
 	NETEVENT_REDIRECT,	   /* arg is struct netevent_redirect ptr */
+	NETEVENT_PATH_UPDATEV6,    /* arg is struct ulid_pair ptr*/
+	NETEVENT_MPS_UPDATEME,     /* arg is struct ulid_pair ptr*/
 };
 
 extern int register_netevent_notifier(struct notifier_block *nb);
diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index c719084..87660c5 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -1,3 +1,4 @@
+
 /*
  * NET		Generic infrastructure for Network protocols.
  *
@@ -57,6 +58,21 @@ struct request_sock {
 	struct sock			*sk;
 	u32				secid;
 	u32				peer_secid;
+#ifdef CONFIG_MTCP
+	u8                              saw_mpc:1;
+#ifdef CONFIG_MTCP_PM
+	u32                             mtcp_loc_token;
+	u32                             mtcp_rem_token;
+	struct multipath_pcb            *mpcb;
+	/*Collision list in the tuple hashtable. We need to find
+	  the req sock when receiving the third msg of the 3-way handshake,
+	  since that one does not contain the token. If this makes
+	  the request sock too long, we can use kmalloc'ed specific entries for
+	  that tuple hashtable. At the moment, though, I extend the 
+	  request_sock.*/
+	struct list_head                collide_tuple;
+#endif
+#endif
 };
 
 static inline struct request_sock *reqsk_alloc(const struct request_sock_ops *ops)
diff --git a/include/net/shim6.h.BACKUP.2991.h b/include/net/shim6.h.BACKUP.2991.h
new file mode 100644
index 0000000..0b0c15d
--- /dev/null
+++ b/include/net/shim6.h.BACKUP.2991.h
@@ -0,0 +1,205 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+# undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+  # define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+  # define PDEBUG(fmt,args...)
+#endif
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/shim6.h.BACKUP.3210.h b/include/net/shim6.h.BACKUP.3210.h
new file mode 100644
index 0000000..0b0c15d
--- /dev/null
+++ b/include/net/shim6.h.BACKUP.3210.h
@@ -0,0 +1,205 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+# undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+  # define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+  # define PDEBUG(fmt,args...)
+#endif
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/shim6.h.BACKUP.3323.h b/include/net/shim6.h.BACKUP.3323.h
new file mode 100644
index 0000000..0b0c15d
--- /dev/null
+++ b/include/net/shim6.h.BACKUP.3323.h
@@ -0,0 +1,205 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+# undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+  # define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+  # define PDEBUG(fmt,args...)
+#endif
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/shim6.h.BASE.2991.h b/include/net/shim6.h.BASE.2991.h
new file mode 100644
index 0000000..8ef2fb1
--- /dev/null
+++ b/include/net/shim6.h.BASE.2991.h
@@ -0,0 +1,207 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+
+#undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+# define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+# define PDEBUG(fmt,args...)
+#endif
+
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/shim6.h.BASE.3210.h b/include/net/shim6.h.BASE.3210.h
new file mode 100644
index 0000000..8ef2fb1
--- /dev/null
+++ b/include/net/shim6.h.BASE.3210.h
@@ -0,0 +1,207 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+
+#undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+# define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+# define PDEBUG(fmt,args...)
+#endif
+
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/shim6.h.BASE.3323.h b/include/net/shim6.h.BASE.3323.h
new file mode 100644
index 0000000..8ef2fb1
--- /dev/null
+++ b/include/net/shim6.h.BASE.3323.h
@@ -0,0 +1,207 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+
+#undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+# define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+# define PDEBUG(fmt,args...)
+#endif
+
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/shim6.h.REMOTE.2991.h b/include/net/shim6.h.REMOTE.2991.h
new file mode 100644
index 0000000..0b0c15d
--- /dev/null
+++ b/include/net/shim6.h.REMOTE.2991.h
@@ -0,0 +1,205 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+# undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+  # define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+  # define PDEBUG(fmt,args...)
+#endif
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/shim6.h.REMOTE.3210.h b/include/net/shim6.h.REMOTE.3210.h
new file mode 100644
index 0000000..0b0c15d
--- /dev/null
+++ b/include/net/shim6.h.REMOTE.3210.h
@@ -0,0 +1,205 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+# undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+  # define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+  # define PDEBUG(fmt,args...)
+#endif
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/shim6.h.REMOTE.3323.h b/include/net/shim6.h.REMOTE.3323.h
new file mode 100644
index 0000000..0b0c15d
--- /dev/null
+++ b/include/net/shim6.h.REMOTE.3323.h
@@ -0,0 +1,205 @@
+/*
+ *	Linux SHIM6 implementation
+ *
+ *	Author:
+ *	Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *	date : October 2007
+ *
+ *      TODO : - add icmpv6 support for messages to transmit to upper layers.
+ *               for now, icmpv6 messages never travel across the shim layer.
+ *             - Add support for context recovery
+ *             - Take TCP states into account for garbage collection
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License 
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _NET_SHIM6_H
+#define _NET_SHIM6_H
+
+
+#include <linux/ipv6.h>
+
+
+#include <net/flow.h>
+#include <linux/skbuff.h>
+#include <net/ipv6.h>
+#include <linux/timer.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/shim6.h>
+#include <net/reap.h>
+#include <net/shim6_types.h>
+#include <net/xfrm.h>
+#include <linux/in6.h>
+
+
+/*Macro for activation/deactivation of debug messages*/
+# undef PDEBUG
+#ifdef CONFIG_IPV6_SHIM6_DEBUG
+  # define PDEBUG(fmt,args...) printk( KERN_DEBUG __FILE__ ": " fmt,##args)
+#else
+  # define PDEBUG(fmt,args...)
+#endif
+
+#define MAX_SHIM6_HEADER (24+sizeof(struct ipv6hdr)+MAX_HEADER)
+
+
+struct shim6hdr_pld 
+{
+	__u8      nexthdr;
+	__u8      hdrlen;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8      ct_1:7,		  
+                  P:1;
+#elif defined(__BIG_ENDIAN_BITFIELD)
+	__u8      P:1,
+		  ct_1:7;
+#else
+#error	"Please fix <asm/byteorder.h>"
+#endif
+	__u8      ct_2;
+	__u32     ct_3;
+};
+
+
+/*This function is to be called for every other packet (when we are sure that 
+  we will not find the shim6 header in a further extension, that is, when every
+  extension (mentioned in the draft as being possibly located before shim6)
+  has been parsed*/
+void shim6_input_std(struct sk_buff* skb);
+
+/* Global locators table management
+ */
+void shim6_add_glob_locator(struct inet6_ifaddr* loc);
+void shim6_del_glob_locator(struct inet6_ifaddr* loc);
+
+
+/*This function is the one to pass to kref_put
+  It should NEVER be run directly*/
+void shim6_ctx_release(struct kref* kref);
+
+
+/* This is a wrapper to allow hiding an implementation 
+ * specific lookup method.
+ * This function increments a refcnt. For this reason, we MUST
+ * call loc_l_put when we have finished with this address.
+ */
+
+#define lookup_loc_l(addr) (shim6_loc_l*)ipv6_get_ifaddr(addr,NULL,0)
+#define loc_l_put(loc_l) in6_ifa_put((struct inet6_ifaddr*)loc_l)
+#define loc_l_hold(loc_l) in6_ifa_hold((struct inet6_ifaddr*)loc_l)
+#define refcnt(loc_l) atomic_read(&((struct inet6_ifaddr*)loc_l)->refcnt)
+
+/*Input function for shim6 packets that do not have the ext header*/
+void shim6_input_std(struct sk_buff* skb);
+
+
+int shim6_xmit(struct sk_buff* skb, struct flowi* fl);
+
+/*Reap functions not defined in reap.h*/
+
+extern int reap_input(struct shim6hdr_ctl* ctl, struct reap_ctx* rctx);
+
+/* Allocates an skb for a shim6/reap control message and pushes the exact 
+ * necessary space for the message and options (according to msg_len and
+ * opt_len), with skb->transport_header pointing to the beginning of the 
+ * space allocated
+ * for the message. The common part (struct shim6hdr_ctl) of the message is
+ * is initialized, according to lengths and message type. 
+ *
+ * Only the csum field in the struct shim6hdr_ctl is not initialized since
+ * it can only be computed after having filled all fields.
+ *
+ * -@msg_len is the length of the message (example : sizeof(struct reaphdr_ka))
+ * -@opt_len is the the sum of all lengths for each option. 
+ *           (computed using the TOTAL_LENGTH() macro for each
+ *            option). This is zero if no option is used.
+ * -@type    is the message type, ex. REAP_TYPE_KEEPALIVE
+ * -@skbp    the address of the skb pointer to be allocated.
+ *           
+ *
+ * returns a negative error code in case of failure (currently only -ENOMEM)
+ *         0 in case of success.
+ */
+
+static inline int shim6_alloc_skb(int msg_len, int opt_len, int type,
+				  struct sk_buff** skbp)
+{
+	struct shim6hdr_ctl* common;
+	struct sk_buff* skb;
+
+	*skbp = skb = alloc_skb(MAX_SHIM6_HEADER+opt_len, 
+				GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ERR "shim6_alloc_skb : no buffer available\n");
+		return -ENOMEM;
+	}
+	
+	
+	skb_reserve(skb,MAX_SHIM6_HEADER+opt_len);
+	skb_push(skb,msg_len+opt_len);
+	skb_reset_transport_header(skb);
+	common=(struct shim6hdr_ctl*) skb_transport_header(skb);
+	
+	memset(common, 0, sizeof(struct shim6hdr_ctl));
+	common->nexthdr=NEXTHDR_NONE;
+	common->hdrlen=(msg_len+opt_len-8)>>3;
+	common->P=SHIM6_MSG_CONTROL;
+	common->type=type;
+	return 0;
+}
+
+
+/*Adds/remove a locator from the global locator list in the daemon*/
+void shim6_new_daemon_loc(struct in6_addr* addr, int ifidx);
+void shim6_del_daemon_loc(struct in6_addr* addr, int ifidx);
+
+/*Filter for shim6 messages to be used by raw sockets, this separates
+  control and data messages*/
+extern int shim6_filter(struct sock *sk, struct sk_buff *skb);
+
+/*Modified version of xfrm6_input_addr (include/net/xfrm.h)
+  That does the xfrm lookup based on the shim6 context tag*/
+extern int shim6_xfrm_input_ct(struct sk_buff *skb, __u64 ct);
+
+/*Modified version of xfrm6_input_addr (net/ipv6/xfrm6_input.c)
+  That does the xfrm lookup based on saddr=ulid_peer, daddr=ulid_local
+  Unfortunately we cannot use xfrm6_input_addr here because the function must
+  be aware of the fact that xany is used as the hash key for daddr, which is
+  needed by the spi lookup (where daddr is part of the key, thus need to be 
+  xany)*/
+int shim6_xfrm_input_ulid(struct sk_buff *skb,  xfrm_address_t *daddr, xfrm_address_t *saddr);
+
+static inline int is_shim6_inbound(struct xfrm_state* x)
+{
+	return (x->id.proto==IPPROTO_SHIM6 && x->shim6 &&
+		(x->shim6->flags & SHIM6_DATA_INBOUND));
+}
+
+extern int sysctl_shim6_tcphint; /*declared in shim6_static.c*/
+extern int sysctl_shim6_enabled; /*idem*/
+
+
+#ifdef CONFIG_MTCP
+/*Currently this is not very efficient, optimize this later.*/
+static inline struct shim6_path *map_pi_path(int pi, struct shim6_path *pa,
+					     int pa_size)
+{
+	int i=0;
+	if (pi==0) pi=1; /*if pi is 0, use the ULIDs*/
+	for (i=0;i<pa_size;i++) {
+		if (pa[i].path_index==pi) return &pa[i];
+	}
+	
+	BUG(); /*Should not arrive here*/
+	return NULL;
+}
+/*TODEL*/
+void shim6_print_map(void); 
+#endif /*CONFIG_MTCP*/
+
+#endif /* _NET_SHIM6_H */
diff --git a/include/net/sock.h b/include/net/sock.h
index 2f47107..642f5bd 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -218,6 +218,9 @@ struct sock {
 	unsigned short		sk_type;
 	int			sk_rcvbuf;
 	socket_lock_t		sk_lock;
+	int                     sk_debug; /*TODEL*/
+	char                    sk_func[30]; /*TODEL*/
+	int                     sk_in_write_xmit; /*TODEL*/
 	/*
 	 * The backlog queue is special, it is always used with
 	 * the per-socket spinlock held and requires low latency
@@ -246,6 +249,7 @@ struct sock {
 	unsigned int		sk_gso_max_size;
 	int			sk_rcvlowat;
 	unsigned long 		sk_flags;
+	unsigned long		sock_flags;
 	unsigned long	        sk_lingertime;
 	struct sk_buff_head	sk_error_queue;
 	struct proto		*sk_prot_creator;
@@ -383,6 +387,16 @@ static __inline__ void sk_add_bind_node(struct sock *sk,
 	hlist_add_head(&sk->sk_bind_node, list);
 }
 
+static inline struct sock *__sk_bind_head(const struct hlist_head *head)
+{
+	return hlist_entry(head->first, struct sock, sk_bind_node);
+}
+
+static inline struct sock *sk_bind_head(const struct hlist_head *head)
+{
+	return hlist_empty(head) ? NULL : __sk_bind_head(head);
+}
+
 #define sk_for_each(__sk, node, list) \
 	hlist_for_each_entry(__sk, node, list, sk_node)
 #define sk_for_each_from(__sk, node) \
@@ -835,10 +849,17 @@ static inline void lock_sock(struct sock *sk)
 extern void release_sock(struct sock *sk);
 
 /* BH context may only use the following locking interface. */
-#define bh_lock_sock(__sk)	spin_lock(&((__sk)->sk_lock.slock))
-#define bh_lock_sock_nested(__sk) \
-				spin_lock_nested(&((__sk)->sk_lock.slock), \
-				SINGLE_DEPTH_NESTING)
+#define bh_lock_sock(__sk)	do {				\
+		sprintf((__sk)->sk_func,"%s",__FUNCTION__);	\
+		spin_lock(&((__sk)->sk_lock.slock));		\
+	} while(0)						\
+	
+
+#define bh_lock_sock_nested(__sk) do {					\
+		sprintf((__sk)->sk_func,"%s",__FUNCTION__);		\
+		spin_lock_nested(&((__sk)->sk_lock.slock),		\
+				 SINGLE_DEPTH_NESTING);			\
+	} while(0)
 #define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
 
 extern struct sock		*sk_alloc(struct net *net, int family,
@@ -1017,12 +1038,15 @@ static inline void sock_orphan(struct sock *sk)
 	write_unlock_bh(&sk->sk_callback_lock);
 }
 
+void mtcp_check_socket(struct sock *sk);
+
 static inline void sock_graft(struct sock *sk, struct socket *parent)
 {
 	write_lock_bh(&sk->sk_callback_lock);
 	sk->sk_sleep = &parent->wait;
 	parent->sk = sk;
 	sk_set_socket(sk, parent);
+	mtcp_check_socket(sk);
 	security_sock_graft(sk, parent);
 	write_unlock_bh(&sk->sk_callback_lock);
 }
@@ -1136,6 +1160,10 @@ static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
 	atomic_add(skb->truesize, &sk->sk_wmem_alloc);
 }
 
+#ifdef CONFIG_MTCP
+extern void mtcp_set_owner_r(struct sk_buff *skb, struct sock *sk);
+#endif
+
 static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
 {
 	skb->sk = sk;
diff --git a/include/net/tcp.h b/include/net/tcp.h
index 438014d..86dbc7e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -30,6 +30,7 @@
 #include <linux/dmaengine.h>
 #include <linux/crypto.h>
 #include <linux/cryptohash.h>
+#include <linux/tcp_probe.h>
 
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -41,6 +42,7 @@
 #include <net/ip.h>
 #include <net/tcp_states.h>
 #include <net/inet_ecn.h>
+#include <net/mtcp.h>
 
 #include <linux/seq_file.h>
 
@@ -167,6 +169,15 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOPT_TIMESTAMP	8	/* Better RTT estimations/PAWS */
 #define TCPOPT_MD5SIG		19	/* MD5 Signature (RFC2385) */
 
+#define TCPOPT_MPC   	        30
+#define TCPOPT_DSN		31
+#define TCPOPT_DFIN		32
+#define TCPOPT_DATA_ACK   	33
+
+#define TCPOPT_ADDR             60
+#define TCPOPT_REMADR           61
+#define TCPOPT_JOIN      	62
+
 /*
  *     TCP option lengths
  */
@@ -176,6 +187,18 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_SACK_PERM      2
 #define TCPOLEN_TIMESTAMP      10
 #define TCPOLEN_MD5SIG         18
+#ifdef CONFIG_MTCP_PM
+#define TCPOLEN_ADDR(num_addr) (2+6*(num_addr))
+#define TCPOLEN_ADDR_BASE      2
+#define TCPOLEN_ADDR_PERBLOCK  6
+#define TCPOLEN_JOIN           7
+#define TCPOLEN_MPC            7
+#else
+#define TCPOLEN_MPC            4
+#endif
+#define TCPOLEN_DSN            12
+#define TCPOLEN_DFIN           2
+#define TCPOLEN_DATA_ACK       6
 
 /* But this is what stacks really send out. */
 #define TCPOLEN_TSTAMP_ALIGNED		12
@@ -186,6 +209,16 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_SACK_PERBLOCK		8
 #define TCPOLEN_MD5SIG_ALIGNED		20
 #define TCPOLEN_MSS_ALIGNED		4
+#ifdef CONFIG_MTCP_PM
+#define TCPOLEN_ADDR_ALIGNED(num_addr) ((5+6*(num_addr)) & (~3))
+#define TCPOLEN_JOIN_ALIGNED            8
+#define TCPOLEN_MPC_ALIGNED             8
+#else
+#define TCPOLEN_MPC_ALIGNED             4
+#endif
+#define TCPOLEN_DSN_ALIGNED             12
+#define TCPOLEN_DFIN_ALIGNED            4
+#define TCPOLEN_DATA_ACK_ALIGNED        8
 
 /* Flags in tp->nonagle */
 #define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */
@@ -284,6 +317,11 @@ extern int		    	tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 
 extern int			tcp_sendmsg(struct kiocb *iocb, struct socket *sock,
 					    struct msghdr *msg, size_t size);
+#ifdef CONFIG_MTCP
+extern int			subtcp_sendmsg(struct kiocb *iocb, 
+					       struct sock *sk,
+					       struct msghdr *msg, size_t size);
+#endif
 extern ssize_t			tcp_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags);
 
 extern int			tcp_ioctl(struct sock *sk, 
@@ -331,7 +369,7 @@ extern void tcp_enter_quickack_mode(struct sock *sk);
 
 static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
 {
- 	rx_opt->tstamp_ok = rx_opt->sack_ok = rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
+ 	rx_opt->tstamp_ok = rx_opt->sack_ok = rx_opt->wscale_ok = rx_opt->snd_wscale = rx_opt->saw_mpc = 0;
 }
 
 #define	TCP_ECN_OK		1
@@ -395,6 +433,7 @@ extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 
 extern void			tcp_parse_options(struct sk_buff *skb,
 						  struct tcp_options_received *opt_rx,
+						  struct multipath_options *mopt,
 						  int estab);
 
 extern u8			*tcp_parse_md5sig_option(struct tcphdr *th);
@@ -433,6 +472,9 @@ extern struct sk_buff *		tcp_make_synack(struct sock *sk,
 
 extern int			tcp_disconnect(struct sock *sk, int flags);
 
+extern void tcp_push(struct sock *sk, int flags, int mss_now,
+			    int nonagle);
+
 
 /* From syncookies.c */
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];
@@ -529,7 +571,15 @@ static inline void tcp_fast_path_check(struct sock *sk)
  */
 static inline u32 tcp_receive_window(const struct tcp_sock *tp)
 {
-	s32 win = tp->rcv_wup + tp->rcv_wnd - tp->rcv_nxt;
+	s32 win;
+	
+	if (tp->mpcb && tp->mpc) { 
+		struct tcp_sock *mpcb_tp=(struct tcp_sock*)(tp->mpcb);
+		win=mpcb_tp->rcv_wup + mpcb_tp->rcv_wnd - mpcb_tp->rcv_nxt;
+	}
+	else {
+		win=tp->rcv_wup + tp->rcv_wnd - tp->rcv_nxt;
+	}
 
 	if (win < 0)
 		win = 0;
@@ -553,8 +603,8 @@ extern u32	__tcp_select_window(struct sock *sk);
 /* This is what the send packet queuing engine uses to pass
  * TCP per-packet control information to the transmission
  * code.  We also store the host-order sequence numbers in
- * here too.  This is 36 bytes on 32-bit architectures,
- * 40 bytes on 64-bit machines, if this grows please adjust
+ * here too.  This is 40 bytes on 32-bit architectures,
+ * 48 bytes on 64-bit machines, if this grows please adjust
  * skbuff.h:skbuff->cb[xxx] size appropriately.
  */
 struct tcp_skb_cb {
@@ -566,6 +616,14 @@ struct tcp_skb_cb {
 	} header;	/* For incoming frames		*/
 	__u32		seq;		/* Starting sequence number	*/
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
+	__u32           data_seq;       /* Starting data seq            */
+	__u32           data_ack;       /* Data level ack (MPTCP)       */
+	__u32           end_data_seq;   /* DATA_SEQ + DFIN + SYN + datalen*/
+	__u16           data_len;       /* Data-level length (MPTCP)    
+					 * a value of 0 indicates that no DSN
+					 * option is attached to that segment
+					 */
+	__u32           sub_seq;        /* subflow seqnum (MPTCP)       */
 	__u32		when;		/* used to compute rtt's	*/
 	__u8		flags;		/* TCP header flags.		*/
 
@@ -580,7 +638,6 @@ struct tcp_skb_cb {
 #define TCPCB_FLAG_URG		0x20
 #define TCPCB_FLAG_ECE		0x40
 #define TCPCB_FLAG_CWR		0x80
-
 	__u8		sacked;		/* State flags for SACK/FACK.	*/
 #define TCPCB_SACKED_ACKED	0x01	/* SKB ACK'd by a SACK block	*/
 #define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/
@@ -699,6 +756,17 @@ static inline void tcp_set_ca_state(struct sock *sk, const u8 ca_state)
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
 
+#ifdef CONFIG_MTCP
+	if (ca_state != icsk->icsk_ca_state) {
+		char buf[200];
+		snprintf(buf,sizeof(buf),
+			 "pi %d: changed ca state : %d -> %d",
+			 tcp_sk(sk)->path_index,
+			 icsk->icsk_ca_state,ca_state);
+		tcpprobe_logmsg(sk,buf);
+	}
+#endif
+
 	if (icsk->icsk_ca_ops->set_state)
 		icsk->icsk_ca_ops->set_state(sk, ca_state);
 	icsk->icsk_ca_state = ca_state;
@@ -798,11 +866,21 @@ static __inline__ __u32 tcp_max_burst(const struct tcp_sock *tp)
 	return tp->reordering;
 }
 
-/* Returns end sequence number of the receiver's advertised window */
-static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
+/* Returns end sequence number of the receiver's advertised window
+ * If @data_seq is 1, we return an end data_seq number (for mptcp)
+ * rather than an end seq number.
+ */
+static inline u32 tcp_wnd_end(const struct tcp_sock *tp, int data_seq)
 {
-	return tp->snd_una + tp->snd_wnd;
+	/*With MPTCP, we return the end DATASEQ number of the receiver's
+	  advertised window*/
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+	struct tcp_sock *mpcb_tp=(struct tcp_sock *)mpcb;
+	
+	if (!data_seq || !tp->mpcb) return tp->snd_una + tp->snd_wnd;
+	else return mpcb_tp->snd_una+mpcb_tp->snd_wnd;
 }
+
 extern int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight);
 
 static inline void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss,
@@ -829,14 +907,15 @@ static inline void tcp_push_pending_frames(struct sock *sk)
 	__tcp_push_pending_frames(sk, tcp_current_mss(sk, 1), tp->nonagle);
 }
 
-static inline void tcp_init_wl(struct tcp_sock *tp, u32 ack, u32 seq)
+static inline void tcp_init_wl(struct tcp_sock *tp, u32 seq)
 {
 	tp->snd_wl1 = seq;
 }
 
-static inline void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
+static inline void tcp_update_wl(struct tcp_sock *tp, u32 seq)
 {
-	tp->snd_wl1 = seq;
+	if (seq)
+		tp->snd_wl1 = seq;
 }
 
 /*
@@ -863,9 +942,11 @@ static inline int tcp_checksum_complete(struct sk_buff *skb)
 
 static inline void tcp_prequeue_init(struct tcp_sock *tp)
 {
+#ifndef CONFIG_MTCP
 	tp->ucopy.task = NULL;
 	tp->ucopy.len = 0;
 	tp->ucopy.memory = 0;
+#endif /*In MTCP, those fields are in the mpcb structure*/
 	skb_queue_head_init(&tp->ucopy.prequeue);
 #ifdef CONFIG_NET_DMA
 	tp->ucopy.dma_chan = NULL;
@@ -883,6 +964,49 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)
  *
  * NOTE: is this not too big to inline?
  */
+#ifdef CONFIG_MTCP
+static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+	
+	/*If the socket is still in the accept queue of the mpcb,
+	  the mpcb prequeue is not yet available*/
+	BUG_ON(!tp->mpcb && !tp->pending);
+	if (tp->mpc && !mpcb) return 0;
+
+	if (!sysctl_tcp_low_latency && mpcb->ucopy.task) {
+		__skb_queue_tail(&tp->ucopy.prequeue, skb);
+		tp->ucopy.memory += skb->truesize;
+		if (tp->ucopy.memory > sk->sk_rcvbuf) {			
+			struct sk_buff *skb1;
+			
+			BUG_ON(sock_owned_by_user(sk));
+			
+			while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) 
+			       != NULL) {
+				sk_backlog_rcv(sk, skb1);
+				NET_INC_STATS_BH(sock_net(sk), 
+						 LINUX_MIB_TCPPREQUEUEDROPPED);
+			}
+			
+			tp->ucopy.memory = 0;
+		} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
+			if (tp->mpc)
+				wake_up_interruptible(
+					tp->mpcb->master_sk->sk_sleep);
+			else
+				wake_up_interruptible(sk->sk_sleep);
+			if (!inet_csk_ack_scheduled(sk))
+				inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
+						          (3 * TCP_RTO_MIN) / 4,
+							  TCP_RTO_MAX);
+		}
+		return 1;
+	}
+	return 0;
+}
+#else
 static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -892,14 +1016,16 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 		tp->ucopy.memory += skb->truesize;
 		if (tp->ucopy.memory > sk->sk_rcvbuf) {
 			struct sk_buff *skb1;
-
+			
 			BUG_ON(sock_owned_by_user(sk));
-
-			while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) != NULL) {
+			
+			while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) 
+			       != NULL) {
 				sk_backlog_rcv(sk, skb1);
-				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPPREQUEUEDROPPED);
+				NET_INC_STATS_BH(sock_net(sk), 
+						 LINUX_MIB_TCPPREQUEUEDROPPED);
 			}
-
+			
 			tp->ucopy.memory = 0;
 		} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
 			wake_up_interruptible(sk->sk_sleep);
@@ -912,6 +1038,7 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 	}
 	return 0;
 }
+#endif
 
 
 #undef STATE_TRACE
@@ -951,7 +1078,32 @@ static inline int tcp_space(const struct sock *sk)
 {
 	return tcp_win_from_space(sk->sk_rcvbuf -
 				  atomic_read(&sk->sk_rmem_alloc));
-} 
+}
+
+#ifdef CONFIG_MTCP
+/*If MPTCP is used, tcp_space returns the aggregate space for the
+  whole communication. */
+static inline int mtcp_space(const struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	
+	if (!tp->mpc) return tcp_space(sk);
+	
+	return tcp_win_from_space(mpcb_sk->sk_rcvbuf-
+				  atomic_read(&mpcb_sk->sk_rmem_alloc));
+}
+
+static inline int mtcp_full_space(const struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+
+	return tcp_win_from_space(mpcb_sk->sk_rcvbuf);
+}
+#endif
 
 static inline int tcp_full_space(const struct sock *sk)
 {
@@ -969,6 +1121,18 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
 	req->mss = rx_opt->mss_clamp;
 	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
+#ifdef CONFIG_MTCP
+	req->saw_mpc = rx_opt->saw_mpc;
+#ifdef CONFIG_MTCP_PM
+	if (!req->mpcb) {
+		/*conn request, prepare a new token for the 
+		  mpcb that will be created in inet_csk_accept(),
+		  and store the received token.*/
+		req->mtcp_rem_token = rx_opt->mtcp_rem_token;
+		req->mtcp_loc_token = mtcp_new_token();
+	}
+#endif
+#endif
 	ireq->tstamp_ok = rx_opt->tstamp_ok;
 	ireq->sack_ok = rx_opt->sack_ok;
 	ireq->snd_wscale = rx_opt->snd_wscale;
diff --git a/include/net/xfrm.h b/include/net/xfrm.h
index 11c890a..99dd059 100644
--- a/include/net/xfrm.h
+++ b/include/net/xfrm.h
@@ -173,7 +173,7 @@ struct xfrm_state
 	struct xfrm_encap_tmpl	*encap;
 
 	/* Data for care-of address */
-	xfrm_address_t	*coaddr;
+	xfrm_address_t	        *coaddr;
 
 	/* IPComp needs an IPIP tunnel for handling uncompressed packets */
 	struct xfrm_state	*tunnel;
@@ -855,7 +855,7 @@ static inline int xfrm_sec_ctx_match(struct xfrm_sec_ctx *s1, struct xfrm_sec_ct
 /* A struct encoding bundle of transformations to apply to some set of flow.
  *
  * dst->child points to the next element of bundle.
- * dst->xfrm  points to an instanse of transformer.
+ * dst->xfrm  points to an instance of transformer.
  *
  * Due to unfortunate limitations of current routing cache, which we
  * have no time to fix, it mirrors struct rtable and bound to the same
@@ -1309,6 +1309,7 @@ extern int xfrm_state_add(struct xfrm_state *x);
 extern int xfrm_state_update(struct xfrm_state *x);
 extern struct xfrm_state *xfrm_state_lookup(xfrm_address_t *daddr, __be32 spi, u8 proto, unsigned short family);
 extern struct xfrm_state *xfrm_state_lookup_byaddr(xfrm_address_t *daddr, xfrm_address_t *saddr, u8 proto, unsigned short family);
+
 #ifdef CONFIG_XFRM_SUB_POLICY
 extern int xfrm_tmpl_sort(struct xfrm_tmpl **dst, struct xfrm_tmpl **src,
 			  int n, unsigned short family);
diff --git a/kernel/sched.c b/kernel/sched.c
index e4bb1dd..fc8c072 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -4722,7 +4722,7 @@ do_wait_for_common(struct completion *x, long timeout, int state)
 
 static long __sched
 wait_for_common(struct completion *x, long timeout, int state)
-{
+{	
 	might_sleep();
 
 	spin_lock_irq(&x->wait.lock);
@@ -8357,6 +8357,7 @@ void __might_sleep(char *file, int line)
 		"in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\n",
 			in_atomic(), irqs_disabled(),
 			current->pid, current->comm);
+	BUG();
 
 	debug_show_held_locks(current);
 	if (irqs_disabled())
diff --git a/kernel/softlockup.c b/kernel/softlockup.c
index dc0b3be..74fd38f 100644
--- a/kernel/softlockup.c
+++ b/kernel/softlockup.c
@@ -141,6 +141,7 @@ void softlockup_tick(void)
 	per_cpu(print_timestamp, this_cpu) = touch_timestamp;
 
 	spin_lock(&print_lock);
+	console_loglevel=8;
 	printk(KERN_ERR "BUG: soft lockup - CPU#%d stuck for %lus! [%s:%d]\n",
 			this_cpu, now - touch_timestamp,
 			current->comm, task_pid_nr(current));
diff --git a/lib/spinlock_debug.c b/lib/spinlock_debug.c
index 9c4b025..116f3d5 100644
--- a/lib/spinlock_debug.c
+++ b/lib/spinlock_debug.c
@@ -113,6 +113,7 @@ static void __spin_lock_debug(spinlock_t *lock)
 		/* lockup suspected: */
 		if (print_once) {
 			print_once = 0;
+			console_loglevel=8;
 			printk(KERN_EMERG "BUG: spinlock lockup on CPU#%d, "
 					"%s/%d, %p\n",
 				raw_smp_processor_id(), current->comm,
diff --git a/net/Kconfig b/net/Kconfig
index d789d79..537acc3 100644
--- a/net/Kconfig
+++ b/net/Kconfig
@@ -211,7 +211,7 @@ config NET_PKTGEN
 
 config NET_TCPPROBE
 	tristate "TCP connection probing"
-	depends on INET && EXPERIMENTAL && PROC_FS && KPROBES
+	depends on INET && EXPERIMENTAL && PROC_FS
 	---help---
 	This module allows for capturing the changes to TCP connection
 	state in response to incoming packets. It is used for debugging
diff --git a/net/core/datagram.c b/net/core/datagram.c
index ee63184..6221552 100644
--- a/net/core/datagram.c
+++ b/net/core/datagram.c
@@ -528,6 +528,7 @@ __sum16 __skb_checksum_complete_head(struct sk_buff *skb, int len)
 			netdev_rx_csum_fault(skb->dev);
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 	}
+
 	return sum;
 }
 EXPORT_SYMBOL(__skb_checksum_complete_head);
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 65f7757..445448f 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -209,6 +209,8 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 	skb->data = data;
 	skb_reset_tail_pointer(skb);
 	skb->end = skb->tail + size;
+	skb->path_index=0;
+	skb->path_mask=0;
 	/* make sure we initialize shinfo sequentially */
 	shinfo = skb_shinfo(skb);
 	atomic_set(&shinfo->dataref, 1);
@@ -424,9 +426,10 @@ static void skb_release_all(struct sk_buff *skb)
  *	Clean the state. This is an internal helper function. Users should
  *	always call kfree_skb
  */
-
+#include <net/tcp.h>
 void __kfree_skb(struct sk_buff *skb)
 {
+	BUG_ON(atomic_read(&skb->users) > 1);
 	skb_release_all(skb);
 	kfree_skbmem(skb);
 }
@@ -529,6 +532,9 @@ static void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)
 #endif
 #endif
 	new->vlan_tci		= old->vlan_tci;
+	
+	new->path_index         = old->path_index;
+	new->path_mask          = old->path_mask;
 
 	skb_copy_secmark(new, old);
 }
@@ -989,8 +995,11 @@ unsigned char *skb_push(struct sk_buff *skb, unsigned int len)
 {
 	skb->data -= len;
 	skb->len  += len;
-	if (unlikely(skb->data<skb->head))
+	if (unlikely(skb->data<skb->head)) {
 		skb_under_panic(skb, len, __builtin_return_address(0));
+		console_loglevel=8;
+		BUG();
+	}
 	return skb->data;
 }
 EXPORT_SYMBOL(skb_push);
diff --git a/net/core/sock.c b/net/core/sock.c
index edf7220..b186251 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -879,6 +879,82 @@ static void sock_copy(struct sock *nsk, const struct sock *osk)
 #endif
 }
 
+/*Code inspired from sk_clone()*/
+void mtcp_inherit_sk(struct sock *sk,struct sock *newsk) 
+{
+	struct sk_filter *filter;		
+#ifdef CONFIG_SECURITY_NETWORK
+	void *sptr;
+	security_sk_alloc(newsk,sk->sk_family,GFP_KERNEL);
+	sptr = newsk->sk_security;
+#endif
+	/*We cannot call sock_copy here, because obj_size may be the size
+	  of tcp6_sock if the app is loading an ipv6 socket.*/
+	memcpy(newsk,sk,sizeof(struct tcp_sock));
+#ifdef CONFIG_SECURITY_NETWORK
+	newsk->sk_security = sptr;
+	security_sk_clone(sk, newsk);
+#endif
+	
+	/* SANITY */
+	get_net(sock_net(newsk));
+	sk_node_init(&newsk->sk_node);
+	sock_lock_init(newsk);
+	bh_lock_sock(newsk);
+	newsk->sk_backlog.head	= newsk->sk_backlog.tail = NULL;
+	
+	atomic_set(&newsk->sk_rmem_alloc, 0);
+	atomic_set(&newsk->sk_wmem_alloc, 0);
+	atomic_set(&newsk->sk_omem_alloc, 0);
+	skb_queue_head_init(&newsk->sk_receive_queue);
+	skb_queue_head_init(&newsk->sk_write_queue);
+#ifdef CONFIG_NET_DMA
+	skb_queue_head_init(&newsk->sk_async_wait_queue);
+#endif
+
+	rwlock_init(&newsk->sk_dst_lock);
+	rwlock_init(&newsk->sk_callback_lock);
+	lockdep_set_class_and_name(&newsk->sk_callback_lock,
+				   af_callback_keys + newsk->sk_family,
+				   af_family_clock_key_strings[newsk->sk_family]);
+	
+	newsk->sk_dst_cache	= NULL;
+	newsk->sk_wmem_queued	= 0;
+	newsk->sk_forward_alloc = 0;
+	newsk->sk_send_head	= NULL;
+	newsk->sk_userlocks	= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;
+	
+	sock_reset_flag(newsk, SOCK_DONE);
+	skb_queue_head_init(&newsk->sk_error_queue);
+	
+	filter = newsk->sk_filter;
+	if (filter != NULL)
+		sk_filter_charge(newsk, filter);
+		
+	newsk->sk_err	   = 0;
+	newsk->sk_priority = 0;
+	atomic_set(&newsk->sk_refcnt, 2);
+	
+	/*
+	 * Increment the counter in the same struct proto as the master
+	 * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that
+	 * is the same as sk->sk_prot->socks, as this field was copied
+	 * with memcpy).
+	 *
+	 * This _changes_ the previous behaviour, where
+	 * tcp_create_openreq_child always was incrementing the
+	 * equivalent to tcp_prot->socks (inet_sock_nr), so this have
+	 * to be taken into account in all callers. -acme
+	 */
+	sk_refcnt_debug_inc(newsk);
+	/*MPTCP: make the mpcb_sk point to the same struct socket
+	  as the master subsocket. Same for sk_sleep*/
+	sk_set_socket(newsk, sk->sk_socket);
+	newsk->sk_sleep	 = sk->sk_sleep;
+	if (newsk->sk_prot->sockets_allocated)
+		atomic_inc(newsk->sk_prot->sockets_allocated);
+}
+
 static struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,
 		int family)
 {
@@ -1235,7 +1311,7 @@ static long sock_wait_for_wmem(struct sock * sk, long timeo)
 			break;
 		if (signal_pending(current))
 			break;
-		set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		set_bit(SOCK_NOSPACE, &sk->sock_flags);
 		prepare_to_wait(sk->sk_sleep, &wait, TASK_INTERRUPTIBLE);
 		if (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf)
 			break;
@@ -1319,7 +1395,7 @@ static struct sk_buff *sock_alloc_send_pskb(struct sock *sk,
 			goto failure;
 		}
 		set_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);
-		set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		set_bit(SOCK_NOSPACE, &sk->sock_flags);
 		err = -EAGAIN;
 		if (!timeo)
 			goto failure;
@@ -1360,13 +1436,18 @@ static void __lock_sock(struct sock *sk)
 	finish_wait(&sk->sk_lock.wq, &wait);
 }
 
-static void __release_sock(struct sock *sk)
+/**
+ *@meta_sk: used by MPCTP: if not NULL, it is the meta_sk to
+ *          which @sk is attached.
+ */
+static void __release_sock(struct sock *sk, struct sock *meta_sk)
 {
 	struct sk_buff *skb = sk->sk_backlog.head;
 
 	do {
 		sk->sk_backlog.head = sk->sk_backlog.tail = NULL;
 		bh_unlock_sock(sk);
+		if (meta_sk) bh_unlock_sock(meta_sk);
 
 		do {
 			struct sk_buff *next = skb->next;
@@ -1385,6 +1466,7 @@ static void __release_sock(struct sock *sk)
 			skb = next;
 		} while (skb != NULL);
 
+		if (meta_sk) bh_lock_sock(meta_sk);
 		bh_lock_sock(sk);
 	} while ((skb = sk->sk_backlog.head) != NULL);
 }
@@ -1630,7 +1712,7 @@ static void sock_def_error_report(struct sock *sk)
 }
 
 static void sock_def_readable(struct sock *sk, int len)
-{
+{       	
 	read_lock(&sk->sk_callback_lock);
 	if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
 		wake_up_interruptible_sync(sk->sk_sleep);
@@ -1672,6 +1754,7 @@ void sk_send_sigurg(struct sock *sk)
 void sk_reset_timer(struct sock *sk, struct timer_list* timer,
 		    unsigned long expires)
 {
+	BUG_ON(is_meta_sk(sk));
 	if (!mod_timer(timer, expires))
 		sock_hold(sk);
 }
@@ -1769,11 +1852,39 @@ void release_sock(struct sock *sk)
 
 	spin_lock_bh(&sk->sk_lock.slock);
 	if (sk->sk_backlog.tail)
-		__release_sock(sk);
+		__release_sock(sk, NULL);
+	if (is_meta_sk(sk)) {
+		struct sock *sk_it;
+		struct tcp_sock *tp_it;
+		/*We need to do the following, because as far
+		  as the meta-socket is locked, every received segment is
+		  put into the backlog queue.*/
+		do {
+			mtcp_for_each_sk((struct multipath_pcb *)sk,sk_it,
+					 tp_it) {
+				/*We do not use _bh here, since bh is already
+				  disabled by the previous spin_lock_bh*/
+				spin_lock(&sk_it->sk_lock.slock);
+				if (sk_it->sk_backlog.tail)
+					__release_sock(sk_it,sk);
+				spin_unlock(&sk_it->sk_lock.slock);
+			}
+		}
+		while (mtcp_test_any_sk((struct multipath_pcb*)sk,sk_it,
+					sk_it->sk_backlog.head));
+		/*The while above is needed because, during we eat the content
+		  of the backlog for one subflow, the backlog for another one 
+		  can receive segments*/
+	}
 	sk->sk_lock.owned = 0;
 	if (waitqueue_active(&sk->sk_lock.wq))
 		wake_up(&sk->sk_lock.wq);
 	spin_unlock_bh(&sk->sk_lock.slock);
+
+	if ((sk->sk_protocol==IPPROTO_TCP || sk->sk_protocol==IPPROTO_MTCPSUB)
+	    && tcp_sk(sk)->push_frames) {
+		mtcp_push_frames(sk);
+	}
 }
 EXPORT_SYMBOL(release_sock);
 
diff --git a/net/core/stream.c b/net/core/stream.c
index 8727cea..1db9f25 100644
--- a/net/core/stream.c
+++ b/net/core/stream.c
@@ -18,6 +18,7 @@
 #include <linux/tcp.h>
 #include <linux/wait.h>
 #include <net/sock.h>
+#include <linux/tcp_probe.h>
 
 /**
  * sk_stream_write_space - stream socket write_space callback.
@@ -30,7 +31,7 @@ void sk_stream_write_space(struct sock *sk)
 	struct socket *sock = sk->sk_socket;
 
 	if (sk_stream_wspace(sk) >= sk_stream_min_wspace(sk) && sock) {
-		clear_bit(SOCK_NOSPACE, &sock->flags);
+		clear_bit(SOCK_NOSPACE, &sk->sock_flags);
 
 		if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
 			wake_up_interruptible(sk->sk_sleep);
@@ -56,14 +57,21 @@ int sk_stream_wait_connect(struct sock *sk, long *timeo_p)
 
 	do {
 		int err = sock_error(sk);
-		if (err)
+		if (err) {
+			printk(KERN_ERR "line %d, err %d\n",__LINE__,err);
 			return err;
+		}
 		if ((1 << sk->sk_state) & ~(TCPF_SYN_SENT | TCPF_SYN_RECV))
 			return -EPIPE;
-		if (!*timeo_p)
+
+		if (!*timeo_p) {
+			printk(KERN_ERR "line %d\n",__LINE__);
 			return -EAGAIN;
-		if (signal_pending(tsk))
+		}
+		if (signal_pending(tsk)) {
+			printk(KERN_ERR "line %d\n",__LINE__);
 			return sock_intr_errno(*timeo_p);
+		}
 
 		prepare_to_wait(sk->sk_sleep, &wait, TASK_INTERRUPTIBLE);
 		sk->sk_write_pending++;
@@ -137,7 +145,7 @@ int sk_stream_wait_memory(struct sock *sk, long *timeo_p)
 		if (sk_stream_memory_free(sk) && !vm_wait)
 			break;
 
-		set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		set_bit(SOCK_NOSPACE, &sk->sock_flags);
 		sk->sk_write_pending++;
 		sk_wait_event(sk, &current_timeo, !sk->sk_err &&
 						  !(sk->sk_shutdown & SEND_SHUTDOWN) &&
diff --git a/net/dccp/proto.c b/net/dccp/proto.c
index d0bd348..b474501 100644
--- a/net/dccp/proto.c
+++ b/net/dccp/proto.c
@@ -379,7 +379,7 @@ unsigned int dccp_poll(struct file *file, struct socket *sock,
 			} else {  /* send SIGIO later */
 				set_bit(SOCK_ASYNC_NOSPACE,
 					&sk->sk_socket->flags);
-				set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+				set_bit(SOCK_NOSPACE, &sk->sock_flags);
 
 				/* Race breaker. If space is freed after
 				 * wspace test but before the flags are set,
diff --git a/net/ipv4/Kconfig b/net/ipv4/Kconfig
index 691268f..2bf4995 100644
--- a/net/ipv4/Kconfig
+++ b/net/ipv4/Kconfig
@@ -291,6 +291,28 @@ config ARPD
 	  and you should also say Y to "Kernel/User network link driver",
 	  below. If unsure, say N.
 
+config MTCP
+       bool "MTCP protocol (EXPERIMENTAL)"
+       depends on EXPERIMENTAL && !SYN_COOKIES && !NET_DMA && !TCP_MD5SIG
+       ---help---
+	This replaces the normal TCP stack with a Multipath TCP stack,
+	able to use several paths at once.
+
+config MTCP_PM
+       bool "built-in MTCP Path Manager"
+       depends on MTCP
+       ---help---
+       This is a built-in path manager for MTCP. It uses
+       TCP options to carry information about the multiple paths, and
+       assumes that several IPv4 addresses are available.
+
+config MTCP_DEBUG
+       bool "MTCP debug messages"
+       depends on MTCP
+       default y
+       ---help---
+       Enables logging of mtcp related debugging messages. Highly recommended.	
+
 config SYN_COOKIES
 	bool "IP: TCP syncookie support (disabled per default)"
 	---help---
diff --git a/net/ipv4/Makefile b/net/ipv4/Makefile
index 80ff87c..8a9c86b 100644
--- a/net/ipv4/Makefile
+++ b/net/ipv4/Makefile
@@ -1,4 +1,3 @@
-#
 # Makefile for the Linux TCP/IP (INET) layer.
 #
 
@@ -11,7 +10,7 @@ obj-y     := route.o inetpeer.o protocol.o \
 	     datagram.o raw.o udp.o udplite.o \
 	     arp.o icmp.o devinet.o af_inet.o  igmp.o \
 	     fib_frontend.o fib_semantics.o \
-	     inet_fragment.o
+	     inet_fragment.o 
 
 obj-$(CONFIG_SYSCTL) += sysctl_net_ipv4.o
 obj-$(CONFIG_IP_FIB_HASH) += fib_hash.o
@@ -22,6 +21,8 @@ obj-$(CONFIG_IP_MROUTE) += ipmr.o
 obj-$(CONFIG_NET_IPIP) += ipip.o
 obj-$(CONFIG_NET_IPGRE) += ip_gre.o
 obj-$(CONFIG_SYN_COOKIES) += syncookies.o
+obj-$(CONFIG_MTCP) += mtcp.o mtcp_ipv4.o pm.o
+obj-$(CONFIG_MTCP_PM) += mtcp_pm.o
 obj-$(CONFIG_INET_AH) += ah4.o
 obj-$(CONFIG_INET_ESP) += esp4.o
 obj-$(CONFIG_INET_IPCOMP) += ipcomp.o
@@ -50,5 +51,7 @@ obj-$(CONFIG_TCP_CONG_YEAH) += tcp_yeah.o
 obj-$(CONFIG_TCP_CONG_ILLINOIS) += tcp_illinois.o
 obj-$(CONFIG_NETLABEL) += cipso_ipv4.o
 
+obj-y += tcp_probe_static.o
+
 obj-$(CONFIG_XFRM) += xfrm4_policy.o xfrm4_state.o xfrm4_input.o \
 		      xfrm4_output.o
diff --git a/net/ipv4/af_inet.c b/net/ipv4/af_inet.c
index 1aa2dc9..2d6c1e9 100644
--- a/net/ipv4/af_inet.c
+++ b/net/ipv4/af_inet.c
@@ -101,6 +101,7 @@
 #include <net/ip_fib.h>
 #include <net/inet_connection_sock.h>
 #include <net/tcp.h>
+#include <net/mtcp.h>
 #include <net/udp.h>
 #include <net/udplite.h>
 #include <linux/skbuff.h>
@@ -143,6 +144,11 @@ void inet_sock_destruct(struct sock *sk)
 		       sk->sk_state, sk);
 		return;
 	}
+
+	if ((sk->sk_protocol==IPPROTO_TCP || sk->sk_protocol==IPPROTO_MTCPSUB)
+	    && tcp_sk(sk)->mpcb)
+		kref_put(&tcp_sk(sk)->mpcb->kref,mpcb_release);
+	
 	if (!sock_flag(sk, SOCK_DEAD)) {
 		printk("Attempt to release alive inet socket %p\n", sk);
 		return;
@@ -413,7 +419,7 @@ out_rcu_unlock:
 int inet_release(struct socket *sock)
 {
 	struct sock *sk = sock->sk;
-
+	
 	if (sk) {
 		long timeout;
 
@@ -514,6 +520,9 @@ int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
 	inet->daddr = 0;
 	inet->dport = 0;
 	sk_dst_reset(sk);
+#ifdef CONFIG_MTCP
+	if (addr->sin_addr.s_addr) mtcp_update_metasocket(sk);
+#endif
 	err = 0;
 out_release_sock:
 	release_sock(sk);
@@ -853,7 +862,11 @@ const struct proto_ops inet_stream_ops = {
 	.shutdown	   = inet_shutdown,
 	.setsockopt	   = sock_common_setsockopt,
 	.getsockopt	   = sock_common_getsockopt,
+#ifdef CONFIG_MTCP
+	.sendmsg	   = mtcp_sendmsg,
+#else
 	.sendmsg	   = tcp_sendmsg,
+#endif
 	.recvmsg	   = sock_common_recvmsg,
 	.mmap		   = sock_no_mmap,
 	.sendpage	   = tcp_sendpage,
@@ -959,7 +972,19 @@ static struct inet_protosw inetsw_array[] =
 	       .capability = CAP_NET_RAW,
 	       .no_check =   UDP_CSUM_DEFAULT,
 	       .flags =      INET_PROTOSW_REUSE,
-       }
+       },
+#ifdef CONFIG_MTCP
+	{
+		.type =       SOCK_STREAM,
+		.protocol =   IPPROTO_MTCPSUB,
+		.prot =       &mtcpsub_prot,
+		.ops =        &inet_stream_ops,
+		.capability = -1,
+		.no_check =   0,
+		.flags =      INET_PROTOSW_PERMANENT |
+		              INET_PROTOSW_ICSK,
+	},
+#endif
 };
 
 #define INETSW_ARRAY_LEN ARRAY_SIZE(inetsw_array)
@@ -1434,6 +1459,12 @@ static int __init inet_init(void)
 	if (rc)
 		goto out_unregister_udp_proto;
 
+#ifdef CONFIG_MTCP
+	rc = proto_register(&mtcpsub_prot, 1);
+	if (rc) 
+		goto out_unregister_raw_proto;
+#endif
+
 	/*
 	 *	Tell SOCKET that we are alive...
 	 */
@@ -1519,6 +1550,8 @@ static int __init inet_init(void)
 	rc = 0;
 out:
 	return rc;
+out_unregister_raw_proto:
+	proto_unregister(&raw_prot);
 out_unregister_udp_proto:
 	proto_unregister(&udp_prot);
 out_unregister_tcp_proto:
diff --git a/net/ipv4/inet_connection_sock.c b/net/ipv4/inet_connection_sock.c
index bd1278a..c925516 100644
--- a/net/ipv4/inet_connection_sock.c
+++ b/net/ipv4/inet_connection_sock.c
@@ -23,6 +23,8 @@
 #include <net/route.h>
 #include <net/tcp_states.h>
 #include <net/xfrm.h>
+#include <net/tcp.h>
+#include <net/mtcp.h>
 
 #ifdef INET_CSK_DEBUG
 const char inet_csk_timer_bug_msg[] = "inet_csk BUG: unknown timer value\n";
@@ -169,7 +171,7 @@ tb_not_found:
 success:
 	if (!inet_csk(sk)->icsk_bind_hash)
 		inet_bind_hash(sk, tb, snum);
-	WARN_ON(inet_csk(sk)->icsk_bind_hash != tb);
+	BUG_ON(inet_csk(sk)->icsk_bind_hash != tb);
 	ret = 0;
 
 fail_unlock:
@@ -263,6 +265,44 @@ struct sock *inet_csk_accept(struct sock *sk, int flags, int *err)
 
 	newsk = reqsk_queue_get_child(&icsk->icsk_accept_queue, sk);
 	WARN_ON(newsk->sk_state == TCP_SYN_RECV);
+
+#ifdef CONFIG_MTCP
+	/*We must have inherited our bind bucket form our father, otherwise
+	  Our slave subsockets will trigger a segfault when calling
+	  __inet_inherit_port*/
+	BUG_ON(!inet_csk(newsk)->icsk_bind_hash);
+
+	/*Init the MTCP mpcb - we need this because when doing 
+	  an accept the init function (e.g. tcp_v6_init_sock for tcp ipv6)
+	  is not called*/
+	if (newsk->sk_protocol==IPPROTO_TCP) {
+		struct tcp_sock *tp=tcp_sk(newsk);
+		struct multipath_pcb *mpcb;
+		struct tcp_sock *mpcb_tp;
+		
+		lock_sock(newsk);
+		mpcb=mtcp_alloc_mpcb(newsk);
+		mpcb_tp=(struct tcp_sock *)mpcb;
+		BUG_ON(!mpcb);
+		if (tp->mopt.list_rcvd) {
+			memcpy(&mpcb->received_options,&tp->mopt,
+			       sizeof(tp->mopt));
+		}
+		set_bit(MPCB_FLAG_SERVER_SIDE,&mpcb->flags);
+		tp->path_index=0;		
+		mtcp_add_sock(mpcb,tp);
+		mtcp_update_metasocket(newsk);
+		mpcb_tp->write_seq=0; /*first byte is IDSN
+					To be replaced later with a random IDSN
+					(well, if it indeed improve security)*/
+		
+		mpcb_tp->copied_seq=0; /* First byte of yet unread data */
+		set_bit(MPCB_FLAG_SERVER_SIDE,&mpcb->flags);
+		mtcp_ask_update(newsk);
+		release_sock(newsk);
+	}
+#endif
+
 out:
 	release_sock(sk);
 	return newsk;
@@ -543,15 +583,29 @@ EXPORT_SYMBOL_GPL(inet_csk_clone);
  * try to jump onto it.
  */
 void inet_csk_destroy_sock(struct sock *sk)
-{
+{	
+#ifdef CONFIG_MTCP
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tcp_sk(sk));
+
+	mtcp_debug("%s: Removing subsocket - pi:%d\n",__FUNCTION__,
+			tcp_sk(sk)->path_index);
+
+	BUG_ON(!mpcb && !tcp_sk(sk)->pending);
+	/*mpcb is NULL if the socket is the child subsocket
+	  waiting in the accept queue of the mpcb.
+	  Child subsockets are not yet attached to the mpcb.
+	  (they will be upon removal in mtcp_check_new_subflow())*/
+	if (mpcb) mtcp_del_sock(mpcb,tcp_sk(sk));
+#endif   
+	
 	WARN_ON(sk->sk_state != TCP_CLOSE);
 	WARN_ON(!sock_flag(sk, SOCK_DEAD));
-
+	
 	/* It cannot be in hash table! */
 	WARN_ON(!sk_unhashed(sk));
-
+	
 	/* If it has not 0 inet_sk(sk)->num, it must be bound */
-	WARN_ON(inet_sk(sk)->num && !inet_csk(sk)->icsk_bind_hash);
+	BUG_ON(inet_sk(sk)->num && !inet_csk(sk)->icsk_bind_hash);
 
 	sk->sk_prot->destroy(sk);
 
diff --git a/net/ipv4/inet_hashtables.c b/net/ipv4/inet_hashtables.c
index 4498190..abb61c3 100644
--- a/net/ipv4/inet_hashtables.c
+++ b/net/ipv4/inet_hashtables.c
@@ -454,7 +454,7 @@ int __inet_hash_connect(struct inet_timewait_death_row *death_row,
 					if (tb->fastreuse >= 0)
 						goto next_port;
 					if (!check_established(death_row, sk,
-								port, &tw))
+							       port, &tw))
 						goto ok;
 					goto next_port;
 				}
@@ -499,7 +499,9 @@ ok:
 	head = &hinfo->bhash[inet_bhashfn(net, snum, hinfo->bhash_size)];
 	tb  = inet_csk(sk)->icsk_bind_hash;
 	spin_lock_bh(&head->lock);
-	if (sk_head(&tb->owners) == sk && !sk->sk_bind_node.next) {
+
+	if (sk->sk_protocol==IPPROTO_MTCPSUB ||
+	    (sk_bind_head(&tb->owners) == sk && !sk->sk_bind_node.next)) {
 		hash(sk);
 		spin_unlock_bh(&head->lock);
 		return 0;
diff --git a/net/ipv4/ip_output.c b/net/ipv4/ip_output.c
index d2a8f8b..ec6e49f 100644
--- a/net/ipv4/ip_output.c
+++ b/net/ipv4/ip_output.c
@@ -160,6 +160,12 @@ int ip_build_and_send_pkt(struct sk_buff *skb, struct sock *sk,
 	iph->protocol = sk->sk_protocol;
 	ip_select_ident(iph, &rt->u.dst, sk);
 
+#ifdef CONFIG_MTCP
+	/*MPTCP hack : see comment in ip6_xmit (ip6_output.c)*/
+	if (iph->protocol==IPPROTO_MTCPSUB)
+		iph->protocol=IPPROTO_TCP;
+#endif
+
 	if (opt && opt->optlen) {
 		iph->ihl += opt->optlen>>2;
 		ip_options_build(skb, opt, daddr, rt, 0);
@@ -220,6 +226,8 @@ static inline int ip_skb_dst_mtu(struct sk_buff *skb)
 	       skb->dst->dev->mtu : dst_mtu(skb->dst);
 }
 
+#include <net/mtcp.h> /*TODEL*/
+
 static int ip_finish_output(struct sk_buff *skb)
 {
 #if defined(CONFIG_NETFILTER) && defined(CONFIG_XFRM)
@@ -376,6 +384,12 @@ packet_routed:
 	iph->daddr    = rt->rt_dst;
 	/* Transport layer set skb->h.foo itself. */
 
+#ifdef CONFIG_MTCP
+	/*MPTCP hack : see comment in ip6_xmit (ip6_output.c)*/
+	if (iph->protocol==IPPROTO_MTCPSUB)
+		iph->protocol=IPPROTO_TCP;
+#endif
+
 	if (opt && opt->optlen) {
 		iph->ihl += opt->optlen >> 2;
 		ip_options_build(skb, opt, inet->daddr, rt, 0);
diff --git a/net/ipv4/mtcp.c b/net/ipv4/mtcp.c
new file mode 100644
index 0000000..2635655
--- /dev/null
+++ b/net/ipv4/mtcp.c
@@ -0,0 +1,1904 @@
+/*
+ *	MPTCP implementation
+ *
+ *	Authors:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *      Partially inspired from initial user space MPTCP stack by Costin Raiciu.
+ *
+ *      date : June 10
+ *
+ *      Important note:
+ *            When one wants to add support for closing subsockets *during*
+ *             a communication, he must ensure that all skbs belonging to
+ *             that socket are removed from the meta-queues. Failing
+ *             to do this would lead to General Protection Fault.
+ *             See also comment in function mtcp_destroy_mpcb().
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+
+#include <net/sock.h>
+#include <net/tcp_states.h>
+#include <net/mtcp.h>
+#include <net/netevent.h>
+#include <net/ipv6.h>
+#include <net/tcp.h>
+#include <linux/list.h>
+#include <linux/jhash.h>
+#include <linux/tcp.h>
+#include <linux/net.h>
+#include <linux/in.h>
+#include <linux/random.h>
+#include <linux/inetdevice.h>
+#include <asm/atomic.h>
+#ifdef CONFIG_SYSCTL
+#include <linux/sysctl.h>
+#endif
+
+/*=====================================*/
+/*DEBUGGING*/
+
+#ifdef MTCP_RCV_QUEUE_DEBUG
+struct mtcp_debug mtcp_debug_array1[1000];
+struct mtcp_debug mtcp_debug_array2[1000];
+
+void print_debug_array(void)
+{
+	int i;
+	printk(KERN_ERR "debug array, path index 1:\n");
+	for (i=0;i<1000 && mtcp_debug_array1[i-1].end==0;i++) {
+		printk(KERN_ERR "\t%s:skb %x, len %d\n",
+		       mtcp_debug_array1[i].func_name,
+		       mtcp_debug_array1[i].seq,
+		       mtcp_debug_array1[i].len);
+	}
+	printk(KERN_ERR "debug array, path index 2:\n");
+	for (i=0;i<1000 && mtcp_debug_array2[i-1].end==0;i++) {
+		printk(KERN_ERR "\t%s:skb %x, len %d\n",
+		       mtcp_debug_array2[i].func_name,
+		       mtcp_debug_array2[i].seq,
+		       mtcp_debug_array2[i].len);
+	}
+}
+
+void freeze_rcv_queue(struct sock *sk, const char *func_name)
+{
+	int i;
+	struct sk_buff *skb;	
+	struct tcp_sock *tp=tcp_sk(sk);
+	int path_index=tp->path_index;
+	struct mtcp_debug *mtcp_debug_array;
+
+	if (path_index==0 || path_index==1)
+		mtcp_debug_array=mtcp_debug_array1;
+	else
+		mtcp_debug_array=mtcp_debug_array2;
+	for (skb=skb_peek(&sk->sk_receive_queue),i=0;
+	     skb && skb!=(struct sk_buff*)&sk->sk_receive_queue;
+	     skb=skb->next,i++) {
+		mtcp_debug_array[i].func_name=func_name;
+		mtcp_debug_array[i].seq=TCP_SKB_CB(skb)->seq;
+		mtcp_debug_array[i].len=skb->len;			
+		mtcp_debug_array[i].end=0;
+		BUG_ON(i>=999);
+	}
+	if (i>0) mtcp_debug_array[i-1].end=1;
+	else {
+		mtcp_debug_array[0].func_name="NO_FUNC";
+		mtcp_debug_array[0].end=1;
+	}
+}
+
+#endif
+/*=====================================*/
+
+/*Sysctl data*/
+
+#ifdef CONFIG_SYSCTL
+
+int sysctl_mptcp_mss = MPTCP_MSS;
+
+static ctl_table mptcp_table[] = {
+	{
+		.ctl_name	= CTL_UNNUMBERED,
+		.procname	= "mptcp_mss",
+		.data		= &sysctl_mptcp_mss,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec
+	},
+	{ .ctl_name = 0 },
+};
+
+static ctl_table mptcp_net_table[] = {
+	{
+		.ctl_name       = CTL_UNNUMBERED,
+		.procname       = "mptcp",
+		.maxlen         = 0,
+		.mode           = 0555,
+		.child          = mptcp_table
+	},
+	{.ctl_name = 0},
+};
+
+static ctl_table mptcp_root_table[] = {
+	{
+		.ctl_name	= CTL_NET,
+		.procname	= "net",
+		.mode		= 0555,
+		.child		= mptcp_net_table
+	},
+        { .ctl_name = 0 }
+};
+#endif
+
+/**
+ * Equivalent of tcp_fin() for MPTCP
+ * Can be called only when the FIN is validly part
+ * of the data seqnum space. Not before when we get holes.
+ */
+static void mtcp_fin(struct sk_buff *skb, struct multipath_pcb *mpcb)
+{
+	struct sock *mpcb_sk=(struct sock*) mpcb;
+	if (is_dfin_seg(mpcb,skb)) {
+		mpcb_sk->sk_shutdown |= RCV_SHUTDOWN;
+		sock_set_flag(mpcb_sk, SOCK_DONE);
+		if (mpcb_sk->sk_state==TCP_ESTABLISHED)
+			tcp_set_state(mpcb_sk,TCP_CLOSE_WAIT);
+	}
+}
+
+static void mtcp_def_readable(struct sock *sk, int len)
+{
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tcp_sk(sk));
+	struct sock *msk=mpcb->master_sk;
+	
+	BUG_ON(!mpcb);
+
+	mtcp_debug("Waking up master subsock...\n");
+	
+	read_lock(&msk->sk_callback_lock);
+	if (msk->sk_sleep && waitqueue_active(msk->sk_sleep))
+		wake_up_interruptible_sync(msk->sk_sleep);
+	sk_wake_async(msk, SOCK_WAKE_WAITD, POLL_IN);
+	read_unlock(&msk->sk_callback_lock);
+}
+
+void mtcp_data_ready(struct sock *sk)
+{
+	struct tcp_sock *tp=tcp_sk(sk);
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+
+	if (mpcb)
+		mpcb->master_sk->sk_data_ready(mpcb->master_sk, 0);
+#ifdef CONFIG_MTCP_PM
+	else {
+		/*This tp is not yet attached to the mpcb*/
+		BUG_ON(!tp->pending);
+		mpcb=mtcp_hash_find(tp->mtcp_loc_token);
+		BUG_ON(!mpcb);
+		mpcb->master_sk->sk_data_ready(mpcb->master_sk, 0);
+		mpcb_put(mpcb);
+	}
+#endif
+}
+
+/**
+ * Creates as many sockets as path indices announced by the Path Manager.
+ * The first path indices are (re)allocated to existing sockets.
+ * New sockets are created if needed.
+ * Note that this is called only at client side.
+ * Server calls mtcp_check_new_subflow().
+ *
+ *
+ * WARNING: We make the assumption that this function is run in user context
+ *      (we use sock_create_kern, that reserves ressources with GFP_KERNEL)
+ *      AND only one user process can trigger the sending of a PATH_UPDATE
+ *      notification. This is in conformance with the fact that only one PM
+ *      can send messages to the MPS, according to our multipath arch.
+ *      (further PMs are cascaded and use the depth attribute).
+ */
+int mtcp_init_subsockets(struct multipath_pcb *mpcb, 
+			 uint32_t path_indices)
+{
+	int i;
+	int retval;
+	struct socket *sock;
+	struct tcp_sock *tp=mpcb->connection_list;
+	struct sock *mpcb_sk=(struct sock *)mpcb;
+	struct tcp_sock *newtp;
+
+	BUG_ON(!tcp_sk(mpcb->master_sk)->mpc);
+	
+	/*First, ensure that we keep existing path indices.*/
+	while (tp!=NULL) {
+		/*disable the corresponding bit*/
+		path_indices&=~PI_TO_FLAG(tp->path_index);
+		tp=tp->next;
+	}
+	
+	for (i=0;i<sizeof(path_indices)*8;i++) {
+		if (!((1<<i) & path_indices))
+			continue;
+		else {
+			struct sockaddr *loculid,*remulid=NULL;
+			int ulid_size=0;
+			struct sockaddr_in loculid_in,remulid_in;
+			struct sockaddr_in6 loculid_in6,remulid_in6;
+			int newpi=i+1;
+			/*a new socket must be created*/
+			retval = sock_create_kern(mpcb_sk->sk_family, 
+						  SOCK_STREAM, 
+						  IPPROTO_MTCPSUB, &sock);
+			if (retval<0) {
+				printk(KERN_ERR "%s:sock_create failed\n",
+				       __FUNCTION__);
+				return retval;
+			}
+			newtp=tcp_sk(sock->sk);
+
+			/*Binding the new socket to the local ulid
+			  (except if we use the MPTCP default PM, in which
+			  case we bind the new socket, directly to its
+			  corresponding locators)*/
+			switch(mpcb_sk->sk_family) {
+			case AF_INET:
+				memset(&loculid,0,sizeof(loculid));
+				loculid_in.sin_family=mpcb_sk->sk_family;
+				
+				memcpy(&remulid_in,&loculid_in,
+				       sizeof(remulid_in));
+				
+				loculid_in.sin_port=mpcb->local_port;
+				remulid_in.sin_port=mpcb->remote_port;
+#ifdef CONFIG_MTCP_PM
+				/*If the MPTCP PM is used, we use the locators 
+				  as subsock ids, while with other PMs, the
+				  ULIDs are those of the master subsock
+				  for all subsocks.*/
+				memcpy(&loculid_in.sin_addr,
+				       mtcp_get_loc_addr(mpcb,newpi),
+				       sizeof(struct in_addr));
+				memcpy(&remulid_in.sin_addr,
+				       mtcp_get_rem_addr(mpcb,newpi),
+				       sizeof(struct in_addr));
+#else
+				memcpy(&loculid_in.sin_addr,
+				       (struct in_addr*)&mpcb->local_ulid.a4,
+				       sizeof(struct in_addr));
+				memcpy(&remulid_in.sin_addr,
+				       (struct in_addr*)&mpcb->remote_ulid.a4,
+				       sizeof(struct in_addr));
+#endif
+				loculid=(struct sockaddr *)&loculid_in;
+				remulid=(struct sockaddr *)&remulid_in;
+				ulid_size=sizeof(loculid_in);
+				break;
+			case AF_INET6:
+				memset(&loculid,0,sizeof(loculid));
+				loculid_in6.sin6_family=mpcb_sk->sk_family;
+				
+				memcpy(&remulid_in6,&loculid_in6,
+				       sizeof(remulid_in6));
+
+				loculid_in6.sin6_port=mpcb->local_port;
+				remulid_in6.sin6_port=mpcb->remote_port;
+				ipv6_addr_copy(&loculid_in6.sin6_addr,
+					       (struct in6_addr*)&mpcb->
+					       local_ulid.a6);
+				ipv6_addr_copy(&remulid_in6.sin6_addr,
+					       (struct in6_addr*)&mpcb->
+					       remote_ulid.a6);
+				
+				loculid=(struct sockaddr *)&loculid_in6;
+				remulid=(struct sockaddr *)&remulid_in6;
+				ulid_size=sizeof(loculid_in6);
+				break;
+			default:
+				BUG();
+			}
+			newtp->path_index=newpi;
+			newtp->mpc=1;
+			newtp->slave_sk=1;
+			
+			mtcp_add_sock(mpcb,newtp);
+						
+			/*Redefine the sk_data_ready function*/
+			((struct sock*)newtp)->sk_data_ready=mtcp_def_readable;
+						
+			retval = sock->ops->bind(sock, loculid, ulid_size);
+			if (retval<0) goto fail_bind;
+			
+			retval = sock->ops->connect(sock,remulid,
+						    ulid_size,O_NONBLOCK);
+			if (retval<0 && retval != -EINPROGRESS) 
+				goto fail_connect;
+			
+			mtcp_debug("New MTCP subsocket created, pi %d src_addr:"
+                                   NIPQUAD_FMT " dst_addr:" NIPQUAD_FMT " \n",
+                                   newpi, NIPQUAD(loculid_in.sin_addr),
+                                   NIPQUAD(remulid_in.sin_addr));
+		}
+	}
+
+	return 0;
+	
+fail_bind:
+	printk(KERN_ERR "MTCP subsocket bind() failed\n");
+fail_connect:
+	printk(KERN_ERR "MTCP subsocket connect() failed, error %d\n", 
+	       retval);
+	/*sock_release will indirectly call mtcp_del_sock()*/
+	sock_release(sock);
+	return -1;
+}
+
+static int netevent_callback(struct notifier_block *self, unsigned long event,
+			     void *ctx)
+{	
+	struct multipath_pcb *mpcb;
+	struct sock *mpcb_sk;
+	struct ulid_pair *up;
+	switch(event) {
+	case NETEVENT_PATH_UPDATEV6:
+		mtcp_debug("%s: Received path update event: %lu\n",__FUNCTION__, event);
+		mpcb=container_of(self,struct multipath_pcb,nb);
+		mpcb_sk=(struct sock*)mpcb;
+		up=ctx;
+		mtcp_debug("mpcb is %p\n",mpcb);
+		if (mpcb_sk->sk_family!=AF_INET6) break;
+		
+		mtcp_debug("ev loc ulid:" NIP6_FMT "\n",NIP6(*up->local));
+		mtcp_debug("ev loc ulid:" NIP6_FMT "\n",NIP6(*up->remote));
+		mtcp_debug("ev loc ulid:" NIP6_FMT "\n",NIP6(*(struct in6_addr*)mpcb->local_ulid.a6));
+		mtcp_debug("ev loc ulid:" NIP6_FMT "\n",NIP6(*(struct in6_addr*)mpcb->remote_ulid.a6));
+		if (ipv6_addr_equal(up->local,
+				    (struct in6_addr*)&mpcb->local_ulid) &&
+		    ipv6_addr_equal(up->remote,
+				    (struct in6_addr*)&mpcb->remote_ulid))
+			mtcp_init_subsockets(mpcb,
+					     up->path_indices);
+		break;
+        }
+        return 0;
+}
+
+/*Ask to the PM to be updated about available path indices
+ *
+ * The argument must be any TCP socket in established state
+ */
+void mtcp_ask_update(struct sock *sk)
+{
+	struct ulid_pair up;
+	struct tcp_sock *tp=tcp_sk(sk);
+
+	mtcp_debug("Entering %s\n",__FUNCTION__); /*TODEL*/
+
+	if (!is_master_sk(tp)) return;
+	/*Currently we only support AF_INET6*/
+	if (sk->sk_family!=AF_INET6) return;
+
+	up.local=&inet6_sk(sk)->saddr;
+	up.remote=&inet6_sk(sk)->daddr;
+	up.path_indices=0; /*This is what we ask for*/
+	call_netevent_notifiers(NETEVENT_MPS_UPDATEME, &up);
+}
+
+/*Defined in net/core/sock.c*/
+void mtcp_inherit_sk(struct sock *sk,struct sock *newsk);
+
+struct multipath_pcb* mtcp_alloc_mpcb(struct sock *master_sk)
+{
+	struct multipath_pcb * mpcb = kmalloc(
+		sizeof(struct multipath_pcb),GFP_KERNEL);
+	struct tcp_sock *mpcb_tp = &mpcb->tp;
+	struct sock *mpcb_sk = (struct sock *) mpcb_tp;
+	struct inet_connection_sock *mpcb_icsk = inet_csk(mpcb_sk);
+
+	memset(mpcb,0,sizeof(struct multipath_pcb));
+	BUG_ON(mpcb->connection_list);
+
+	/*mpcb_sk inherits master sk*/
+	mtcp_inherit_sk(master_sk,mpcb_sk);
+	BUG_ON(mpcb->connection_list);
+
+	/*Will be replaced by the IDSN later. Currently the 
+	  IDSN is zero*/
+	mpcb_tp->copied_seq = mpcb_tp->rcv_nxt = mpcb_tp->rcv_wup = 0;
+	mpcb_tp->snd_sml = mpcb_tp->snd_una = mpcb_tp->snd_nxt = 0;
+	
+	mpcb_tp->mpcb=mpcb;
+	mpcb_tp->mpc=1;
+	mpcb_tp->mss_cache=sysctl_mptcp_mss;
+
+	skb_queue_head_init(&mpcb_tp->out_of_order_queue);
+	skb_queue_head_init(&mpcb->reinject_queue);
+	
+	mpcb_sk->sk_rcvbuf = sysctl_rmem_default;
+	mpcb_sk->sk_sndbuf = sysctl_wmem_default;
+	mpcb_sk->sk_state = TCPF_CLOSE;
+	/*inherit locks the mpcb_sk, so we must release it here.*/
+	bh_unlock_sock(mpcb_sk);
+	sock_put(mpcb_sk);
+	
+	mpcb->master_sk=master_sk;
+
+	kref_init(&mpcb->kref);
+
+	spin_lock_init(&mpcb->lock);
+	mutex_init(&mpcb->mutex);
+	mpcb->nb.notifier_call=netevent_callback;
+	register_netevent_notifier(&mpcb->nb);
+	mpcb_tp->window_clamp=tcp_sk(master_sk)->window_clamp;
+	mpcb_tp->rcv_ssthresh=tcp_sk(master_sk)->rcv_ssthresh;
+	
+#ifdef CONFIG_MTCP_PM
+	/*Init the accept_queue structure, we support a queue of 4 pending
+	  connections, it does not need to be huge, since we only store 
+	  here pending subflow creations*/
+	reqsk_queue_alloc(&mpcb_icsk->icsk_accept_queue,32);
+	/*Pi 1 is reserved for the master subflow*/
+	mpcb->next_unused_pi=2;
+	/*For the server side, the local token has already been allocated*/
+	if (!tcp_sk(master_sk)->mtcp_loc_token)
+		tcp_sk(master_sk)->mtcp_loc_token=mtcp_new_token();
+
+	/*Adding the mpcb in the token hashtable*/
+	mtcp_hash_insert(mpcb,loc_token(mpcb));
+#endif
+		
+	return mpcb;
+}
+
+void mpcb_release(struct kref* kref)
+{
+	struct multipath_pcb *mpcb;
+	mpcb=container_of(kref,struct multipath_pcb,kref);
+	mutex_destroy(&mpcb->mutex);
+#ifdef CONFIG_MTCP_PM
+	mtcp_pm_release(mpcb);
+#endif
+	mtcp_debug("%s: Will free mpcb\n", __FUNCTION__);
+#ifdef CONFIG_SECURITY_NETWORK
+	security_sk_free((struct sock *)mpcb);
+#endif
+	kfree(mpcb);
+}
+
+void mpcb_get(struct multipath_pcb *mpcb)
+{
+	kref_get(&mpcb->kref);
+}
+void mpcb_put(struct multipath_pcb *mpcb)
+{
+	kref_put(&mpcb->kref,mpcb_release);
+}
+
+/*Warning: can only be called in user context
+  (due to unregister_netevent_notifier)*/
+void mtcp_destroy_mpcb(struct multipath_pcb *mpcb)
+{
+	mtcp_debug("%s: Destroying mpcb\n", __FUNCTION__);
+#ifdef CONFIG_MTCP_PM
+	/*Detach the mpcb from the token hashtable*/
+	mtcp_hash_remove(mpcb);
+#endif
+	/*Stop listening to PM events*/
+	unregister_netevent_notifier(&mpcb->nb);
+
+	kref_put(&mpcb->kref,mpcb_release);
+}
+
+/*MUST be called in user context
+ */
+void mtcp_add_sock(struct multipath_pcb *mpcb,struct tcp_sock *tp)
+{
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	struct sock *sk=(struct sock*)tp;
+	struct sk_buff *skb;
+
+	/*first subflow*/
+	if (!tp->path_index) tp->path_index=1;
+
+	/*Adding new node to head of connection_list*/
+	mutex_lock(&mpcb->mutex); /*To protect against concurrency with
+				    mtcp_recvmsg and mtcp_sendmsg*/
+	local_bh_disable(); /*To protect against concurrency with
+			      mtcp_del_sock*/
+	tp->mpcb = mpcb;
+	tp->next=mpcb->connection_list;
+	mpcb->connection_list=tp;
+
+#ifdef CONFIG_MTCP_PM
+	/*Same token for all subflows*/
+	tp->rx_opt.mtcp_rem_token=
+		tcp_sk(mpcb->master_sk)->rx_opt.mtcp_rem_token;
+	tp->pending=0;
+#endif
+	
+	mpcb->cnt_subflows++;
+	mtcp_update_window_clamp(mpcb);
+	atomic_add(atomic_read(&((struct sock *)tp)->sk_rmem_alloc),
+		   &mpcb_sk->sk_rmem_alloc);
+	
+	/*The socket is already established if it was in the
+	  accept queue of the mpcb*/
+	if (((struct sock*)tp)->sk_state==TCP_ESTABLISHED) {
+		mpcb->cnt_established++;
+		mtcp_update_sndbuf(mpcb);
+		mpcb_sk->sk_state=TCP_ESTABLISHED;
+	}
+	
+	kref_get(&mpcb->kref);
+
+	/*Empty the receive queue of the added new subsocket
+	  we do it with bh disabled, because before the mpcb is attached,
+	  all segs are received in subflow queue,and after the mpcb is 
+	  attached, all segs are received in meta-queue. So moving segments
+	  from subflow to meta-queue must be done atomically with the 
+	  setting of tp->mpcb.*/
+	if (tp->mpc)
+		while ((skb = skb_peek(&sk->sk_receive_queue))) {
+			__skb_unlink(skb, &sk->sk_receive_queue);
+			if (mtcp_queue_skb(sk,skb)==MTCP_EATEN)
+				__kfree_skb(skb);
+		}
+	local_bh_enable();
+	mutex_unlock(&mpcb->mutex);
+	
+	mtcp_debug("Added subsocket with pi %d, cnt_subflows now %d\n",
+	       tp->path_index,mpcb->cnt_subflows);
+}
+
+void mtcp_del_sock(struct multipath_pcb *mpcb, struct tcp_sock *tp)
+{
+	struct tcp_sock *tp_prev;
+	int done=0;
+
+	if (!in_interrupt()) {
+		/*Then we must take the mutex to avoid racing
+		  with mtcp_add_sock*/
+		mutex_lock(&mpcb->mutex);
+	}
+
+	tp_prev=mpcb->connection_list;	
+
+	if (tp_prev==tp) {
+		mpcb->connection_list=tp->next;
+		mpcb->cnt_subflows--;
+		done=1;
+	}
+	else for (;tp_prev && tp_prev->next;tp_prev=tp_prev->next) {
+			if (tp_prev->next==tp) {
+				tp_prev->next=tp->next;
+				mpcb->cnt_subflows--;
+				done=1;
+				break;
+			}
+		}
+
+	if ((struct sock *)tp==mpcb->master_sk)
+		mpcb->master_sk=NULL;
+
+	tp->next=NULL;
+	if (!in_interrupt())
+		mutex_unlock(&mpcb->mutex);
+	BUG_ON(!done);
+}
+
+/**
+ * Updates the metasocket ULID/port data, based on the given sock.
+ * The argument sock must be the sock accessible to the application.
+ * In this function, we update the meta socket info, based on the changes 
+ * in the application socket (bind, address allocation, ...)
+ */
+void mtcp_update_metasocket(struct sock *sk)
+{
+	struct tcp_sock *tp;
+	struct multipath_pcb *mpcb;
+	struct sock *mpcb_sk;
+	if (sk->sk_protocol != IPPROTO_TCP) return;
+	tp=tcp_sk(sk);
+	mpcb=mpcb_from_tcpsock(tp);
+	mpcb_sk=(struct sock*)mpcb;
+
+	mpcb_sk->sk_family=sk->sk_family;
+	mpcb->remote_port=inet_sk(sk)->dport;
+	mpcb->local_port=inet_sk(sk)->sport;
+	
+	switch (sk->sk_family) {
+	case AF_INET:
+		mpcb->remote_ulid.a4=inet_sk(sk)->daddr;
+		mpcb->local_ulid.a4=inet_sk(sk)->saddr;
+
+
+		mtcp_debug("%s: loc_ulid:"NIPQUAD_FMT " rem_ulid:" NIPQUAD_FMT
+				" \n", __FUNCTION__,
+				NIPQUAD(mpcb->local_ulid.a4),
+				NIPQUAD(mpcb->remote_ulid.a4));
+		break;
+	case AF_INET6:
+		ipv6_addr_copy((struct in6_addr*)&mpcb->remote_ulid,
+			       &inet6_sk(sk)->daddr);
+		ipv6_addr_copy((struct in6_addr*)&mpcb->local_ulid,
+			       &inet6_sk(sk)->saddr);
+
+		mtcp_debug("%s: mum loc ulid:" NIP6_FMT "\n", __FUNCTION__, NIP6(*(struct in6_addr*)mpcb->local_ulid.a6));
+		mtcp_debug("%s: mum loc ulid:" NIP6_FMT "\n", __FUNCTION__, NIP6(*(struct in6_addr*)mpcb->remote_ulid.a6));
+
+		break;
+	}
+#ifdef CONFIG_MTCP_PM
+	/*Searching for suitable local addresses,
+	  except is the socket is loopback, in which case we simply
+	  don't do multipath*/
+	if (!ipv4_is_loopback(inet_sk(sk)->saddr) &&
+	    !ipv4_is_loopback(inet_sk(sk)->daddr))
+		mtcp_set_addresses(mpcb);
+	/*If this added new local addresses, build new paths with them*/
+	if (mpcb->num_addr4 || mpcb->num_addr6) mtcp_update_patharray(mpcb);
+#endif	
+}
+
+/*copied from tcp_output.c*/
+static inline unsigned int tcp_cwnd_test(struct tcp_sock *tp)
+{
+	u32 in_flight, cwnd;
+
+	in_flight = tcp_packets_in_flight(tp);
+	cwnd = tp->snd_cwnd;
+	if (in_flight < cwnd)
+		return (cwnd - in_flight);
+
+	return 0;
+}
+
+int mtcp_is_available(struct sock *sk)
+{
+	/*Set of states for which we are allowed to send data*/
+	if ((1 << sk->sk_state) & 
+	    ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))
+		return 0;
+	if (tcp_sk(sk)->pf ||
+	    (tcp_sk(sk)->mpcb->noneligible & 
+	     PI_TO_FLAG(tcp_sk(sk)->path_index)) ||
+	    inet_csk(sk)->icsk_ca_state==TCP_CA_Loss)
+		return 0;
+	if (tcp_cwnd_test(tcp_sk(sk))) return 1;
+	return 0;
+}
+
+/**
+ *This is the scheduler. This function decides on which flow to send
+ *  a given MSS. If all subflows are found to be busy, NULL is returned
+ * The flow is selected based on the estimation of how much time will be
+ * needed to send the segment. If all paths have full cong windows, we
+ * simply block. The flow able to send the segment the soonest get it. 
+ * All subsocked must be locked before calling this function.
+ */
+struct sock* get_available_subflow(struct multipath_pcb *mpcb, 
+				   struct sk_buff *skb, int *pf)
+{
+	struct tcp_sock *tp;
+	struct sock *sk;
+	struct sock *bestsk=NULL;
+	unsigned int min_time_to_peer=0xffffffff;
+	int bh=in_interrupt(); 
+
+	if (!mpcb) return NULL;
+	
+	if (!bh)
+		mutex_lock(&mpcb->mutex);
+
+	/*if there is only one subflow, bypass the scheduling function*/
+	if (mpcb->cnt_subflows==1) {
+		bestsk=(struct sock *)mpcb->connection_list;
+		if (!mtcp_is_available(bestsk))
+			bestsk=NULL;
+		goto out;
+	}
+
+	/*First, find the best subflow*/
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		unsigned int time_to_peer;
+		if (pf && tp->pf) *pf|=PI_TO_FLAG(tp->path_index);
+		if (!mtcp_is_available(sk)) continue;
+		/*If the skb has already been enqueued in this sk, try to find
+		  another one*/
+		if (PI_TO_FLAG(tp->path_index) & skb->path_mask) continue;
+		
+		/*If there is no bw estimation available currently, 
+		  we only give it data when it has available space in the
+		  cwnd (see above)*/
+		if (!tp->cur_bw_est) {
+			/*If a subflow is available, send immediately*/
+			if (tcp_packets_in_flight(tp)<tp->snd_cwnd) {
+				bestsk=sk;
+				break;
+			}
+			else continue;
+		}
+		
+		/*Time to reach peer, estimated in units of jiffies*/
+		time_to_peer=
+			((sk->sk_wmem_queued/tp->cur_bw_est)<<
+			 tp->bw_est.shift)+ /*time to reach network*/
+			(tp->srtt>>3); /*Time to reach peer*/
+		
+		if (time_to_peer<min_time_to_peer) {
+			min_time_to_peer=time_to_peer;
+			bestsk=sk;
+		}
+	}
+	
+out:
+	if (!bh)
+		mutex_unlock(&mpcb->mutex);
+	return bestsk;
+}
+
+int mtcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
+		 size_t size)
+{
+	struct sock *master_sk = sock->sk;
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tcp_sk(master_sk));
+	struct sock *mpcb_sk=(struct sock *) mpcb;
+	size_t copied = 0;
+	int err;
+	int flags = msg->msg_flags;
+	long timeo = sock_sndtimeo(master_sk, flags & MSG_DONTWAIT);
+
+	/*At the moment it should be sure 100% that the mpcb pointer is defined
+	  because at the client, alloc_mpcb is called in tcp_v4_init_sock(),
+	  at the server it is called in inet_csk_accept(). sendmsg() can be
+	  called before none of these functions.*/
+	BUG_ON(!mpcb);
+
+	lock_sock(master_sk);	
+
+	/*If the master sk is not yet established, we need to wait
+	  until the establishment, so as to know whether the mpc option
+	  is present.*/
+	if (!tcp_sk(master_sk)->mpc) {
+		if ((1 << master_sk->sk_state) & 
+		    ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) {
+			err = sk_stream_wait_connect(master_sk,
+						     &timeo);
+			if (err) {
+				printk(KERN_ERR "err is %d, state %d\n",err,
+				       master_sk->sk_state);
+				goto out_err_nompc;
+			}
+			/*The flag mast be re-checked, because it may have
+			  appeared during sk_stream_wait_connect*/
+			if (!tcp_sk(master_sk)->mpc) {
+				copied=subtcp_sendmsg(iocb,master_sk, msg, 
+						      size);
+				goto out_nompc;
+			}
+			
+		}
+		else {
+			copied=subtcp_sendmsg(iocb,master_sk, msg, size);
+			goto out_nompc;
+		}
+	}
+
+	release_sock(master_sk);
+	lock_sock(mpcb_sk);
+
+	verif_wqueues(mpcb);
+
+#ifdef CONFIG_MTCP_PM
+	/*Any new subsock we can use ?*/
+	mtcp_check_new_subflow(mpcb);
+#endif
+	
+	/* Compute the total number of bytes stored in the message*/	
+	copied = subtcp_sendmsg(NULL,mpcb_sk,msg, 0);
+	if (copied<0) {
+		printk(KERN_ERR "%s: returning error "
+		       "to app:%d\n",__FUNCTION__,(int)copied);
+		goto out_mpc;
+	}
+	
+out_mpc:
+	release_sock(mpcb_sk);
+	return copied;
+out_nompc:
+ 	release_sock(master_sk);
+	return copied;
+out_err_nompc:
+	err = sk_stream_error(master_sk, flags, err);
+	TCP_CHECK_TIMER(master_sk);
+	release_sock(master_sk);
+	return err;
+}
+
+/**
+ * mtcp_wait_data - wait for data to arrive at sk_receive_queue
+ * on any of the subsockets attached to the mpcb
+ * @mpcb:  the mpcb to wait on
+ * @sk:    its master socket
+ * @timeo: for how long
+ *
+ * Now socket state including sk->sk_err is changed only under lock,
+ * hence we may omit checks after joining wait queue.
+ * We check receive queue before schedule() only as optimization;
+ * it is very likely that release_sock() added new data.
+ */
+static int __mtcp_wait_data(struct multipath_pcb *mpcb, struct sock *master_sk,
+			    long *timeo)
+{
+	int rc; struct sock *sk; struct tcp_sock *tp;
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	DEFINE_WAIT(wait);
+
+	prepare_to_wait(master_sk->sk_sleep, &wait, TASK_INTERRUPTIBLE);
+
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		set_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);
+		tp->wait_data_bit_set=1;
+	}
+	rc = mtcp_wait_event_any_sk(mpcb, sk, tp, timeo, 
+				    (!skb_queue_empty(
+					    &mpcb_sk->sk_receive_queue) ||
+				     !skb_queue_empty(&tp->ucopy.prequeue)));
+
+	mtcp_for_each_sk(mpcb,sk,tp)
+		if (tp->wait_data_bit_set) {
+			clear_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);
+			tp->wait_data_bit_set=0;
+		}
+	finish_wait(master_sk->sk_sleep, &wait);
+	return rc;
+}
+
+/**
+ * mtcp_rcv_check_subflows, check for arrival of new subflows
+ * while waiting for arriving data
+ * WARNING: that function assumes that the mutex lock is held.
+ * @return: the number of newly added subflows
+ */
+static int mtcp_rcv_check_subflows(struct multipath_pcb *mpcb, int flags)
+{
+	int cnt_subflows,new_subflows=0;
+	struct tcp_sock *tp;	
+	cnt_subflows=mpcb->cnt_subflows;
+#ifdef CONFIG_MTCP_PM
+	/*TODO: do something similar for other path managers*/
+	mutex_unlock(&mpcb->mutex);
+	new_subflows=mtcp_check_new_subflow(mpcb);
+	mutex_lock(&mpcb->mutex);
+#endif
+	/*We may have received data on a newly created
+	  subsocket, check if the list has grown*/
+	if (cnt_subflows!=mpcb->cnt_subflows) {
+		/*We must ensure that for each new tp, 
+		  the seq pointer is correctly set. In 
+		  particular we'll get a segfault if
+		  the pointer is NULL*/
+		mtcp_for_each_newtp(mpcb,tp,
+				    cnt_subflows) {
+			if (flags & MSG_PEEK) {
+				tp->peek_seq=tp->copied_seq;
+				tp->seq=&tp->peek_seq;
+			}
+			else 
+				tp->seq=&tp->copied_seq;
+			
+			BUG_ON(sock_owned_by_user(
+				       (struct sock*)tp));
+			/*Here, all subsocks are locked
+			  so we must also lock
+			  new subsocks*/
+			lock_sock((struct sock*)tp);
+		}
+	}
+	return new_subflows;
+}
+
+int mtcp_wait_data(struct multipath_pcb *mpcb, struct sock *master_sk,
+		   long *timeo, int flags) {
+	int rc;
+	int new_subflows=0;
+
+	new_subflows=mtcp_rcv_check_subflows(mpcb, flags);		
+	/*If no data is received but a new subflow appears,
+	  we attach the new subflow and wait again for data.*/
+	do {
+		rc=__mtcp_wait_data(mpcb,master_sk,timeo);
+		new_subflows=mtcp_rcv_check_subflows(mpcb, flags);
+	} while(!rc && new_subflows); /*if a new subflow appeared, and no data,
+					loop to check if data appeared in the
+					newly arrived subsock.*/
+	return rc;
+}
+
+void mtcp_ofo_queue(struct multipath_pcb *mpcb)
+{
+	struct sk_buff *skb=NULL;
+	struct sock *mpcb_sk=(struct sock *) mpcb;
+	struct tcp_sock *mpcb_tp=tcp_sk(mpcb_sk);
+	
+	while ((skb = skb_peek(&mpcb_tp->out_of_order_queue)) != NULL) {
+		if (after(TCP_SKB_CB(skb)->data_seq, mpcb_tp->rcv_nxt))
+			break;
+				
+		if (!after(TCP_SKB_CB(skb)->end_data_seq, mpcb_tp->rcv_nxt)) {
+			printk(KERN_ERR "ofo packet was already received."
+			       "skb->end_data_seq:%x,exp. rcv_nxt:%x\n",
+			       TCP_SKB_CB(skb)->end_data_seq,mpcb_tp->rcv_nxt);
+			/*Should not happen in the current design*/
+			BUG();
+		}
+		
+		__skb_unlink(skb, &mpcb_tp->out_of_order_queue);
+
+		__skb_queue_tail(&mpcb_sk->sk_receive_queue, skb);
+		mpcb_tp->rcv_nxt=TCP_SKB_CB(skb)->end_data_seq;
+		if (tcp_hdr(skb)->fin)
+			mtcp_fin(skb,mpcb);
+	}
+}
+
+/* Clean up the receive buffer for full frames taken by the user,
+ * then send an ACK if necessary.  COPIED is the number of bytes
+ * tcp_recvmsg has given to the user so far, it speeds up the
+ * calculation of whether or not we must ACK for the sake of
+ * a window update.
+ */
+static void mtcp_cleanup_rbuf(struct sock *mpcb_sk, int copied)
+{
+	struct tcp_sock *mpcb_tp = tcp_sk(mpcb_sk);
+	struct multipath_pcb *mpcb=mpcb_tp->mpcb;
+	struct sock *sk;
+	struct tcp_sock *tp;
+	int time_to_ack = 0;
+	
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		const struct inet_connection_sock *icsk = inet_csk(sk);
+		if (!inet_csk_ack_scheduled(sk))
+			continue;
+		   /* Delayed ACKs frequently hit locked sockets during bulk
+		    * receive. */
+		if (icsk->icsk_ack.blocked ||
+		    /* Once-per-two-segments ACK was not sent by tcp_input.c */
+		    tp->rcv_nxt - tp->rcv_wup > icsk->icsk_ack.rcv_mss ||
+		    /*
+		     * If this read emptied read buffer, we send ACK, if
+		     * connection is not bidirectional, user drained
+		     * receive buffer and there was a small segment
+		     * in queue.
+		     */
+		    (copied > 0 &&
+		     ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED2) ||
+		      ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED) &&
+		       !icsk->icsk_ack.pingpong)) &&
+		      !atomic_read(&mpcb_sk->sk_rmem_alloc)))
+			time_to_ack = 1;
+	}
+
+	/* We send an ACK if we can now advertise a non-zero window
+	 * which has been raised "significantly".
+	 *
+	 * Even if window raised up to infinity, do not send window open ACK
+	 * in states, where we will not receive more. It is useless.
+	 */
+	if (copied > 0 && !time_to_ack && 
+	    !(mpcb_sk->sk_shutdown & RCV_SHUTDOWN)) {
+		__u32 rcv_window_now = tcp_receive_window(mpcb_tp);
+
+		/* Optimize, __tcp_select_window() is not cheap. */
+		if (2*rcv_window_now <= mpcb_tp->window_clamp) {
+			__u32 new_window = __tcp_select_window(mpcb->master_sk);
+
+			/* Send ACK now, if this read freed lots of space
+			 * in our buffer. Certainly, new_window is new window.
+			 * We can advertise it now, if it is not less than 
+			 * current one.
+			 * "Lots" means "at least twice" here.
+			 */
+			if (new_window && new_window >= 2 * rcv_window_now)
+				time_to_ack = 1;
+		}
+	}
+	/*If we need to send an explicit window update, we need to choose
+	  some subflow to send it. At the moment, we use the master subsock 
+	  for this.*/
+	if (time_to_ack)
+		tcp_send_ack(mpcb->master_sk);
+}
+
+/*Eats data from the meta-receive queue*/
+int mtcp_check_rcv_queue(struct multipath_pcb *mpcb,struct msghdr *msg, 
+			 size_t *len, u32 *data_seq, int *copied, int flags)
+{
+	struct sk_buff *skb;
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	int err;
+	struct tcp_sock *tp;
+
+	do {
+		u32 data_offset = 0;
+		unsigned long used;
+		int dfin = 0;
+
+		skb = skb_peek(&mpcb_sk->sk_receive_queue);
+		
+		do {
+			if (!skb) goto exit;
+
+			tp=tcp_sk(skb->sk);
+			
+			if (is_dfin_seg(mpcb,skb))
+				dfin=1;
+			
+			if (before(*data_seq,TCP_SKB_CB(skb)->data_seq)) {
+				printk(KERN_ERR "%s bug: copied %X "
+				       "dataseq %X\n", __FUNCTION__, *data_seq,
+				       TCP_SKB_CB(skb)->data_seq);
+				BUG();
+			}
+			data_offset = *data_seq - TCP_SKB_CB(skb)->data_seq;
+			if (data_offset < skb->len || dfin)
+				break;
+
+			if (skb->len + dfin != TCP_SKB_CB(skb)->end_data_seq - 
+			    TCP_SKB_CB(skb)->data_seq) {
+				printk(KERN_ERR "skb->len:%d, should be %d\n",
+				       skb->len,
+				       TCP_SKB_CB(skb)->end_data_seq -
+				       TCP_SKB_CB(skb)->data_seq);
+				BUG();
+			}
+			WARN_ON(!(flags & MSG_PEEK));
+			skb = skb->next;
+		} while (skb != (struct sk_buff *)&mpcb_sk->sk_receive_queue);
+
+		if (skb == (struct sk_buff *)&mpcb_sk->sk_receive_queue) 
+			goto exit;
+
+		used = skb->len - data_offset;
+		if (*len < used)
+			used = *len;
+		
+		err=skb_copy_datagram_iovec(skb, data_offset,
+					    msg->msg_iov, used);
+		BUG_ON(err);
+		
+		*data_seq+=used+dfin;
+		*copied+=used;
+		*len-=used;
+		
+ 		if (*data_seq==TCP_SKB_CB(skb)->end_data_seq && 
+		    !(flags & MSG_PEEK))
+			sk_eat_skb(mpcb_sk, skb, 0);
+		else if (!(flags & MSG_PEEK) && *len!=0) {
+				printk(KERN_ERR 
+				       "%s bug: copied %#x "
+				       "dataseq %#x, *len %d\n", __FUNCTION__, 
+				       *data_seq, 
+				       TCP_SKB_CB(skb)->data_seq, (int)*len);
+				printk(KERN_ERR "init data_seq:%#x,used:%d\n",
+				       skb->data_seq,(int)used);
+				BUG();
+		}
+	} while (*len>0);
+	/*This checks whether an explicit window update is needed to unblock
+	  the receiver*/
+exit:
+	mtcp_cleanup_rbuf(mpcb_sk,*copied);
+	return 0;
+}
+
+int mtcp_queue_skb(struct sock *sk,struct sk_buff *skb)
+{
+	struct tcp_sock *tp=tcp_sk(sk);
+	struct multipath_pcb *mpcb;
+	int fin=tcp_hdr(skb)->fin;
+	struct sock *mpcb_sk;
+	struct tcp_sock *mpcb_tp;
+	int ans;
+
+	mpcb=mpcb_from_tcpsock(tp);
+	if (tp->pending)
+		mpcb=mtcp_hash_find(tp->mtcp_loc_token);
+	mpcb_sk=(struct sock *) mpcb;
+	mpcb_tp=tcp_sk(mpcb_sk);
+
+	if (!tp->mpc || !mpcb) {
+		__skb_queue_tail(&sk->sk_receive_queue, skb);
+		return MTCP_QUEUED;
+	}
+	
+	if (!skb->len && tcp_hdr(skb)->fin && !tp->rx_opt.saw_dfin) {
+		/*Pure subflow FIN (without DFIN)
+		  just update subflow and return*/
+		++tp->copied_seq;
+		return MTCP_EATEN;
+	}
+	
+	/*In all cases, we remove it from the subsock, so copied_seq
+	  must be advanced*/
+	tp->copied_seq=TCP_SKB_CB(skb)->end_seq+fin;
+	tcp_rcv_space_adjust(sk);
+	
+	/*Verify that the mapping info has been read*/
+	if(TCP_SKB_CB(skb)->data_len) {
+		mtcp_get_dataseq_mapping(tp,skb);
+	}
+	
+	/*Is this a duplicate segment ?*/
+	if (!before(mpcb_tp->rcv_nxt,TCP_SKB_CB(skb)->end_data_seq)) {
+		/*Duplicate segment. We can arrive here only if a segment 
+		  has been retransmitted by the sender on another subflow.
+		  Retransmissions on the same subflow are handled at the
+		  subflow level.*/
+
+		/* We do not read the skb, since it was already received on
+		   another subflow*/
+		ans=MTCP_EATEN;
+		goto out;
+	}
+	
+	if (before(mpcb_tp->rcv_nxt,TCP_SKB_CB(skb)->data_seq)) {
+		
+		if (!skb_peek(&mpcb_tp->out_of_order_queue)) {
+			/* Initial out of order segment */
+			mtcp_debug("First meta-ofo segment\n");
+			__skb_queue_head(&mpcb_tp->out_of_order_queue, skb);
+			ans=MTCP_QUEUED;
+			goto queued;
+		}
+		else {
+			struct sk_buff *skb1 = mpcb_tp->out_of_order_queue.prev;
+			/* Find place to insert this segment. */
+			do {
+				if (!after(TCP_SKB_CB(skb1)->data_seq, 
+					   TCP_SKB_CB(skb)->data_seq))
+					break;
+			} while ((skb1 = skb1->prev) !=
+				 (struct sk_buff *)
+				 &mpcb_tp->out_of_order_queue);
+
+			/* Do skb overlap to previous one? */
+			if (skb1 != 
+			    (struct sk_buff *)&mpcb_tp->out_of_order_queue &&
+			    before(TCP_SKB_CB(skb)->data_seq, 
+				   TCP_SKB_CB(skb1)->end_data_seq)) {
+				if (!after(TCP_SKB_CB(skb)->end_data_seq, 
+					   TCP_SKB_CB(skb1)->end_data_seq)) {
+					/* All the bits are present. Drop. */
+					/* We do not read the skb, since it was
+					   already received on
+					   another subflow */
+					ans=MTCP_EATEN;
+					goto out;
+				}
+				if (!after(TCP_SKB_CB(skb)->data_seq, 
+					   TCP_SKB_CB(skb1)->data_seq)) {
+					/*skb and skb1 have the same starting 
+					  point, but skb terminates after skb1*/
+					printk(KERN_ERR "skb->data_seq:%x,"
+					       "skb->end_data_seq:%x,"
+					       "skb1->data_seq:%x,"
+					       "skb1->end_data_seq:%x,"
+					       "skb->seq:%x,"
+					       "skb1->seq:%x""\n",
+					       TCP_SKB_CB(skb)->data_seq,
+					       TCP_SKB_CB(skb)->end_data_seq,
+					       TCP_SKB_CB(skb1)->data_seq,
+					       TCP_SKB_CB(skb1)->end_data_seq,
+					       TCP_SKB_CB(skb)->seq,
+					       TCP_SKB_CB(skb1)->seq);
+					BUG();
+					skb1 = skb1->prev;
+				}
+			}
+			__skb_insert(skb, skb1, skb1->next, 
+				     &mpcb_tp->out_of_order_queue);
+			/* And clean segments covered by new one as whole. */
+			while ((skb1 = skb->next) !=
+			       (struct sk_buff *)&mpcb_tp->out_of_order_queue &&
+			       after(TCP_SKB_CB(skb)->end_data_seq, 
+				     TCP_SKB_CB(skb1)->data_seq)) {
+				if (!before(TCP_SKB_CB(skb)->end_data_seq, 
+					    TCP_SKB_CB(skb1)->end_data_seq)) {
+					skb_unlink(skb1, 
+						     &mpcb_tp->
+						     out_of_order_queue);
+					__kfree_skb(skb1);
+				}
+				else break;
+			}
+			ans=MTCP_QUEUED;
+			goto queued;
+		}
+	}
+	else {
+		__skb_queue_tail(&mpcb_sk->sk_receive_queue, skb);
+		mpcb_tp->rcv_nxt=TCP_SKB_CB(skb)->end_data_seq;
+
+		if (fin)
+			mtcp_fin(skb,mpcb);
+
+		/*Check if this fills a gap in the ofo queue*/
+		if (!skb_queue_empty(&mpcb_tp->out_of_order_queue))
+			mtcp_ofo_queue(mpcb);
+
+		ans=MTCP_QUEUED;
+		goto queued;
+	}
+
+queued:
+	/* Uncharge the old socket, and then charge the new one */
+	if (skb->destructor)
+		skb->destructor(skb);
+
+	skb_set_owner_r(skb, mpcb_sk);
+out:
+	if (tp->pending) 
+		mpcb_put(mpcb);
+	return ans;
+}
+
+/**
+ * specific version of skb_entail (tcp.c),that allows appending to any
+ * subflow.
+ * Here, we do not set the data seq, since it remains the same. However, 
+ * we do change the subflow seqnum.
+ *
+ * Note that we make the assumption that, within the local system, every
+ * segment has tcb->sub_seq==tcb->seq, that is, the dataseq is not shifted
+ * compared to the subflow seqnum. Put another way, the dataseq referenced
+ * is actually the number of the first data byte in the segment.
+ */
+void mtcp_skb_entail(struct sock *sk, struct sk_buff *skb)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp_skb_cb *tcb = TCP_SKB_CB(skb);
+	int fin=(tcb->flags & TCPCB_FLAG_FIN)?1:0;
+	
+	tcb->seq      = tcb->end_seq = tcb->sub_seq = tp->write_seq;
+	tcb->sacked = 0; /*reset the sacked field: from the point of view
+			   of this subflow, we are sending a brand new
+			   segment*/
+	tcp_add_write_queue_tail(sk, skb);
+	sk->sk_wmem_queued += skb->truesize;
+	sk_mem_charge(sk, skb->truesize);
+
+	/*Take into account seg len*/
+	tp->write_seq += skb->len+fin;
+	tcb->end_seq += skb->len+fin;
+}
+
+/*Algorithm by Bryan Kernighan to count bits in a word*/
+static inline int count_bits(unsigned int v)
+{
+	unsigned int c; /* c accumulates the total bits set in v*/
+	for (c = 0; v; c++)
+	{
+		v &= v - 1; /* clear the least significant bit set*/
+	}
+	return c;
+}
+
+/**
+ * Reinject data from one TCP subflow to the mpcb_sk 
+ * The @skb given pertains to the original tp, that keeps it
+ * because the skb is still sent on the original tp. But additionnally,
+ * it is sent on the other subflow. 
+ *
+ * @pre : @sk must be the mpcb_sk
+ */
+int __mtcp_reinject_data(struct sk_buff *orig_skb, struct sock *sk)
+{
+	struct sk_buff *skb;
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcphdr *th;
+	struct sock *sk_it;
+	struct tcp_sock *tp_it;
+	
+	/*A segment can be added to the reinject queue only if 
+	  there is at least one working subflow that has never sent
+	  this data*/
+	mtcp_for_each_sk(tp->mpcb,sk_it,tp_it) {
+		if (sk_it->sk_state!=TCP_ESTABLISHED)
+			continue;
+		/*If the skb has already been enqueued in this sk, try to find
+		  another one*/
+		if (PI_TO_FLAG(tp_it->path_index) & orig_skb->path_mask) 
+			continue;
+		
+		/*candidate subflow found, we can reinject*/
+		break;
+	}
+	
+	if (!sk_it) {
+		if ((PI_TO_FLAG(1) & orig_skb->path_mask) &&
+		    (PI_TO_FLAG(9) & orig_skb->path_mask))
+			tcpprobe_logmsg(sk,"skb already injected to all "
+					"paths");
+		return 0; /*no candidate found*/
+	}
+
+	skb=skb_clone(orig_skb,GFP_ATOMIC);
+	if (unlikely(!skb))
+		return -ENOBUFS;
+	skb->sk=sk;
+
+	th=tcp_hdr(skb);
+	
+	BUG_ON(!skb);
+	BUG_ON(skb->path_mask!=orig_skb->path_mask);
+	
+	skb_queue_tail(&tp->mpcb->reinject_queue,skb);
+	return 0;
+}
+
+/*Inserts data into the reinject queue*/
+void mtcp_reinject_data(struct sock *orig_sk)
+{
+	struct sk_buff *skb_it;
+	struct tcp_sock *orig_tp = tcp_sk(orig_sk);
+	struct multipath_pcb *mpcb=orig_tp->mpcb;
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+
+	BUG_ON(is_meta_sk(orig_sk));
+	
+	verif_wqueues(mpcb);
+	
+	tcp_for_write_queue(skb_it,orig_sk) {
+		skb_it->path_mask|=PI_TO_FLAG(orig_tp->path_index);
+		if (unlikely(__mtcp_reinject_data(skb_it,mpcb_sk)<0))
+			break;
+	}
+	
+	tcpprobe_logmsg(orig_sk,"after reinj, reinj queue size:%d",
+			skb_queue_len(&mpcb->reinject_queue));
+	
+
+	tcp_push(mpcb_sk, 0, sysctl_mptcp_mss, TCP_NAGLE_PUSH);
+
+	if (orig_tp->pf==0)
+		tcpprobe_logmsg(orig_sk,"pi %d: entering pf state",
+				orig_tp->path_index);
+	orig_tp->pf=1;
+
+	verif_wqueues(mpcb);
+}
+
+/**
+ * To be called when a segment is in order. That is, either when it is received 
+ * and is immediately in subflow-order, or when it is stored in the ofo-queue
+ * and becomes in-order. This function retrieves the data_seq and end_data_seq
+ * values, needed for that segment to be transmitted to the meta-flow.
+ * *If the segment already holds a mapping, the current mapping is replaced 
+ *  with the one provided in the segment.
+ * *If the segment contains no mapping, we check if its dataseq can be derived 
+ *  from the currently stored mapping. If it cannot, then there is an error,
+ *  and it must be dropped.
+ *
+ * - If the mapping has been correctly updated, or the skb has correctly 
+ *   been given its dataseq, we then check if the segment is in meta-order.
+ *   i) if it is: we return 1
+ *   ii) if its end_data_seq is older then mpcb->copied_seq, it is a 
+ *       reinjected segment arrived late. We return 2, to indicate to the 
+ *       caller that the segment can be eaten by the subflow immediately.
+ *   iii) if it is not in meta-order (keep in mind that the precondition 
+ *        requires that it is in subflow order): we return 0
+ *   iv) particular case: if the segment is a subflow-only segment
+ *      (e.g. FIN without DFIN), we return 3.
+ * - If the skb is faulty (does not contain a dataseq option, and seqnum
+ *   not contained in currently stored mapping), we return -1
+ * - If the tp is a pending tp, and the mpcb is destroyed (not anymore
+ *   in the hashtable), we return -1.
+ */
+int mtcp_get_dataseq_mapping(struct tcp_sock *tp, struct sk_buff *skb)
+{
+	int changed=0;
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+	int ans;
+
+	BUG_ON(!mpcb && !tp->pending);
+	/*We must be able to find the mapping even for a pending
+	  subsock, because that pending subsock can trigger the wake up of
+	  the application. (it is holds the next DSN)*/
+	if (tp->pending) {
+		mpcb=mtcp_hash_find(tp->mtcp_loc_token);
+		if(!mpcb) return -1;
+	}
+
+	if (TCP_SKB_CB(skb)->data_len) {
+		tp->map_data_seq=TCP_SKB_CB(skb)->data_seq;
+		tp->map_data_len=TCP_SKB_CB(skb)->data_len;
+		tp->map_subseq=TCP_SKB_CB(skb)->sub_seq;
+		changed=1;
+	}
+	
+	/*Is it a subflow only FIN ?*/
+	if (tcp_hdr(skb)->fin && !tp->rx_opt.saw_dfin) {
+		return 3;
+	}
+
+	/*Even if we have received a mapping update, it may differ from
+	  the seqnum contained in the
+	  TCP header. In that case we must recompute the data_seq and 
+	  end_data_seq accordingly. This is what happens in case of TSO, because
+	  the NIC keeps the option as is.*/
+	
+	if (before(TCP_SKB_CB(skb)->seq,tp->map_subseq) ||
+	    after(TCP_SKB_CB(skb)->end_seq,
+		  tp->map_subseq+tp->map_data_len+tcp_hdr(skb)->fin)) {
+		
+		printk(KERN_ERR "seq:%x,tp->map_subseq:%x,"
+		       "end_seq:%x,tp->map_data_len:%d,changed:%d\n",
+		       TCP_SKB_CB(skb)->seq,tp->map_subseq,
+		       TCP_SKB_CB(skb)->end_seq,tp->map_data_len,
+		       changed);
+		BUG(); /*If we only speak with our own implementation,
+			 reaching this point can only be a bug, later we
+			 can remove this.*/
+		ans=1;
+		goto out;
+	}
+	/*OK, the segment is inside the mapping, we can
+	  derive the dataseq. Note that we maintain 
+	  TCP_SKB_CB(skb)->data_len to zero, so as not to mix
+	  received mappings and derived dataseqs.*/
+	TCP_SKB_CB(skb)->data_seq=tp->map_data_seq+
+		(TCP_SKB_CB(skb)->seq-tp->map_subseq);
+	TCP_SKB_CB(skb)->end_data_seq=
+		TCP_SKB_CB(skb)->data_seq+skb->len;
+	if (mpcb->received_options.dfin_rcvd &&
+	    TCP_SKB_CB(skb)->end_data_seq+1==mpcb->received_options.fin_dsn) {
+		TCP_SKB_CB(skb)->end_data_seq++;
+	}
+	TCP_SKB_CB(skb)->data_len=0; /*To indicate that there is not anymore
+				       general mapping information in that 
+				       segment (the mapping info is now 
+				       consumed)*/
+		
+	/*Check now if the segment is in meta-order, it is considered
+	  in meta-order if the next expected DSN is contained in the
+	  segment*/
+	
+	if (!before(mpcb->tp.copied_seq,TCP_SKB_CB(skb)->data_seq) &&
+	    before(mpcb->tp.copied_seq,TCP_SKB_CB(skb)->end_data_seq))
+		ans=1;
+	else if (!before(mpcb->tp.copied_seq,TCP_SKB_CB(skb)->end_data_seq))
+		ans=2;
+	else ans=0;
+	
+out:
+	if (tp->pending)
+		mpcb_put(mpcb);
+	return ans;
+}
+
+/* Obtain a reference to a local port for the given sock,
+ * snum MUST have a valid port number, since it must be a copy 
+ * of the snum from a master TCP socket.
+ */
+int mtcpsub_get_port(struct sock *sk, unsigned short snum)
+{
+	struct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;
+	struct inet_bind_hashbucket *head;
+	struct hlist_node *node;
+	struct inet_bind_bucket *tb;
+	int ret;
+	struct net *net = sock_net(sk);
+
+	local_bh_disable();
+	if (!snum) {
+		ret=-1;
+		goto fail; /*snum is required in MTCPSUB, since it must be
+			     the copy of the originating socket*/
+	} else {
+		head = &hashinfo->bhash[inet_bhashfn(net, snum,
+				hashinfo->bhash_size)];
+		spin_lock(&head->lock);
+		inet_bind_bucket_for_each(tb, node, &head->chain)
+			if (tb->ib_net == net && tb->port == snum)
+				goto success;
+	}
+	tb = NULL;
+	ret = 1;
+	goto fail_unlock;
+success:
+	if (!inet_csk(sk)->icsk_bind_hash)
+		inet_bind_hash(sk, tb, snum);
+	BUG_ON(inet_csk(sk)->icsk_bind_hash != tb);
+	ret = 0;
+
+fail_unlock:
+	spin_unlock(&head->lock);
+fail:
+	local_bh_enable();
+	return ret;
+}
+
+/*Cleans the meta-socket retransmission queue.
+  @sk must be the metasocket.*/
+void mtcp_clean_rtx_queue(struct sock *sk)
+{
+	struct sk_buff *skb;
+	struct tcp_sock *tp=tcp_sk(sk);
+	
+	BUG_ON(!is_meta_tp(tp));
+	
+	check_send_head(sk,0);
+	
+	while ((skb = tcp_write_queue_head(sk)) && skb != tcp_send_head(sk)) {
+		struct tcp_skb_cb *scb = TCP_SKB_CB(skb);
+		if (before(tp->snd_una,scb->end_data_seq))
+			break;
+		if(skb==tcp_send_head(sk)) {
+			printk(KERN_ERR "removing the send head !\n");
+			printk(KERN_ERR "was it ever transmitted ?\n");
+			printk(KERN_ERR "dsn is %#x\n",
+			       TCP_SKB_CB(skb)->data_seq);
+			BUG();
+		}
+		tcp_unlink_write_queue(skb, sk);
+		tp->packets_out-=tcp_skb_pcount(skb);
+		sk_wmem_free_skb(sk, skb);
+	}
+	check_send_head(sk,1);
+}
+
+/*At the moment we apply a simple addition algorithm.
+  We will complexify later*/
+void mtcp_update_window_clamp(struct multipath_pcb *mpcb)
+{
+	struct tcp_sock *tp;
+	struct sock *sk;
+	struct tcp_sock *mpcb_tp = (struct tcp_sock *) mpcb;
+	struct sock *mpcb_sk = (struct sock *)mpcb;
+	u32 new_clamp=0;
+	u32 new_rcv_ssthresh=0;
+	u32 new_rcvbuf=0;
+
+	/*Can happen if called from non mpcb sock.*/
+	if (!mpcb) return;
+
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		new_clamp += tp->window_clamp;
+		new_rcv_ssthresh += tp->rcv_ssthresh;
+		new_rcvbuf += sk->sk_rcvbuf;
+	}
+	mpcb_tp->window_clamp = new_clamp;
+	mpcb_tp->rcv_ssthresh = new_rcv_ssthresh;
+	mpcb_sk->sk_rcvbuf = new_rcvbuf;
+}
+
+/*Update the mpcb send window, based on the contributions
+  of each subflow*/
+void mtcp_update_sndbuf(struct multipath_pcb *mpcb)
+{
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	struct tcp_sock *tp;
+	struct sock *sk;
+	int new_sndbuf=0;
+	mtcp_for_each_sk(mpcb,sk,tp)
+		new_sndbuf += sk->sk_sndbuf;
+	mpcb_sk->sk_sndbuf = new_sndbuf;
+}
+
+extern void tcp_check_space(struct sock *sk);
+
+void mtcp_push_frames(struct sock *sk)
+{
+	struct tcp_sock *tp=tcp_sk(sk);
+	
+	tp->push_frames=0;
+	lock_sock(sk);
+	tcp_push_pending_frames(sk);
+	tcp_check_space(sk);
+	/*Note release sock can call us again, which is correct because 
+	  it would mean that we received new acks while we were pushing.*/
+	release_sock(sk);
+}
+
+//#define DEBUG_WQUEUES 1
+#ifdef DEBUG_WQUEUES
+void verif_wqueues(struct multipath_pcb *mpcb) 
+{
+	struct sock *sk;
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	struct tcp_sock *tp;
+	struct sk_buff *skb;
+	int sum;
+
+	local_bh_disable();
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		sum=0;
+		tcp_for_write_queue(skb,sk) {
+			sum+=skb->truesize;
+		}
+		if (sum!=sk->sk_wmem_queued) {
+			printk(KERN_ERR "wqueue leak_1: enqueued:%d, recorded "
+			       "value:%d\n",
+			       sum,sk->sk_wmem_queued);
+			
+			tcp_for_write_queue(skb,sk) {
+				printk(KERN_ERR "skb truesize:%d\n",
+				       skb->truesize);
+			}
+			
+			local_bh_enable();
+			BUG();
+		}
+	}
+	sum=0;
+	tcp_for_write_queue(skb,mpcb_sk)
+		sum+=skb->truesize;		
+	BUG_ON(sum!=mpcb_sk->sk_wmem_queued);
+	local_bh_enable();
+}
+#else
+void verif_wqueues(struct multipath_pcb *mpcb)
+{
+	return;
+}
+#endif
+
+//#define DEBUG_RQUEUES 1
+#ifdef DEBUG_RQUEUES
+void verif_rqueues(struct multipath_pcb *mpcb) 
+{
+	struct sock *sk;
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	struct tcp_sock *tp;
+	struct sk_buff *skb;
+	int sum;
+
+	local_bh_disable();
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		sum=0;
+		skb_queue_walk(&sk->sk_receive_queue, skb) {
+			sum+=skb->truesize;
+		}
+		skb_queue_walk(&tp->out_of_order_queue, skb) {
+			sum+=skb->truesize;
+		}
+		/*TODO: add meta-rcv and meta-ofo-queues*/
+		if (sum!=atomic_read(&sk->sk_rmem_alloc)) {
+			printk(KERN_ERR "rqueue leak: enqueued:%d, recorded "
+			       "value:%d\n",
+			       sum,sk->sk_rmem_alloc);
+			
+			local_bh_enable();
+			BUG();
+		}
+	}
+	local_bh_enable();
+}
+#else
+void verif_rqueues(struct multipath_pcb *mpcb)
+{
+	return;
+}
+#endif
+
+/*Removes a segment received on one subflow, but containing DSNs
+  that were already received on another subflow
+  Note that if the segment is not the head of the receive queue,
+  we keep it in the list for future removal, because we cannot advance
+  the tcp counters.
+  WARNING: this may remove the skb, so no further reference to it
+  should happen after calling this function.
+*/
+void mtcp_check_eat_old_seg(struct sock *sk, struct sk_buff *skb)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcphdr *th = tcp_hdr(skb);
+	if (skb!=skb_peek(&sk->sk_receive_queue))
+		return;
+	BUG_ON(tp->copied_seq!=TCP_SKB_CB(skb)->seq);
+	/*OK, eat the segment, and advance tcp counters*/
+	tp->copied_seq += skb->len;
+	tcp_rcv_space_adjust(sk);
+	if (tp->copied_seq!=TCP_SKB_CB(skb)->end_seq && !th->fin) {
+		printk(KERN_ERR "corrupted seg: seq:%#x,end_seq:%#x,len:%d\n",
+		       TCP_SKB_CB(skb)->seq,TCP_SKB_CB(skb)->end_seq,skb->len);
+		BUG();
+	}
+	inet_csk_schedule_ack(sk);
+	sk_eat_skb(sk,skb,0);
+}
+
+/**
+ * Returns the next segment to be sent from the mptcp meta-queue.
+ * (chooses the reinject queue if any segment is waiting in it, otherwise,
+ * chooses the normal write queue).
+ * Sets *@reinject to 1 if the returned segment comes from the 
+ * reinject queue. Otherwise sets @reinject to 0.
+ */
+struct sk_buff* mtcp_next_segment(struct sock *sk, int *reinject)
+{	
+	struct multipath_pcb *mpcb=tcp_sk(sk)->mpcb;
+	struct sk_buff *skb;
+	if (reinject) *reinject=0;
+	if (!is_meta_sk(sk))
+		return tcp_send_head(sk);
+	if ((skb=skb_peek(&mpcb->reinject_queue))) {
+ 		if (reinject) *reinject=1; /*Segments in reinject queue are 
+					     already cloned*/
+		return skb;
+	}
+	else return tcp_send_head(sk);
+}
+
+/*Sets the socket pointer of the mpcb_sk after an accept at the socket level
+ * Set also the sk_sleep pointer, because it has just been copied by
+ * sock_graft() */
+void mtcp_check_socket(struct sock *sk)
+{
+	if (sk->sk_protocol==IPPROTO_TCP && tcp_sk(sk)->mpcb) {
+		struct sock *mpcb_sk=(struct sock*)(tcp_sk(sk)->mpcb);
+		sk_set_socket(mpcb_sk,sk->sk_socket);
+		mpcb_sk->sk_sleep=sk->sk_sleep;
+	}
+}
+EXPORT_SYMBOL(mtcp_check_socket);
+
+extern void tcp_queue_skb(struct sock *sk, struct sk_buff *skb);
+void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags);
+
+/*Sends the datafin*/
+void mtcp_send_fin(struct sock *mpcb_sk)
+{
+	struct sk_buff *skb;
+	struct multipath_pcb *mpcb=(struct multipath_pcb *)mpcb_sk;
+	struct tcp_sock *mpcb_tp=tcp_sk(mpcb_sk);
+	if (tcp_send_head(mpcb_sk)) {
+		skb = tcp_write_queue_tail(mpcb_sk);
+		TCP_SKB_CB(skb)->flags |= TCPCB_FLAG_FIN;
+		TCP_SKB_CB(skb)->data_len++;
+		TCP_SKB_CB(skb)->end_data_seq++;
+		mpcb_tp->write_seq++;
+	}
+	else {
+		for (;;) {
+			skb = alloc_skb_fclone(MAX_TCP_HEADER, 
+					       GFP_KERNEL);
+			if (skb)
+				break;
+			yield();
+		}
+		/* Reserve space for headers and prepare control bits. */
+		skb_reserve(skb, MAX_TCP_HEADER);
+		tcp_init_nondata_skb(skb, 0,
+				     TCPCB_FLAG_ACK | TCPCB_FLAG_FIN);
+		TCP_SKB_CB(skb)->data_seq=mpcb_tp->write_seq;
+		TCP_SKB_CB(skb)->data_len=1;
+		TCP_SKB_CB(skb)->end_data_seq=mpcb_tp->write_seq+1;
+		/* FIN eats a sequence byte, write_seq advanced by 
+		   tcp_queue_skb(). */
+		tcp_queue_skb(mpcb_sk, skb);
+	}
+	set_bit(MPCB_FLAG_FIN_ENQUEUED,&mpcb->flags);
+	__tcp_push_pending_frames(mpcb_sk, sysctl_mptcp_mss, TCP_NAGLE_OFF);
+}
+
+extern int tcp_close_state(struct sock *sk);
+void mtcp_close(struct sock *master_sk, long timeout)
+{
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tcp_sk(master_sk));
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	struct tcp_sock *mpcb_tp=tcp_sk(mpcb_sk);
+	struct sk_buff *skb;
+	int data_was_unread = 0;
+	int state;
+	
+	mtcp_debug("%s: Close of mpcb_sk\n",__FUNCTION__);
+
+	/*destroy the mpcb, it will really disappear when the last subsock 
+	  is destroyed*/
+	mpcb_get(mpcb);
+	mtcp_destroy_mpcb(mpcb);	
+	
+	if (!tcp_sk(master_sk)->mpc) {
+		mpcb_put(mpcb);
+		return tcp_close(master_sk,timeout);		
+	}
+
+	lock_sock(mpcb_sk);
+	mpcb_sk->sk_shutdown = SHUTDOWN_MASK;
+
+	/*  We need to flush the recv. buffs.  We do this only on the
+	 *  descriptor close, not protocol-sourced closes, because the
+	 *  reader process may not have drained the data yet!
+	 */
+	while ((skb = __skb_dequeue(&mpcb_sk->sk_receive_queue)) != NULL) {
+		u32 len = TCP_SKB_CB(skb)->end_data_seq - 
+			TCP_SKB_CB(skb)->data_seq -
+			(is_dfin_seg(mpcb,skb)?1:0);
+		data_was_unread += len;
+		__kfree_skb(skb);
+	}
+
+	sk_mem_reclaim(mpcb_sk);
+
+	if (tcp_close_state(mpcb_sk))
+		mtcp_send_fin(mpcb_sk);
+	else if (mpcb_tp->snd_nxt==mpcb_tp->write_seq) {
+		struct sock *sk_it, *sk_tmp;
+		/*The FIN has been sent already, we need to 
+		  call tcp_close() on the subsocks
+		  ourselves.*/
+		mtcp_for_each_sk_safe(mpcb,sk_it,sk_tmp)
+			tcp_close(sk_it,timeout);
+	}
+	
+	sk_stream_wait_close(mpcb_sk, timeout);
+
+	state = mpcb_sk->sk_state;
+	sock_orphan(mpcb_sk);
+	atomic_inc(mpcb_sk->sk_prot->orphan_count);
+
+	/* It is the last release_sock in its life. It will remove backlog. */
+	release_sock(mpcb_sk);
+	mpcb_put(mpcb);
+}
+
+#ifdef MTCP_DEBUG_PKTS_OUT
+int check_pkts_out(struct sock* sk) {
+	int cnt=0;
+	struct sk_buff *skb;
+	struct tcp_sock *tp=tcp_sk(sk);
+	/*TODEL: sanity check on packets_out*/
+	if (tp->mpc && !is_meta_tp(tp)) {
+		tcp_for_write_queue(skb,sk) {
+			if (skb == tcp_send_head(sk))
+				break;
+			else cnt+=tcp_skb_pcount(skb);
+		}
+		BUG_ON(tp->packets_out!=cnt);
+	}
+	else cnt=-10;
+
+	return cnt;
+}
+
+void check_send_head(struct sock *sk, int num) {
+	struct sk_buff *head=tcp_send_head(sk);
+	struct sk_buff *skb;
+	int found=0;
+	if (head) {
+		tcp_for_write_queue(skb,sk) {
+			if (skb==head) {
+				found=1;
+				break;
+			}			
+		}
+	}
+	else found=1;
+	if(!found) {
+		printk(KERN_ERR "num:%d\n",num);
+		BUG();
+	}
+}
+#endif
+
+/*General initialization of mptcp
+ */
+static int __init mptcp_init(void)
+{
+#ifdef CONFIG_SYSCTL
+	register_sysctl_table(mptcp_root_table);
+#endif
+	return 0;
+}
+module_init(mptcp_init);
+
+MODULE_LICENSE("GPL");
+
+EXPORT_SYMBOL(mtcp_sendmsg);
diff --git a/net/ipv4/mtcp_ipv4.c b/net/ipv4/mtcp_ipv4.c
new file mode 100644
index 0000000..be61c35
--- /dev/null
+++ b/net/ipv4/mtcp_ipv4.c
@@ -0,0 +1,117 @@
+/*
+ *	MTCP implementation
+ *
+ *	Author:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *
+ *      date : March 2010
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <net/sock.h>
+#include <net/mtcp.h>
+#include <net/tcp.h>
+#include <net/transp_v6.h>
+
+extern struct timewait_sock_ops tcp_timewait_sock_ops;
+
+/* NOTE: A lot of things set to zero explicitly by call to
+ *       sk_alloc() so need not be done here.
+ */
+static int mtcpsub_v4_init_sock(struct sock *sk)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	skb_queue_head_init(&tp->out_of_order_queue);
+	tcp_init_xmit_timers(sk);
+	tcp_prequeue_init(tp);
+
+	icsk->icsk_rto = TCP_TIMEOUT_INIT;
+	tp->mdev = TCP_TIMEOUT_INIT;
+
+	/* So many TCP implementations out there (incorrectly) count the
+	 * initial SYN frame in their delayed-ACK and congestion control
+	 * algorithms that we must have the following bandaid to talk
+	 * efficiently to them.  -DaveM
+	 */
+	tp->snd_cwnd = 2;
+
+	/* See draft-stevens-tcpca-spec-01 for discussion of the
+	 * initialization of these values.
+	 */
+	tp->snd_ssthresh = 0x7fffffff;	/* Infinity */
+	tp->snd_cwnd_clamp = ~0;
+	tp->mss_cache = 536;
+
+	tp->reordering = sysctl_tcp_reordering;
+	icsk->icsk_ca_ops = &tcp_init_congestion_ops;
+
+	sk->sk_state = TCP_CLOSE;
+
+	sk->sk_write_space = sk_stream_write_space;
+	sock_set_flag(sk, SOCK_USE_WRITE_QUEUE);
+
+	icsk->icsk_af_ops = &ipv4_specific;
+	icsk->icsk_sync_mss = tcp_sync_mss;
+#ifdef CONFIG_TCP_MD5SIG
+	tp->af_specific = &tcp_sock_ipv4_specific;
+#endif
+
+	sk->sk_sndbuf = sysctl_tcp_wmem[1];
+	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
+
+	atomic_inc(&tcp_sockets_allocated);
+
+	return 0;
+}
+
+/*While the metasocket (seen by applications as a normal TCP socket) is
+ * dealt with as a TCP socket, we define a specific protocol for MTCP subflows.
+ * The new protocol is mostly based on normal TCP functions, and mainly serves
+ * for disambiguating whether we are inside a subflow or a metaflow.*/
+struct proto mtcpsub_prot = {
+	.name			= "MTCPSUB",
+	.owner			= THIS_MODULE,
+	.close			= tcp_close,
+	.connect		= tcp_v4_connect,
+	.disconnect		= tcp_disconnect,
+	.accept			= inet_csk_accept,
+	.ioctl			= tcp_ioctl,
+	.init			= mtcpsub_v4_init_sock,
+	.destroy		= tcp_v4_destroy_sock,
+	.shutdown		= tcp_shutdown,
+	.setsockopt		= tcp_setsockopt,
+	.getsockopt		= tcp_getsockopt,
+	.recvmsg		= tcp_recvmsg,
+	.backlog_rcv		= tcp_v4_do_rcv,
+	.hash			= inet_hash,
+	.unhash			= inet_unhash,
+	.get_port		= mtcpsub_get_port,
+	.enter_memory_pressure	= tcp_enter_memory_pressure,
+	.sockets_allocated	= &tcp_sockets_allocated,
+	.orphan_count		= &tcp_orphan_count,
+	.memory_allocated	= &tcp_memory_allocated,
+	.memory_pressure	= &tcp_memory_pressure,
+	.sysctl_mem		= sysctl_tcp_mem,
+	.sysctl_wmem		= sysctl_tcp_wmem,
+	.sysctl_rmem		= sysctl_tcp_rmem,
+	.max_header		= MAX_TCP_HEADER,
+	.obj_size		= sizeof(struct tcp_sock),
+	.twsk_prot		= &tcp_timewait_sock_ops,
+	.rsk_prot		= &tcp_request_sock_ops,
+	.h.hashinfo		= &tcp_hashinfo,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt	= compat_tcp_setsockopt,
+	.compat_getsockopt	= compat_tcp_getsockopt,
+#endif
+};
+
+
+EXPORT_SYMBOL(mtcpsub_prot);
diff --git a/net/ipv4/mtcp_pm.c b/net/ipv4/mtcp_pm.c
new file mode 100644
index 0000000..76ddfd8
--- /dev/null
+++ b/net/ipv4/mtcp_pm.c
@@ -0,0 +1,1268 @@
+/*
+ *	MTCP PM implementation
+ *
+ *	Authors:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *      date : June 10
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <net/mtcp.h>
+#include <net/mtcp_pm.h>
+#include <linux/netdevice.h>
+#include <linux/inetdevice.h>
+#include <linux/list.h>
+#include <linux/tcp.h>
+#include <net/inet_sock.h>
+#include <net/tcp.h>
+
+#define MTCP_HASH_SIZE                16
+#define hash_tk(token) \
+	jhash_1word(token,0)%MTCP_HASH_SIZE
+
+
+extern struct ip_options *tcp_v4_save_options(struct sock *sk,
+					      struct sk_buff *skb);
+extern void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags);
+extern void tcp_options_write(__be32 *ptr, struct tcp_sock *tp,
+		       const struct tcp_out_options *opts,
+		       __u8 **md5_hash);
+extern void tcp_v4_send_ack(struct sk_buff *skb, u32 seq, u32 ack,
+			    u32 win, u32 ts, int oif,
+			    struct tcp_md5sig_key *key,
+			    int reply_flags);
+
+
+static struct list_head tk_hashtable[MTCP_HASH_SIZE];
+static rwlock_t tk_hash_lock; /*hashtable protection*/
+
+/*This second hashtable is needed to retrieve request socks
+  created as a result of a join request. While the SYN contains
+  the token, the final ack does not, so we need a separate hashtable
+  to retrieve the mpcb.*/
+static struct list_head tuple_hashtable[MTCP_HASH_SIZE];
+static spinlock_t tuple_hash_lock; /*hashtable protection*/
+
+/* General initialization of MTCP_PM
+ */
+static int __init mtcp_pm_init(void) 
+{
+	int i;
+	for (i=0;i<MTCP_HASH_SIZE;i++) {
+		INIT_LIST_HEAD(&tk_hashtable[i]);
+		INIT_LIST_HEAD(&tuple_hashtable[i]);
+	}
+	
+	rwlock_init(&tk_hash_lock);
+	spin_lock_init(&tuple_hash_lock);
+
+	return 0;
+}
+
+void mtcp_hash_insert(struct multipath_pcb *mpcb,u32 token)
+{
+	int hash=hash_tk(token);
+	write_lock_bh(&tk_hash_lock);
+	list_add(&mpcb->collide_tk,&tk_hashtable[hash]);
+	write_unlock_bh(&tk_hash_lock);
+}
+
+/*This function increments the refcount of the mpcb struct.
+  It is the responsibility of the caller to decrement when releasing
+  the structure.*/
+struct multipath_pcb* mtcp_hash_find(u32 token)
+{
+	int hash=hash_tk(token);
+	struct multipath_pcb *mpcb;
+	read_lock(&tk_hash_lock);
+	list_for_each_entry(mpcb,&tk_hashtable[hash],collide_tk) {
+		if (token==loc_token(mpcb)) {
+			mpcb_get(mpcb);
+			read_unlock(&tk_hash_lock);
+			return mpcb;
+		}
+	}
+	read_unlock(&tk_hash_lock);
+	return NULL;
+}
+
+void mtcp_hash_remove(struct multipath_pcb *mpcb)
+{
+	struct inet_connection_sock *mpcb_icsk=
+		(struct inet_connection_sock*)mpcb;
+	struct listen_sock *lopt = mpcb_icsk->icsk_accept_queue.listen_opt;
+	
+	/*remove from the token hashtable*/
+	write_lock_bh(&tk_hash_lock);
+	list_del(&mpcb->collide_tk);
+	write_unlock_bh(&tk_hash_lock);
+	
+	/*Remove all pending request socks.
+	  Note that the lock order is important: it must respect
+	  the order in mtcp_search_req() to avoid deadlocks*/
+	spin_lock_bh(&tuple_hash_lock);
+	spin_lock_bh(&mpcb->lock);
+	if (lopt->qlen != 0) {
+		unsigned int i;
+		for (i = 0; i < lopt->nr_table_entries; i++) {
+			struct request_sock *cur_ref;
+			cur_ref = lopt->syn_table[i];
+			while (cur_ref) {
+				/*remove from global tuple hashtable
+				  We use list_del_init because that
+				  function supports multiple deletes, with
+				  only the first one actually deleting.
+				  This is useful since mtcp_check_req()
+				  might try to remove it as well*/
+				list_del_init(&cur_ref->collide_tuple);
+				/*remove from local hashtable*/
+				cur_ref=cur_ref->dl_next;
+			}
+		}
+	}
+	spin_unlock_bh(&mpcb->lock);
+	spin_unlock_bh(&tuple_hash_lock);
+}
+
+void mtcp_pm_release(struct multipath_pcb *mpcb)
+{
+	struct inet_connection_sock *mpcb_icsk=
+		(struct inet_connection_sock*)mpcb;
+	struct listen_sock *lopt = mpcb_icsk->icsk_accept_queue.listen_opt;
+		
+	/*Remove all pending request socks.*/
+	if (lopt->qlen != 0) {
+		unsigned int i;
+		for (i = 0; i < lopt->nr_table_entries; i++) {
+			struct request_sock **cur_ref;
+			cur_ref = &lopt->syn_table[i];
+			while (*cur_ref) {
+				struct request_sock *todel;
+				printk(KERN_ERR 
+				       "Destroying request_sock\n");
+				lopt->qlen--;
+				todel=*cur_ref;
+				/*remove from local hashtable, it has
+				  been removed already from the global one by 
+				  mtcp_hash_remove()*/
+				*cur_ref=(*cur_ref)->dl_next;
+				reqsk_free(todel);
+			}
+		}
+	}
+
+	/*remove all pending child socks associated to this mpcb*/
+	while (!reqsk_queue_empty(&mpcb_icsk->icsk_accept_queue)) {
+		struct sock *child;
+		struct request_sock *req;
+		req = reqsk_queue_remove(&mpcb_icsk->icsk_accept_queue);
+		child=req->sk;
+
+		/*The code hereafter comes from 
+		  inet_csk_listen_stop()*/
+		bh_lock_sock(child);
+		WARN_ON(sock_owned_by_user(child));
+		sock_hold(child);
+		
+		tcp_disconnect(child, O_NONBLOCK);
+		
+		sock_orphan(child);
+		
+		inet_csk_destroy_sock(child);
+		
+		bh_unlock_sock(child);
+		sock_put(child);
+		reqsk_free(req);
+	}
+}
+
+/* Generates a token for a new MPTCP connection
+ * Currently we assign sequential tokens to
+ * successive MPTCP connections. In the future we
+ * will need to define random tokens, while avoiding
+ * collisions.
+ */
+u32 mtcp_new_token(void)
+{
+	static atomic_t latest_token={.counter=0};
+	return atomic_inc_return(&latest_token);
+}
+
+struct path4 *find_path_mapping4(struct in_addr *loc,struct in_addr *rem,
+				 struct multipath_pcb *mpcb)
+{
+	int i;
+	for (i=0;i<mpcb->pa4_size;i++)
+		if (mpcb->pa4[i].loc.addr.s_addr == loc->s_addr &&
+		    mpcb->pa4[i].rem.addr.s_addr == rem->s_addr)
+			return &mpcb->pa4[i];
+	return NULL;
+}
+
+struct in_addr *mtcp_get_loc_addr(struct multipath_pcb *mpcb, int path_index)
+{
+	int i;
+ 	if (path_index<=1)
+		return (struct in_addr*)&mpcb->local_ulid.a4;
+	for (i=0;i<mpcb->pa4_size;i++) {
+		if (mpcb->pa4[i].path_index==path_index)
+			return &mpcb->pa4[i].loc.addr;
+	}
+	BUG();
+	return NULL;
+}
+
+struct in_addr *mtcp_get_rem_addr(struct multipath_pcb *mpcb, int path_index)
+{
+	int i;
+ 	if (path_index<=1)
+		return (struct in_addr*)&mpcb->remote_ulid.a4;
+	for (i=0;i<mpcb->pa4_size;i++) {
+		if (mpcb->pa4[i].path_index==path_index)
+			return &mpcb->pa4[i].rem.addr;
+	}
+	
+	/*should not arrive here*/
+	printk(KERN_ERR "pa4_size:%d,pi:%d\n",mpcb->pa4_size,path_index);
+	for (i=0;i<mpcb->pa4_size;i++) {
+		printk(KERN_ERR "existing pi:%d\n",mpcb->pa4[i].path_index);
+	}
+	
+	BUG();
+	return NULL;
+}
+
+u8 mtcp_get_loc_addrid(struct multipath_pcb *mpcb, int path_index)
+{
+	int i;
+	/*master subsocket has both addresses with id 0*/
+	if (path_index<=1) return 0;
+	for (i=0;i<mpcb->pa4_size;i++) {
+		if (mpcb->pa4[i].path_index==path_index)
+			return mpcb->pa4[i].loc.id;
+	}
+	/*should not arrive here*/
+	printk(KERN_ERR "pa4_size:%d,pi:%d\n",mpcb->pa4_size,path_index);
+	for (i=0;i<mpcb->pa4_size;i++) {
+		printk(KERN_ERR "existing pi:%d\n",mpcb->pa4[i].path_index);
+	}
+	
+	BUG();
+	return -1;
+}
+
+
+/*For debugging*/
+void print_patharray(struct path4 *pa, int size)
+{
+	int i;
+	printk(KERN_ERR "==================\n");
+	for (i=0;i<size;i++) {
+		printk(KERN_ERR NIPQUAD_FMT "/%d->"
+		       NIPQUAD_FMT "/%d, pi %d\n",
+		       NIPQUAD(pa[i].loc.addr),pa[i].loc.id,
+		       NIPQUAD(pa[i].rem.addr),pa[i].rem.id,
+		       pa[i].path_index);
+	}
+}
+
+
+
+/*This is the MPTCP PM mapping table*/
+static void __mtcp_update_patharray(struct multipath_pcb *mpcb)
+{
+	struct path4 *new_pa4, *old_pa4;
+	int i,j,newpa_idx=0;
+	struct sock *mpcb_sk=(struct sock *)mpcb;
+	/*Count how many paths are available
+	  We add 1 to size of local and remote set, to include the 
+	  ULID*/
+	int ulid_v4=(mpcb_sk->sk_family==AF_INET)?1:0;
+	int pa4_size=(mpcb->num_addr4+ulid_v4)*
+		(mpcb->received_options.num_addr4+ulid_v4)-ulid_v4;
+
+	new_pa4=kmalloc(pa4_size*sizeof(struct path4),GFP_ATOMIC);
+	
+	if (ulid_v4) {
+		/*ULID src with other dest*/
+		for (j=0;j<mpcb->received_options.num_addr4;j++) {
+			struct path4 *p=find_path_mapping4(
+				(struct in_addr*)&mpcb->local_ulid.a4,
+				&mpcb->received_options.addr4[j].addr,mpcb);
+			if (p)
+				memcpy(&new_pa4[newpa_idx++],p,
+				       sizeof(struct path4));
+			else {
+				/*local addr*/
+				new_pa4[newpa_idx].loc.addr.s_addr=
+					mpcb->local_ulid.a4;
+				new_pa4[newpa_idx].loc.id=0; /*ulid has id 0*/
+				/*remote addr*/
+				memcpy(&new_pa4[newpa_idx].rem,
+				       &mpcb->received_options.addr4[j],
+				       sizeof(struct mtcp_loc4));
+				/*new path index to be given*/
+				new_pa4[newpa_idx++].path_index=
+					mpcb->next_unused_pi++;
+			}			
+		}
+		/*ULID dest with other src*/
+		for (i=0;i<mpcb->num_addr4;i++) {
+			struct path4 *p=find_path_mapping4(
+				&mpcb->addr4[i].addr,
+				(struct in_addr*)&mpcb->remote_ulid.a4,mpcb);
+			if (p)
+				memcpy(&new_pa4[newpa_idx++],p,
+				       sizeof(struct path4));
+			else {
+				/*local addr*/
+				memcpy(&new_pa4[newpa_idx].loc,
+				       &mpcb->addr4[i],
+				       sizeof(struct mtcp_loc4));
+				
+				/*remote addr*/
+				new_pa4[newpa_idx].rem.addr.s_addr=
+					mpcb->remote_ulid.a4;
+				new_pa4[newpa_idx].rem.id=0; /*ulid has id 0*/
+				/*new path index to be given*/
+				new_pa4[newpa_idx++].path_index=
+					mpcb->next_unused_pi++;
+			}
+		}
+	}
+	/*Try all other combinations now*/
+	for (i=0;i<mpcb->num_addr4;i++)
+		for (j=0;j<mpcb->received_options.num_addr4;j++) {
+			struct path4 *p=find_path_mapping4(
+				&mpcb->addr4[i].addr,
+				&mpcb->received_options.addr4[j].addr,mpcb);
+			if (p)
+				memcpy(&new_pa4[newpa_idx++],p,
+				       sizeof(struct path4));	
+			else {
+				/*local addr*/
+				memcpy(&new_pa4[newpa_idx].loc,
+				       &mpcb->addr4[i],
+				       sizeof(struct mtcp_loc4));
+				/*remote addr*/
+				memcpy(&new_pa4[newpa_idx].rem,
+				       &mpcb->received_options.addr4[j],
+				       sizeof(struct mtcp_loc4));
+				
+				/*new path index to be given*/
+				new_pa4[newpa_idx++].path_index=
+					mpcb->next_unused_pi++;
+			}
+		}
+	
+	
+	/*Replacing the mapping table*/
+	old_pa4=mpcb->pa4;
+	mpcb->pa4=new_pa4;
+	mpcb->pa4_size=pa4_size;
+	if (old_pa4) kfree(old_pa4);
+}
+
+void mtcp_update_patharray(struct multipath_pcb *mpcb)
+{
+	spin_lock_bh(&mpcb->lock);
+	__mtcp_update_patharray(mpcb);
+	spin_unlock_bh(&mpcb->lock);
+}
+
+void mtcp_set_addresses(struct multipath_pcb *mpcb)
+{
+	struct net_device *dev;
+	int id = 1;
+	int num_addr4 = 0;
+
+	read_lock(&dev_base_lock); 
+
+	for_each_netdev (&init_net,dev) {
+		if (netif_running(dev)) {
+			struct in_device *in_dev = dev->ip_ptr;
+			struct in_ifaddr *ifa;
+			
+			if (dev->flags & IFF_LOOPBACK)
+				continue;
+			
+			for (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next) {
+				if (num_addr4 == MTCP_MAX_ADDR) {
+					mtcp_debug("%s: At max num of local addresses: "
+							"%d --- not adding address: " NIPQUAD_FMT "\n",
+							__FUNCTION__, MTCP_MAX_ADDR, NIPQUAD(ifa->ifa_address));
+					goto out;
+				}
+
+				if (ifa->ifa_address ==
+						inet_sk(mpcb->master_sk)->saddr)
+					continue;
+				if (ifa->ifa_scope == RT_SCOPE_HOST)
+					continue;
+				mpcb->addr4[num_addr4].addr.s_addr =
+					ifa->ifa_address;
+				mpcb->addr4[num_addr4++].id = id++;
+			}
+		}
+	}
+
+out:
+	read_unlock(&dev_base_lock);
+
+	/*We update num_addr4 at the end to avoid racing with the ADDR option
+	  trigger (in tcp_established_options()), 
+	  which can interrupt us in the middle of this function,
+	  and decide to already send the set of addresses, even though all
+	  addresses have not yet been read.*/
+	mpcb->num_addr4 = mpcb->addr_unsent = num_addr4;
+}
+
+/**
+ * Based on function tcp_v4_conn_request (tcp_ipv4.c)
+ * Returns -1 if there is no space anymore to store an additional 
+ * address
+ */
+int mtcp_v4_add_raddress(struct multipath_options *mopt,			
+			 struct in_addr *addr, u8 id)
+{
+	int i;
+	int num_addr4 = mopt->num_addr4;
+
+	/*If the id is zero, this is the ULID, do not add it.*/
+	if (!id) return 0;
+	
+	BUG_ON(num_addr4 > MTCP_MAX_ADDR);
+
+	for (i = 0; i < num_addr4; i++) {
+		/* Address is already in the list --- continue */
+		if (mopt->addr4[i].addr.s_addr == addr->s_addr)
+			return 0;
+
+		/* This may be the case, when the peer is behind a NAT. He is
+		 * trying to JOIN, thus sending the JOIN with a certain ID.
+		 * However the src_addr of the IP-packet has been changed. We
+		 * update the addr in the list, because this is the address as
+		 * OUR BOX sees it. */
+		if (mopt->addr4[i].id == id &&
+		    mopt->addr4[i].addr.s_addr != addr->s_addr) {
+			/* update the address*/
+			mtcp_debug("%s: updating old addr:" NIPQUAD_FMT
+					" to addr " NIPQUAD_FMT " with id:%d\n",
+					__FUNCTION__,
+					NIPQUAD(mopt->addr4[i].addr.s_addr),
+					NIPQUAD(addr->s_addr), id);
+			mopt->addr4[i].addr.s_addr = addr->s_addr;
+			mopt->list_rcvd = 1;
+			return 0;
+		}
+	}
+
+	/* Do we have already the maximum number of local/remote addresses? */
+	if (num_addr4 == MTCP_MAX_ADDR) {
+		mtcp_debug("%s: At max num of remote addresses: %d --- not "
+				"adding address: " NIPQUAD_FMT "\n",
+				__FUNCTION__, MTCP_MAX_ADDR, NIPQUAD(addr->s_addr));
+		return -1;
+	}
+
+	/*Address is not known yet, store it*/
+	mopt->addr4[num_addr4].addr.s_addr = addr->s_addr;
+	mopt->addr4[num_addr4].id = id;
+	mopt->list_rcvd = 1;
+	mopt->num_addr4++;
+
+	return 0;
+}
+
+
+static struct dst_entry* mtcp_route_req(const struct request_sock *req)
+{
+	struct rtable *rt;
+	const struct inet_request_sock *ireq = inet_rsk(req);
+	struct ip_options *opt = inet_rsk(req)->opt;
+	struct flowi fl = { .nl_u = { .ip4_u =
+				      { .daddr = ((opt && opt->srr) ?
+						  opt->faddr :
+						  ireq->rmt_addr),
+					.saddr = ireq->loc_addr } },
+			    .proto = IPPROTO_TCP,
+			    .flags = 0,
+			    .uli_u = { .ports =
+				       { .sport = ireq->loc_port,
+					 .dport = ireq->rmt_port } } };
+	security_req_classify_flow(req, &fl);
+	if (ip_route_output_flow(&init_net, &rt, &fl, NULL, 0)) {
+		IP_INC_STATS_BH(&init_net, IPSTATS_MIB_OUTNOROUTES);
+		return NULL;
+	}
+	if (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway) {
+		ip_rt_put(rt);
+		IP_INC_STATS_BH(&init_net, IPSTATS_MIB_OUTNOROUTES);
+		return NULL;
+	}
+	return &rt->u.dst;
+}
+
+static unsigned mtcp_synack_options(struct request_sock *req,
+				    unsigned mss, struct sk_buff *skb,
+				    struct tcp_out_options *opts,
+				    struct tcp_md5sig_key **md5)
+{
+	unsigned size = 0;
+	struct inet_request_sock *ireq = inet_rsk(req);
+	char doing_ts;
+
+	*md5 = NULL;
+
+	/* we can't fit any SACK blocks in a packet with MD5 + TS
+	   options. There was discussion about disabling SACK rather than TS in
+	   order to fit in better with old, buggy kernels, but that was deemed
+	   to be unnecessary. */
+	doing_ts = ireq->tstamp_ok && !(*md5 && ireq->sack_ok);
+
+	opts->mss = mss;
+	size += TCPOLEN_MSS_ALIGNED;
+
+	if (likely(ireq->wscale_ok)) {
+		opts->ws = ireq->rcv_wscale;
+		if(likely(opts->ws))
+			size += TCPOLEN_WSCALE_ALIGNED;
+	}
+	if (likely(doing_ts)) {
+		opts->options |= OPTION_TS;
+		opts->tsval = TCP_SKB_CB(skb)->when;
+		opts->tsecr = req->ts_recent;
+		size += TCPOLEN_TSTAMP_ALIGNED;
+	}
+	if (likely(ireq->sack_ok)) {
+		opts->options |= OPTION_SACK_ADVERTISE;
+		if (unlikely(!doing_ts))
+			size += TCPOLEN_SACKPERM_ALIGNED;
+	}
+
+	return size;
+}
+
+static __inline__ void
+TCP_ECN_make_synack(struct request_sock *req, struct tcphdr *th)
+{
+	if (inet_rsk(req)->ecn_ok)
+		th->ece = 1;
+}
+
+/*
+ * Prepare a SYN-ACK, for JOINed subflows
+ */
+static struct sk_buff *mtcp_make_synack(struct sock *master_sk, 
+					struct dst_entry *dst,
+					struct request_sock *req)
+{
+	struct inet_request_sock *ireq = inet_rsk(req);
+	struct tcp_sock *master_tp = tcp_sk(master_sk);
+	struct tcphdr *th;
+	int tcp_header_size;
+	struct tcp_out_options opts;
+	struct sk_buff *skb;
+	struct tcp_md5sig_key *md5;
+	__u8 *md5_hash_location;
+	int mss;
+
+	skb = alloc_skb(MAX_TCP_HEADER + 15, GFP_ATOMIC);
+	if (skb == NULL)
+		return NULL;
+	
+	/* Reserve space for headers. */
+	skb_reserve(skb, MAX_TCP_HEADER);
+
+	skb->dst = dst_clone(dst);
+
+	mss = dst_metric(dst, RTAX_ADVMSS);
+	if (master_tp->rx_opt.user_mss && master_tp->rx_opt.user_mss < mss)
+		mss = master_tp->rx_opt.user_mss;
+
+	if (req->rcv_wnd == 0) { /* ignored for retransmitted syns */
+		__u8 rcv_wscale;
+		/* Set this up on the first call only */
+		req->window_clamp = dst_metric(dst, RTAX_WINDOW);
+		/* tcp_full_space because it is guaranteed to be the first 
+		   packet */
+		tcp_select_initial_window(
+			tcp_win_from_space(sysctl_rmem_default),
+			mss - (ireq->tstamp_ok ? TCPOLEN_TSTAMP_ALIGNED : 0),
+			&req->rcv_wnd,
+			&req->window_clamp,
+			ireq->wscale_ok,
+			&rcv_wscale);
+		ireq->rcv_wscale = rcv_wscale;
+	}
+
+	memset(&opts, 0, sizeof(opts));
+
+	TCP_SKB_CB(skb)->when = tcp_time_stamp;
+	tcp_header_size = mtcp_synack_options(req, mss,
+					      skb, &opts, &md5) +
+		sizeof(struct tcphdr);       
+	
+	skb_push(skb, tcp_header_size);
+	skb_reset_transport_header(skb);
+
+	th = tcp_hdr(skb);
+	memset(th, 0, sizeof(struct tcphdr));
+	th->syn = 1;
+	th->ack = 1;
+	TCP_ECN_make_synack(req, th);
+	th->source = ireq->loc_port;
+	th->dest = ireq->rmt_port;
+	/* Setting of flags are superfluous here for callers (and ECE is
+	 * not even correctly set)
+	 */
+	tcp_init_nondata_skb(skb, tcp_rsk(req)->snt_isn,
+			     TCPCB_FLAG_SYN | TCPCB_FLAG_ACK);
+	th->seq = htonl(TCP_SKB_CB(skb)->seq);
+	th->ack_seq = htonl(tcp_rsk(req)->rcv_isn + 1);
+	
+	/* RFC1323: The window in SYN & SYN/ACK segments is never scaled. */
+	th->window = htons(min(req->rcv_wnd, 65535U));
+	tcp_options_write((__be32 *)(th + 1), NULL, &opts, &md5_hash_location);
+	th->doff = (tcp_header_size >> 2);
+
+	return skb;
+}
+
+/*
+ *	Send a SYN-ACK after having received a SYN.
+ *	This still operates on a request_sock only, not on a big
+ *	socket.
+ */
+static int __mtcp_v4_send_synack(struct sock *master_sk,
+				 struct request_sock *req,
+				 struct dst_entry *dst)
+{
+	const struct inet_request_sock *ireq = inet_rsk(req);
+	int err = -1;
+	struct sk_buff * skb;
+
+	/* First, grab a route. */
+	if (!dst && (dst = mtcp_route_req(req)) == NULL)
+		return -1;
+
+	skb = mtcp_make_synack(master_sk, dst, req);
+
+	if (skb) {
+		struct tcphdr *th = tcp_hdr(skb);
+
+		th->check = tcp_v4_check(skb->len,
+					 ireq->loc_addr,
+					 ireq->rmt_addr,
+					 csum_partial((char *)th, skb->len,
+						      skb->csum));
+
+		err = ip_build_and_send_pkt(skb, master_sk, ireq->loc_addr,
+					    ireq->rmt_addr,
+					    ireq->opt);
+		err = net_xmit_eval(err);
+	}
+
+	dst_release(dst);
+	return err;
+}
+
+/*Copied from net/ipv4/inet_connection_sock.c*/
+static inline u32 inet_synq_hash(const __be32 raddr, const __be16 rport,
+				 const u32 rnd, const u32 synq_hsize)
+{
+	return jhash_2words((__force u32)raddr, (__force u32)rport, rnd) & (synq_hsize - 1);
+}
+
+static void mtcp_reqsk_queue_hash_add(struct request_sock *req,
+				      unsigned long timeout)
+{
+	struct inet_connection_sock *mpcb_icsk=
+		(struct inet_connection_sock*)(req->mpcb);
+	struct listen_sock *lopt = mpcb_icsk->icsk_accept_queue.listen_opt;
+	const u32 h_local = inet_synq_hash(inet_rsk(req)->rmt_addr, 
+					   inet_rsk(req)->rmt_port,
+					   lopt->hash_rnd, 
+					   lopt->nr_table_entries);
+	const u32 h_global = inet_synq_hash(inet_rsk(req)->rmt_addr, 
+					    inet_rsk(req)->rmt_port,
+					    0, 
+					    MTCP_HASH_SIZE);
+	spin_lock(&tuple_hash_lock);
+	spin_lock(&req->mpcb->lock);
+	reqsk_queue_hash_req(&mpcb_icsk->icsk_accept_queue, 
+			     h_local, req, timeout);
+	list_add(&req->collide_tuple,&tuple_hashtable[h_global]);
+	spin_unlock(&req->mpcb->lock);
+	spin_unlock(&tuple_hash_lock);
+}
+
+/*Copied from tcp_ipv4.c*/
+static inline __u32 tcp_v4_init_sequence(struct sk_buff *skb)
+{
+	return secure_tcp_sequence_number(ip_hdr(skb)->daddr,
+					  ip_hdr(skb)->saddr,
+					  tcp_hdr(skb)->dest,
+					  tcp_hdr(skb)->source);
+}
+
+static int mtcp_v4_join_request(struct multipath_pcb *mpcb, struct sk_buff *skb)
+{
+	struct inet_request_sock *ireq;
+	struct request_sock *req;
+	struct tcp_options_received tmp_opt;
+	__be32 saddr = ip_hdr(skb)->saddr;
+	__be32 daddr = ip_hdr(skb)->daddr;
+	__u32 isn = TCP_SKB_CB(skb)->when;	
+
+	req = inet_reqsk_alloc(&tcp_request_sock_ops);
+	if (!req)
+		return -1;
+	
+	tcp_clear_options(&tmp_opt);
+	tmp_opt.mss_clamp = 536;
+	tmp_opt.user_mss  = tcp_sk(mpcb->master_sk)->rx_opt.user_mss;
+	
+	tcp_parse_options(skb, &tmp_opt, &mpcb->received_options, 0);
+	
+	if (tmp_opt.saw_tstamp && !tmp_opt.rcv_tsval) {
+		/* Some OSes (unknown ones, but I see them on web server, which
+		 * contains information interesting only for windows'
+		 * users) do not send their stamp in SYN. It is easy case.
+		 * We simply do not advertise TS support.
+		 */
+		tmp_opt.saw_tstamp = 0;
+		tmp_opt.tstamp_ok  = 0;
+	}
+	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+
+	req->mpcb=mpcb;
+	req->mtcp_loc_token = loc_token(mpcb);
+	tcp_openreq_init(req, &tmp_opt, skb);
+
+	ireq = inet_rsk(req);
+	ireq->loc_addr = daddr;
+	ireq->rmt_addr = saddr;
+	ireq->opt = tcp_v4_save_options(NULL, skb);
+
+	/*Todo: add the sanity checks here. See tcp_v4_conn_request*/
+
+	isn = tcp_v4_init_sequence(skb);
+
+	tcp_rsk(req)->snt_isn = isn;
+
+ 	if (__mtcp_v4_send_synack(mpcb->master_sk, req, NULL))
+		goto drop_and_free;
+
+	/*Adding to request queue in metasocket*/
+	mtcp_reqsk_queue_hash_add(req, TCP_TIMEOUT_INIT);
+	return 0;
+
+drop_and_free:
+	reqsk_free(req);
+	return -1;
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#define AF_INET_FAMILY(fam) ((fam) == AF_INET)
+#else
+#define AF_INET_FAMILY(fam) 1
+#endif
+
+
+/*Removes a request sock from its local mpcb hashtable*/
+static void mtcp_reqsk_local_remove(struct request_sock *r)
+{
+	struct multipath_pcb *mpcb=r->mpcb;
+	struct inet_connection_sock *mpcb_icsk=
+		(struct inet_connection_sock*)mpcb;
+	struct listen_sock *lopt = mpcb_icsk->icsk_accept_queue.listen_opt;
+	struct request_sock *req,**prev;
+	const struct inet_request_sock *i = inet_rsk(r);
+
+	for (prev = &lopt->syn_table[inet_synq_hash(i->rmt_addr, i->rmt_port, 
+						    lopt->hash_rnd,
+						    lopt->nr_table_entries)];
+	     (req = *prev) != NULL;
+	     prev = &req->dl_next) {
+		if (req==r)
+			break;
+	}
+	BUG_ON(!req);
+
+	reqsk_queue_unlink(&mpcb_icsk->icsk_accept_queue, req, prev);
+}
+
+/*inspired from inet_csk_search_req
+ * After this, the kref count of the mpcb associated with the request_sock
+ * is incremented. Thus it is the responsibility of the caller
+ * to call mpcb_put() when the reference is not needed anymore.
+ */
+static struct request_sock *mtcp_search_req(const __be16 rport, 
+					    const __be32 raddr,
+					    const __be32 laddr)
+{
+	struct request_sock *req;
+	int found=0;
+	
+	spin_lock(&tuple_hash_lock);
+	list_for_each_entry(req,&tuple_hashtable[
+				    inet_synq_hash(raddr, rport, 0, 
+						   MTCP_HASH_SIZE)],
+			    collide_tuple) {
+		const struct inet_request_sock *ireq = inet_rsk(req);
+		
+		if (ireq->rmt_port == rport &&
+		    ireq->rmt_addr == raddr &&
+		    ireq->loc_addr == laddr &&
+		    AF_INET_FAMILY(req->rsk_ops->family)) {
+			WARN_ON(req->sk);
+			found=1;
+			break;
+		}
+	}
+	
+	if (found) mpcb_get(req->mpcb);
+	spin_unlock(&tuple_hash_lock);
+
+	if (!found) return NULL;
+
+	return req;
+}
+
+/*copied from net/ipv4/tcp_minisocks.c*/
+static __inline__ int tcp_in_window(u32 seq, u32 end_seq, u32 s_win, u32 e_win)
+{
+	if (seq == s_win)
+		return 1;
+	if (after(end_seq, s_win) && before(seq, e_win))
+		return 1;
+	return (seq == e_win && seq == end_seq);
+}
+
+static inline void mtcp_reqsk_queue_add(struct request_sock_queue *queue,
+					struct request_sock *req,
+					struct sock *child)
+{
+	req->sk = child;
+
+	if (queue->rskq_accept_head == NULL)
+		queue->rskq_accept_head = req;
+	else
+		queue->rskq_accept_tail->dl_next = req;
+
+	queue->rskq_accept_tail = req;
+	req->dl_next = NULL;
+}
+
+/*
+ *      (adapted from tcp_check_req)    
+ *	Process an incoming packet for SYN_RECV sockets represented
+ *	as a request_sock.
+ */
+
+static struct sock *mtcp_check_req(struct sk_buff *skb,
+				   struct request_sock *req)
+{
+	const struct tcphdr *th = tcp_hdr(skb);
+	__be32 flg = tcp_flag_word(th) & 
+		(TCP_FLAG_RST|TCP_FLAG_SYN|TCP_FLAG_ACK);
+	int paws_reject = 0;
+	struct tcp_options_received tmp_opt;
+	struct multipath_options mtp;
+	struct sock *child;
+	struct multipath_pcb *mpcb=req->mpcb;
+	struct inet_connection_sock *mpcb_icsk=
+		(struct inet_connection_sock*)mpcb;
+
+	if (!inet_csk(mpcb->master_sk)->icsk_bind_hash) {
+		/*This cannot happen, because the bind hash must be inherited
+		  by the child we are trying to create.*/
+		printk(KERN_ERR "no bind bucket in master_sk\n");
+		printk(KERN_ERR "mpcb flags:%lx\n",mpcb->flags);
+		printk(KERN_ERR "master sk addr: " NIPQUAD_FMT ":%d->"
+		       NIPQUAD_FMT ":%d\n",
+		       NIPQUAD(inet_sk(mpcb->master_sk)->saddr),
+		       ntohs(inet_sk(mpcb->master_sk)->sport),
+		       NIPQUAD(inet_sk(mpcb->master_sk)->daddr),
+		       ntohs(inet_sk(mpcb->master_sk)->dport));
+		BUG();
+	}
+
+	tmp_opt.saw_tstamp = 0;
+	if (th->doff > (sizeof(struct tcphdr)>>2)) {
+		tcp_parse_options(skb, &tmp_opt, &mtp, 0);
+
+		if (tmp_opt.saw_tstamp) {
+			tmp_opt.ts_recent = req->ts_recent;
+			/* We do not store true stamp, but it is not required,
+			 * it can be estimated (approximately)
+			 * from another data.
+			 */
+			tmp_opt.ts_recent_stamp = get_seconds() - ((TCP_TIMEOUT_INIT/HZ)<<req->retrans);
+			paws_reject = tcp_paws_check(&tmp_opt, th->rst);
+		}
+	}
+
+	/* Check for pure retransmitted SYN. */
+	if (TCP_SKB_CB(skb)->seq == tcp_rsk(req)->rcv_isn &&
+	    flg == TCP_FLAG_SYN &&
+	    !paws_reject) {
+		/*
+		 * RFC793 draws (Incorrectly! It was fixed in RFC1122)
+		 * this case on figure 6 and figure 8, but formal
+		 * protocol description says NOTHING.
+		 * To be more exact, it says that we should send ACK,
+		 * because this segment (at least, if it has no data)
+		 * is out of window.
+		 *
+		 *  CONCLUSION: RFC793 (even with RFC1122) DOES NOT
+		 *  describe SYN-RECV state. All the description
+		 *  is wrong, we cannot believe to it and should
+		 *  rely only on common sense and implementation
+		 *  experience.
+		 *
+		 * Enforce "SYN-ACK" according to figure 8, figure 6
+		 * of RFC793, fixed by RFC1122.
+		 */
+		__mtcp_v4_send_synack(mpcb->master_sk, req, NULL);
+		return NULL;
+	}
+
+	/* Further reproduces section "SEGMENT ARRIVES"
+	   for state SYN-RECEIVED of RFC793.
+	   It is broken, however, it does not work only
+	   when SYNs are crossed.
+
+	   You would think that SYN crossing is impossible here, since
+	   we should have a SYN_SENT socket (from connect()) on our end,
+	   but this is not true if the crossed SYNs were sent to both
+	   ends by a malicious third party.  We must defend against this,
+	   and to do that we first verify the ACK (as per RFC793, page
+	   36) and reset if it is invalid.  Is this a true full defense?
+	   To convince ourselves, let us consider a way in which the ACK
+	   test can still pass in this 'malicious crossed SYNs' case.
+	   Malicious sender sends identical SYNs (and thus identical sequence
+	   numbers) to both A and B:
+
+		A: gets SYN, seq=7
+		B: gets SYN, seq=7
+
+	   By our good fortune, both A and B select the same initial
+	   send sequence number of seven :-)
+
+		A: sends SYN|ACK, seq=7, ack_seq=8
+		B: sends SYN|ACK, seq=7, ack_seq=8
+
+	   So we are now A eating this SYN|ACK, ACK test passes.  So
+	   does sequence test, SYN is truncated, and thus we consider
+	   it a bare ACK.
+
+	   If icsk->icsk_accept_queue.rskq_defer_accept, we silently drop this
+	   bare ACK.  Otherwise, we create an established connection.  Both
+	   ends (listening sockets) accept the new incoming connection and try
+	   to talk to each other. 8-)
+
+	   Note: This case is both harmless, and rare.  Possibility is about the
+	   same as us discovering intelligent life on another plant tomorrow.
+
+	   But generally, we should (RFC lies!) to accept ACK
+	   from SYNACK both here and in tcp_rcv_state_process().
+	   tcp_rcv_state_process() does not, hence, we do not too.
+
+	   Note that the case is absolutely generic:
+	   we cannot optimize anything here without
+	   violating protocol. All the checks must be made
+	   before attempt to create socket.
+	 */
+
+	/* RFC793 page 36: "If the connection is in any non-synchronized 
+	 *                  state ... and the incoming segment acknowledges 
+	 *                  something not yet sent (the segment carries an 
+	 *                  unacceptable ACK) ... a reset is sent."
+	 *
+	 * Invalid ACK: reset will be sent by listening socket
+	 */
+	if ((flg & TCP_FLAG_ACK) &&
+	    (TCP_SKB_CB(skb)->ack_seq != tcp_rsk(req)->snt_isn + 1))
+		req->rsk_ops->send_reset(NULL,skb);
+
+	/* Also, it would be not so bad idea to check rcv_tsecr, which
+	 * is essentially ACK extension and too early or too late values
+	 * should cause reset in unsynchronized states.
+	 */
+
+	/* RFC793: "first check sequence number". */
+
+	if (paws_reject || !tcp_in_window(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq,
+					  tcp_rsk(req)->rcv_isn + 1, tcp_rsk(req)->rcv_isn + 1 + req->rcv_wnd)) {
+		/* Out of window: send ACK and drop. */
+		if (!(flg & TCP_FLAG_RST))
+			tcp_v4_send_ack(skb, tcp_rsk(req)->snt_isn + 1,
+					tcp_rsk(req)->rcv_isn + 1, req->rcv_wnd,
+					req->ts_recent, 0, NULL,
+					inet_rsk(req)->no_srccheck ? 
+					IP_REPLY_ARG_NOSRCCHECK : 0);
+		return NULL;
+	}
+
+	/* In sequence, PAWS is OK. */
+
+	if (tmp_opt.saw_tstamp && !after(TCP_SKB_CB(skb)->seq, tcp_rsk(req)->rcv_isn + 1))
+		req->ts_recent = tmp_opt.rcv_tsval;
+
+	if (TCP_SKB_CB(skb)->seq == tcp_rsk(req)->rcv_isn) {
+		/* Truncate SYN, it is out of window starting
+		   at tcp_rsk(req)->rcv_isn + 1. */
+		flg &= ~TCP_FLAG_SYN;
+	}
+
+	/* RFC793: "second check the RST bit" and
+	 *	   "fourth, check the SYN bit"
+	 */
+	if (flg & (TCP_FLAG_RST|TCP_FLAG_SYN))
+		goto embryonic_reset;
+
+	/* ACK sequence verified above, just make sure ACK is
+	 * set.  If ACK not set, just silently drop the packet.
+	 */
+	if (!(flg & TCP_FLAG_ACK))
+		return NULL;
+
+	/* OK, ACK is valid, create big socket and
+	 * feed this segment to it. It will repeat all
+	 * the tests. THIS SEGMENT MUST MOVE SOCKET TO
+	 * ESTABLISHED STATE. If it will be dropped after
+	 * socket is created, wait for troubles.
+	 */
+	child = inet_csk(mpcb->master_sk)->icsk_af_ops->
+		syn_recv_sock(mpcb->master_sk, 
+			      skb, req, 
+			      NULL);
+
+	if (child == NULL)
+		goto listen_overflow;
+
+	/*The child is a clone of the master socket, we must now reset
+	  some of the fields*/
+	tcp_sk(child)->mpcb=NULL; /*necessary for inet_csk_detroy_sock()
+				    will be set when removed from the 
+				    accept queue*/
+	tcp_sk(child)->mpc=1;
+	tcp_sk(child)->slave_sk=1;
+	tcp_sk(child)->rx_opt.mtcp_rem_token=req->mtcp_rem_token;
+	tcp_sk(child)->mtcp_loc_token=req->mtcp_loc_token;
+	tcp_sk(child)->pending=1;
+	tcp_sk(child)->bw_est.time=0;
+		
+	child->sk_sndmsg_page=NULL;
+	
+	/*Deleting from global hashtable*/
+	spin_lock(&tuple_hash_lock);
+	/*list_del_init: see comment in mtcp_hash_remove()*/
+	list_del_init(&req->collide_tuple);
+	spin_unlock(&tuple_hash_lock);
+	/*Deleting from local hashtable*/
+	mtcp_reqsk_local_remove(req);
+	reqsk_queue_removed(&mpcb_icsk->icsk_accept_queue, req);
+	mtcp_reqsk_queue_add(&mpcb_icsk->icsk_accept_queue, req, child);
+	return child;
+	
+listen_overflow:
+	if (!sysctl_tcp_abort_on_overflow) {
+		inet_rsk(req)->acked = 1;
+		return NULL;
+	}
+	
+embryonic_reset:
+	if (!(flg & TCP_FLAG_RST))
+		req->rsk_ops->send_reset(NULL, skb);
+
+	mtcp_reqsk_local_remove(req);
+	reqsk_queue_removed(&mpcb_icsk->icsk_accept_queue, req);
+	reqsk_free(req);
+	return NULL;
+}
+
+int mtcp_syn_recv_sock(struct sk_buff *skb)
+{
+	struct tcphdr *th=tcp_hdr(skb);
+	struct iphdr *iph=ip_hdr(skb);
+	struct request_sock *req;
+	struct sock *child;
+
+	req=mtcp_search_req(th->source,iph->saddr,iph->daddr);
+	if (!req)
+		return 0;
+
+	/*If this is a valid ack, we can build a full socket*/
+	child=mtcp_check_req(skb,req);
+	if (child)
+		tcp_child_process(req->mpcb->master_sk,
+				  child,skb);
+	/*The refcount has been incremented by mtcp_search_req*/
+	mpcb_put(req->mpcb);
+	return 1;
+}
+
+/**
+ *
+ * Returns 1 if a join option has been found, and a new request_sock has been 
+ * created. Else returns 0.
+ */
+int mtcp_lookup_join(struct sk_buff *skb)
+{
+	struct tcphdr *th=tcp_hdr(skb);
+	const struct iphdr *iph = ip_hdr(skb);
+	unsigned char *ptr;
+	int length = (th->doff * 4) - sizeof(struct tcphdr);
+	u32 token;
+	struct multipath_pcb *mpcb;
+	int ans;
+
+	/*Jump through the options to check whether JOIN is there*/
+	ptr = (unsigned char *)(th + 1);
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return 0;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2) /* "silly options" */
+				return 0;
+			if (opsize > length)
+				return 0; /* don't parse partial options */
+			if (opcode==TCPOPT_JOIN) {
+				token=ntohl(*(u32*)ptr);
+				mpcb=mtcp_hash_find(token);			
+				if (!mpcb) {
+					printk(KERN_ERR 
+					       "%s:mpcb not found:%x\n",
+					       __FUNCTION__,token);
+					goto finished;
+				}
+				/*OK, this is a new syn/join, let's 
+				  create a new open request and 
+				  send syn+ack*/
+				ans=mtcp_v4_add_raddress(&mpcb->
+							 received_options, 
+							 (struct in_addr*)
+							 &iph->saddr, *(ptr+4));
+				if (ans<0) {
+					mpcb_put(mpcb);
+					goto finished;
+				}
+				mtcp_v4_join_request(mpcb, skb);		
+				mpcb_put(mpcb);
+				goto finished;
+			}
+			ptr += opsize-2;
+			length -= opsize;
+		}
+	}
+
+	return 0;
+finished:
+	kfree_skb(skb);
+	return 1;
+}
+
+/**
+ *Sends an update notification to the MPS
+ *Since this particular PM works in the TCP layer, that is, the same
+ *as the MPS, we "send" the notif through function call, not message
+ *passing.
+ * Warning: this can be called only from user context, not soft irq
+ **/
+static void mtcp_send_updatenotif(struct multipath_pcb *mpcb)
+{
+	int i;
+	u32 path_indices=1; /*Path index 1 is reserved for master sk.*/
+	for (i=0;i<mpcb->pa4_size;i++) {
+		path_indices|=PI_TO_FLAG(mpcb->pa4[i].path_index);
+	}
+	mtcp_init_subsockets(mpcb,path_indices);
+}
+
+/**
+ * checks whether a new established subflow has appeared,
+ * in which case that subflow is added to the path set. 
+ * @return: the number of newly attached subflows
+ */
+int mtcp_check_new_subflow(struct multipath_pcb *mpcb)
+{
+	struct sock *child;
+	struct request_sock *acc_req, *req;
+	struct inet_request_sock *ireq;
+	struct path4 *p;
+	int nb_new=0;
+	struct inet_connection_sock *mpcb_icsk=
+		(struct inet_connection_sock*)mpcb;
+
+	if (unlikely(mpcb->received_options.list_rcvd)) {
+		mpcb->received_options.list_rcvd=0;
+		mtcp_update_patharray(mpcb);
+		/*The server uses additional subflows only on request
+		  from the client.*/
+		if (!test_bit(MPCB_FLAG_SERVER_SIDE,&mpcb->flags))
+			mtcp_send_updatenotif(mpcb);
+	}
+	
+	spin_lock_bh(&mpcb->lock);
+	/* make all the listen_opt local to us */
+	acc_req = reqsk_queue_yank_acceptq(&mpcb_icsk->icsk_accept_queue);
+	spin_unlock_bh(&mpcb->lock);
+	
+	while ((req = acc_req) != NULL) {
+		acc_req = req->dl_next;
+		ireq =  inet_rsk(req);
+		child = req->sk;
+		BUG_ON(!child);
+		/*Apply correct path index to that subflow*/
+		p=find_path_mapping4((struct in_addr*)&ireq->loc_addr,
+				     (struct in_addr*)&ireq->rmt_addr,
+				     mpcb);
+
+		if (unlikely(!p)) {
+			/*It is possible that we don't find the mapping,
+			  if we have not yet updated our set of local
+			  addresses.*/
+			mtcp_set_addresses(mpcb);
+			/*If this added new local addresses, build new paths 
+			  with them*/
+			if (mpcb->num_addr4 || mpcb->num_addr6) 
+				mtcp_update_patharray(mpcb);
+			/*ok, we are up to date, retry*/
+			p=find_path_mapping4((struct in_addr*)&ireq->loc_addr,
+					     (struct in_addr*)&ireq->rmt_addr,
+					     mpcb);
+			BUG_ON(!p);
+		}
+
+		tcp_sk(child)->path_index=p->path_index;
+		/*Point it to the same struct socket as the master*/
+		sk_set_socket(child,mpcb->master_sk->sk_socket);
+		
+		mtcp_add_sock(mpcb,tcp_sk(child));
+		reqsk_free(req);
+		nb_new++;
+	}
+	return nb_new;
+}
+
+module_init(mtcp_pm_init);
+
+MODULE_LICENSE("GPL");
+
diff --git a/net/ipv4/pm.c b/net/ipv4/pm.c
new file mode 100644
index 0000000..00de6da
--- /dev/null
+++ b/net/ipv4/pm.c
@@ -0,0 +1,77 @@
+/*
+ *	Path Manager implementation - netlink communication with user space.
+ *
+ *	Author:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *
+ *      Support for the Generic Multipath Architecture (GMA).
+ *
+ *      date : June 2009
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <net/netlink.h>
+#include <net/net_namespace.h>
+#include <net/netevent.h>
+
+#include <linux/pm_netlink.h>
+
+struct sock *pmnl_sk;
+
+static void pm_netlink_rcv(struct sk_buff *skb)
+{
+	struct nlmsghdr *nlh;
+	int skblen,nlmsglen;
+	struct nl_ulid_pair *data;
+	struct ulid_pair up;
+		
+	skblen = skb->len;
+	if (skblen < sizeof(*nlh))
+		return;
+	
+	nlh = nlmsg_hdr(skb);
+	nlmsglen = nlh->nlmsg_len;
+	if (nlmsglen < sizeof(*nlh) || skblen < nlmsglen)
+		return;
+
+	switch(nlh->nlmsg_type) {
+	case PM_NL_PATHUPDATE:
+		if (nlmsglen<sizeof(*data)) {
+			printk(KERN_ERR "nlmsglen:%d\n",nlmsglen);
+			return;
+		}
+		data=nlmsg_data(nlh);
+		up.local=&data->local;
+		up.remote=&data->remote;
+		up.path_indices=data->path_indices;
+		call_netevent_notifiers(NETEVENT_PATH_UPDATEV6, &up);
+		break;
+	}
+}
+
+
+int __init pm_netlink_init(void)
+{
+	
+	printk(KERN_INFO "Initializing PM netlink socket\n");
+	
+	pmnl_sk = netlink_kernel_create(&init_net,NETLINK_PM, 
+					PMNLGRP_MAX, 
+					pm_netlink_rcv, NULL,THIS_MODULE);
+	if (!pmnl_sk) {
+		printk(KERN_ERR "PM: failed to create netlink socket\n");
+		return -ENOMEM;
+	}
+	
+	return 0;
+}
+
+module_init(pm_netlink_init);
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index c5aca0b..f9dd0e9 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -275,6 +275,14 @@
 #include <asm/uaccess.h>
 #include <asm/ioctls.h>
 
+#undef DEBUG_TCP /*set to define if you want debugging messages*/
+
+#ifdef DEBUG_TCP
+#define PDEBUG_SEND(fmt,args...) printk( KERN_ERR __FILE__ ": " fmt,##args)
+#else
+#define PDEBUG_SEND(fmt,args...)
+#endif /*DEBUG_TCP*/
+
 int sysctl_tcp_fin_timeout __read_mostly = TCP_FIN_TIMEOUT;
 
 atomic_t tcp_orphan_count = ATOMIC_INIT(0);
@@ -331,6 +339,114 @@ EXPORT_SYMBOL(tcp_enter_memory_pressure);
  *	take care of normal races (between the test and the event) and we don't
  *	go look at any of the socket buffers directly.
  */
+#ifdef CONFIG_MTCP
+unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)
+{
+	unsigned int mask;
+	struct sock *master_sk = sock->sk;
+	struct tcp_sock *master_tp = tcp_sk(master_sk);
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(master_tp);
+	struct sock *mpcb_sk = (master_tp->mpc)?(struct sock *) mpcb:
+		master_sk;
+	struct tcp_sock *mpcb_tp = tcp_sk(mpcb_sk);
+	
+	poll_wait(file, master_sk->sk_sleep, wait);
+
+#ifdef CONFIG_MTCP_PM
+	if (master_tp->mpc)
+		mtcp_check_new_subflow(mpcb);
+#endif
+
+	if (master_sk->sk_state == TCP_LISTEN) {
+		return inet_csk_listen_poll(master_sk);
+	}
+
+	/* Socket is not locked. We are protected from async events
+	 * by poll logic and correct handling of state changes
+	 * made by other threads is impossible in any case.
+	 */
+
+	mask = 0;
+	/*The subsocks are responsible for transferring their errors
+	  here, so that they become visible to the mpcb.*/
+	if (mpcb_sk->sk_err)
+		mask = POLLERR;
+	
+	/*
+	 * POLLHUP is certainly not done right. But poll() doesn't
+	 * have a notion of HUP in just one direction, and for a
+	 * socket the read side is more interesting.
+	 *
+	 * Some poll() documentation says that POLLHUP is incompatible
+	 * with the POLLOUT/POLLWR flags, so somebody should check this
+	 * all. But careful, it tends to be safer to return too many
+	 * bits than too few, and you can easily break real applications
+	 * if you don't tell them that something has hung up!
+	 *
+	 * Check-me.
+	 *
+	 * Check number 1. POLLHUP is _UNMASKABLE_ event (see UNIX98 and
+	 * our fs/select.c). It means that after we received EOF,
+	 * poll always returns immediately, making impossible poll() on write()
+	 * in state CLOSE_WAIT. One solution is evident --- to set POLLHUP
+	 * if and only if shutdown has been made in both directions.
+	 * Actually, it is interesting to look how Solaris and DUX
+	 * solve this dilemma. I would prefer, if POLLHUP were maskable,
+	 * then we could set it on SND_SHUTDOWN. BTW examples given
+	 * in Stevens' books assume exactly this behaviour, it explains
+	 * why POLLHUP is incompatible with POLLOUT.	--ANK
+	 *
+	 * NOTE. Check for TCP_CLOSE is added. The goal is to prevent
+	 * blocking on fresh not-connected or disconnected socket. --ANK
+	 */
+	if (mpcb_sk->sk_shutdown == SHUTDOWN_MASK || 
+	    mpcb_sk->sk_state == TCP_CLOSE)
+		mask |= POLLHUP;
+	if (mpcb_sk->sk_shutdown & RCV_SHUTDOWN)
+		mask |= POLLIN | POLLRDNORM | POLLRDHUP;
+	
+	/* Connected? */
+	
+	if ((1 << master_sk->sk_state) & ~(TCPF_SYN_SENT | TCPF_SYN_RECV)) {
+		int target = sock_rcvlowat(master_sk,0,INT_MAX);
+		
+		if (mpcb_tp->urg_seq == mpcb_tp->copied_seq &&
+		    !sock_flag(master_sk, SOCK_URGINLINE) &&
+		    mpcb_tp->urg_data)
+			target--;
+		
+		/* Potential race condition. If read of tp below will
+		 * escape above sk->sk_state, we can be illegally awaken
+		 * in SYN_* states. */
+		if (mpcb_tp->rcv_nxt - mpcb_tp->copied_seq >=target)
+			mask |= POLLIN | POLLRDNORM;
+
+		if (!(mpcb_sk->sk_shutdown & SEND_SHUTDOWN)) {
+			if (sk_stream_wspace(mpcb_sk) >= 
+			    sk_stream_min_wspace(mpcb_sk)) {
+				mask |= POLLOUT | POLLWRNORM;
+			} else {  /* send SIGIO later */
+				set_bit(SOCK_ASYNC_NOSPACE,
+					&mpcb_sk->sk_socket->flags);
+				set_bit(SOCK_NOSPACE, &mpcb_sk->sock_flags);
+				
+				/* Race breaker. If space is freed after
+				 * wspace test but before the flags are set,
+				 * IO signal will be lost.
+				 */
+				if (sk_stream_wspace(mpcb_sk) >= 
+				    sk_stream_min_wspace(mpcb_sk))
+					mask |= POLLOUT | POLLWRNORM;
+			}
+		}
+		else printk(KERN_ERR "mpcb is in shutdown state\n");
+		
+		if (mpcb_tp->urg_data & TCP_URG_VALID)
+			mask |= POLLPRI;
+	}
+	return mask;
+}
+#else
 unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)
 {
 	unsigned int mask;
@@ -338,6 +454,7 @@ unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	poll_wait(file, sk->sk_sleep, wait);
+
 	if (sk->sk_state == TCP_LISTEN)
 		return inet_csk_listen_poll(sk);
 
@@ -398,13 +515,13 @@ unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)
 			mask |= POLLIN | POLLRDNORM;
 
 		if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
-			if (sk_stream_wspace(sk) >= sk_stream_min_wspace(sk)) {
+			if (sk_stream_wspace(sk) >= sk_stream_min_wspace(sk))
 				mask |= POLLOUT | POLLWRNORM;
-			} else {  /* send SIGIO later */
+			else {  /* send SIGIO later */
 				set_bit(SOCK_ASYNC_NOSPACE,
 					&sk->sk_socket->flags);
-				set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
-
+				set_bit(SOCK_NOSPACE, &sk->sock_flags);
+				
 				/* Race breaker. If space is freed after
 				 * wspace test but before the flags are set,
 				 * IO signal will be lost.
@@ -419,6 +536,7 @@ unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)
 	}
 	return mask;
 }
+#endif
 
 int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg)
 {
@@ -480,12 +598,22 @@ static inline int forced_push(struct tcp_sock *tp)
 static inline void skb_entail(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
 	struct tcp_skb_cb *tcb = TCP_SKB_CB(skb);
+	struct tcp_sock *mpcb_tp=(struct tcp_sock*)mpcb;
 
-	skb->csum    = 0;
-	tcb->seq     = tcb->end_seq = tp->write_seq;
-	tcb->flags   = TCPCB_FLAG_ACK;
-	tcb->sacked  = 0;
+	skb->csum     = 0;
+
+	/*in MPTCP mode, the subflow seqnum is given later*/
+	if (tp->mpc)
+		tcb->seq      = tcb->end_seq = tcb->sub_seq = 0;
+	else
+		tcb->seq      = tcb->end_seq = tcb->sub_seq = tp->write_seq;
+
+	tcb->data_seq = tcb->end_data_seq = mpcb_tp->write_seq;
+	tcb->data_len = 0;
+	tcb->flags    = TCPCB_FLAG_ACK;
+	tcb->sacked   = 0;
 	skb_header_release(skb);
 	tcp_add_write_queue_tail(sk, skb);
 	sk->sk_wmem_queued += skb->truesize;
@@ -501,17 +629,21 @@ static inline void tcp_mark_urg(struct tcp_sock *tp, int flags,
 		tp->snd_up = tp->write_seq;
 }
 
-static inline void tcp_push(struct sock *sk, int flags, int mss_now,
-			    int nonagle)
+void tcp_push(struct sock *sk, int flags, int mss_now,
+	      int nonagle)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	if (tcp_send_head(sk)) {
-		struct sk_buff *skb = tcp_write_queue_tail(sk);
-		if (!(flags & MSG_MORE) || forced_push(tp))
-			tcp_mark_push(tp, skb);
-		tcp_mark_urg(tp, flags, skb);
-		__tcp_push_pending_frames(sk, mss_now,
+	struct tcp_sock *tp=tcp_sk(sk);
+	struct sock *mpcb_sk=(tp->mpc)?(struct sock*)(tp->mpcb):sk;
+	struct tcp_sock *mpcb_tp = tcp_sk(mpcb_sk);
+	
+	if (mtcp_next_segment(mpcb_sk,NULL)) {
+		struct sk_buff *skb = tcp_write_queue_tail(mpcb_sk);
+		if (!skb) skb=skb_peek_tail(&tp->mpcb->reinject_queue);
+
+		if (!(flags & MSG_MORE) || forced_push(mpcb_tp))
+			tcp_mark_push(mpcb_tp, skb);
+		tcp_mark_urg(mpcb_tp, flags, skb);
+		__tcp_push_pending_frames(mpcb_sk, mss_now,
 					  (flags & MSG_MORE) ? TCP_NAGLE_CORK : nonagle);
 	}
 }
@@ -560,6 +692,11 @@ ssize_t tcp_splice_read(struct socket *sock, loff_t *ppos,
 	ssize_t spliced;
 	int ret;
 
+#ifdef CONFIG_MTCP
+	printk(KERN_ERR "%s not supported yet\n",__FUNCTION__);
+	BUG();
+#endif
+
 	/*
 	 * We can't seek on a socket input
 	 */
@@ -664,6 +801,12 @@ static ssize_t do_tcp_sendpages(struct sock *sk, struct page **pages, int poffse
 	ssize_t copied;
 	long timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);
 
+	if (tp->mpc) {
+		printk(KERN_ERR "%s: function not yet supported\n",
+		       __FUNCTION__);
+		BUG();
+	}
+	
 	/* Wait for a connection to finish. */
 	if ((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))
 		if ((err = sk_stream_wait_connect(sk, &timeo)) != 0)
@@ -747,11 +890,15 @@ new_segment:
 		continue;
 
 wait_for_sndbuf:
-		set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		set_bit(SOCK_NOSPACE, &sk->sock_flags);
 wait_for_memory:
 		if (copied)
 			tcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);
 
+		if (!sk->sk_sleep) {
+			printk(KERN_ERR "meta-sk:%d\n",is_meta_sk(sk));
+			BUG();
+		}
 		if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
 			goto do_error;
 
@@ -778,7 +925,7 @@ ssize_t tcp_sendpage(struct socket *sock, struct page *page, int offset,
 	struct sock *sk = sock->sk;
 
 	if (!(sk->sk_route_caps & NETIF_F_SG) ||
-	    !(sk->sk_route_caps & NETIF_F_ALL_CSUM))
+	    !(sk->sk_route_caps & NETIF_F_ALL_CSUM) || tcp_sk(sk)->mpc)
 		return sock_no_sendpage(sock, page, offset, size, flags);
 
 	lock_sock(sk);
@@ -812,10 +959,23 @@ static inline int select_size(struct sock *sk)
 	return tmp;
 }
 
-int tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
-		size_t size)
+
+int tcp_sendmsg(struct kiocb *iocb, struct socket *sock, 
+		       struct msghdr *msg,
+		       size_t size)
+{
+	return subtcp_sendmsg(iocb,sock->sk,msg,size);
+}
+
+/**
+ * In the original version of tcp_sendmsg, size is not used.
+ * If CONFIG_MTCP is set, size is interpreted as the offset inside the message
+ * to copy from. (that is, byte 0 to size-1 are simply ignored.
+ * With mptcp, @sk must be the master subsocket.
+ */
+int subtcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
+		   size_t size)
 {
-	struct sock *sk = sock->sk;
 	struct iovec *iov;
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *skb;
@@ -824,22 +984,39 @@ int tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
 	int err, copied;
 	long timeo;
 
+#ifndef CONFIG_MTCP
 	lock_sock(sk);
+#endif
 	TCP_CHECK_TIMER(sk);
 
 	flags = msg->msg_flags;
 	timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);
 
 	/* Wait for a connection to finish. */
-	if ((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))
-		if ((err = sk_stream_wait_connect(sk, &timeo)) != 0)
+	if ((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) {
+#ifdef CONFIG_MTCP
+		/*Should not happen anymore, since the check is done in
+		  mtcp_sendmsg*/
+		BUG();
+#endif
+		if ((err = sk_stream_wait_connect(
+			     is_meta_sk(sk)?tp->mpcb->master_sk:sk, 
+			     &timeo)) != 0) {
 			goto out_err;
-
+		}
+	}
+	
 	/* This should be in poll */
 	clear_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);
-
+	
+#ifdef CONFIG_MTCP
+	/*If we want to support TSO later, we'll need 
+	  to define xmit_size_goal to something much larger*/
+	mss_now = size_goal = sysctl_mptcp_mss;
+#else
 	mss_now = tcp_current_mss(sk, !(flags&MSG_OOB));
 	size_goal = tp->xmit_size_goal;
+#endif
 
 	/* Ok commence sending. */
 	iovlen = msg->msg_iovlen;
@@ -847,32 +1024,59 @@ int tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
 	copied = 0;
 
 	err = -EPIPE;
+#ifndef CONFIG_MTCP
 	if (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))
 		goto do_error;
+#endif
 
+	PDEBUG_SEND("%s:line %d, size %d,iovlen %d\n",__FUNCTION__,
+	       __LINE__,(int)size,(int)iovlen);
 	while (--iovlen >= 0) {
 		int seglen = iov->iov_len;
 		unsigned char __user *from = iov->iov_base;
 
 		iov++;
-
+		
+#ifdef CONFIG_MTCP
+		/*Skipping the offset (stored in the size argument)*/
+		if (tp->mpc) {
+			PDEBUG_SEND("seglen:%d\n",seglen);
+			if (seglen>=size) {				
+				seglen-=size;
+				from+=size;
+				size=0;
+			}
+			else {
+				size-=seglen;
+				continue;
+			}
+		}
+#endif
 		while (seglen > 0) {
 			int copy;
 
 			skb = tcp_write_queue_tail(sk);
 
-			if (!tcp_send_head(sk) ||
+			/*If this happen, the write queue has been emptied
+			  without setting the send_head to NULL.
+			  Normally the send_head is set to NULL with 
+			  tcp_advance_send_head, called by 
+			  tcp_event_new_data_sent on the meta_sk.
+			  If we transmit an skb without advancing the send
+			  head, the skb will be suppressed, while the send
+			  head will still point to it.*/
+			check_send_head(sk,4);
+			BUG_ON(!skb && tcp_send_head(sk));
+			
+			if (!tcp_send_head(sk) || 
 			    (copy = size_goal - skb->len) <= 0) {
-
-new_segment:
-				/* Allocate new segment. If the interface is SG,
-				 * allocate skb fitting to single page.
-				 */
+			new_segment:
+				
 				if (!sk_stream_memory_free(sk))
 					goto wait_for_sndbuf;
-
+				
 				skb = sk_stream_alloc_skb(sk, select_size(sk),
-						sk->sk_allocation);
+							  sk->sk_allocation);
 				if (!skb)
 					goto wait_for_memory;
 
@@ -892,16 +1096,20 @@ new_segment:
 
 			/* Where to copy to? */
 			if (skb_tailroom(skb) > 0) {
+				PDEBUG_SEND("to tail room\n");
 				/* We have some space in skb head. Superb! */
 				if (copy > skb_tailroom(skb))
 					copy = skb_tailroom(skb);
+				PDEBUG_SEND("%s:line %d\n",__FUNCTION__,__LINE__);
 				if ((err = skb_add_data(skb, from, copy)) != 0)
 					goto do_fault;
+				PDEBUG_SEND("%s:line %d\n",__FUNCTION__,__LINE__);
 			} else {
 				int merge = 0;
 				int i = skb_shinfo(skb)->nr_frags;
 				struct page *page = TCP_PAGE(sk);
 				int off = TCP_OFF(sk);
+				PDEBUG_SEND("coalesce\n");
 
 				if (skb_can_coalesce(skb, i, page, off) &&
 				    off != PAGE_SIZE) {
@@ -972,45 +1180,64 @@ new_segment:
 
 			if (!copied)
 				TCP_SKB_CB(skb)->flags &= ~TCPCB_FLAG_PSH;
-
 			tp->write_seq += copy;
 			TCP_SKB_CB(skb)->end_seq += copy;
 			skb_shinfo(skb)->gso_segs = 0;
-
+#ifdef CONFIG_MTCP
+			if (tp->mpc) {
+				TCP_SKB_CB(skb)->data_len += copy;
+				TCP_SKB_CB(skb)->end_data_seq += copy;
+			}
+#endif
+			PDEBUG_SEND("%s:line %d\n",__FUNCTION__,__LINE__);
 			from += copy;
 			copied += copy;
 			if ((seglen -= copy) == 0 && iovlen == 0)
 				goto out;
+			PDEBUG_SEND("%s:line %d\n",__FUNCTION__,__LINE__);
 
 			if (skb->len < size_goal || (flags & MSG_OOB))
 				continue;
 
+			PDEBUG_SEND("%s:line %d\n",__FUNCTION__,__LINE__);
+
 			if (forced_push(tp)) {
 				tcp_mark_push(tp, skb);
 				__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);
 			} else if (skb == tcp_send_head(sk))
 				tcp_push_one(sk, mss_now);
 			continue;
-
-wait_for_sndbuf:
+			
+		wait_for_sndbuf:
 			set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
-wait_for_memory:
+			tcpprobe_logmsg(sk, "wait_for_sndbuf");
+			
+		wait_for_memory:
 			if (copied)
-				tcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);
-
-			if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
+				tcp_push(sk, flags & ~MSG_MORE, mss_now, 
+					 TCP_NAGLE_PUSH);
+			if ((err = sk_stream_wait_memory(sk, &timeo)) 
+			    != 0)
 				goto do_error;
-
+			
+			BUG_ON(!sk_stream_memory_free(sk));
+			PDEBUG_SEND("%s:line %d\n",__FUNCTION__,__LINE__);
+#ifndef CONFIG_MTCP
 			mss_now = tcp_current_mss(sk, !(flags&MSG_OOB));
 			size_goal = tp->xmit_size_goal;
+#endif
 		}
 	}
 
 out:
 	if (copied)
 		tcp_push(sk, flags, mss_now, tp->nonagle);
+
 	TCP_CHECK_TIMER(sk);
+#ifndef CONFIG_MTCP
 	release_sock(sk);
+#endif
+	PDEBUG_SEND("%s:line %d, copied %d\n",__FUNCTION__,__LINE__,copied);
 	return copied;
 
 do_fault:
@@ -1022,14 +1249,17 @@ do_fault:
 		tcp_check_send_head(sk, skb);
 		sk_wmem_free_skb(sk, skb);
 	}
-
+	
 do_error:
+	tcpprobe_logmsg(sk, "error in subtcp_sendmsg");
 	if (copied)
 		goto out;
 out_err:
 	err = sk_stream_error(sk, flags, err);
 	TCP_CHECK_TIMER(sk);
+#ifndef CONFIG_MTCP
 	release_sock(sk);
+#endif
 	return err;
 }
 
@@ -1044,6 +1274,8 @@ static int tcp_recv_urg(struct sock *sk, long timeo,
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
+	mtcp_debug("Receiving urgent data\n");
+
 	/* No URG data to read. */
 	if (sock_flag(sk, SOCK_URGINLINE) || !tp->urg_data ||
 	    tp->urg_data == TCP_URG_READ)
@@ -1153,6 +1385,9 @@ static void tcp_prequeue_process(struct sock *sk)
 	struct sk_buff *skb;
 	struct tcp_sock *tp = tcp_sk(sk);
 
+	mtcp_debug("Entering %s for pi %d\n",__FUNCTION__,
+	           tp->path_index);
+
 	NET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPPREQUEUED);
 
 	/* RX process wants to run with disabled BHs, though it is not
@@ -1203,6 +1438,11 @@ int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 	u32 offset;
 	int copied = 0;
 
+	if (tp->mpc) {
+		printk(KERN_ERR "tcp_read_sock primitive not yet supported\n");
+		BUG();
+	}
+
 	if (sk->sk_state == TCP_LISTEN)
 		return -ENOTCONN;
 	while ((skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {
@@ -1258,6 +1498,10 @@ int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 	return copied;
 }
 
+#ifndef CONFIG_MTCP
+#define tcp_recvmsg_fallback tcp_recvmsg
+#endif
+
 /*
  *	This routine copies from a sock struct into the user buffer.
  *
@@ -1266,10 +1510,18 @@ int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
  *	Probably, code can be easily improved even more.
  */
 
-int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
-		size_t len, int nonblock, int flags, int *addr_len)
+int tcp_recvmsg_fallback(struct kiocb *iocb, struct sock *sk, 
+			 struct msghdr *msg,
+			 size_t len, int nonblock, int flags, int *addr_len)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+#ifdef CONFIG_MTCP
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+	struct sock *mpcb_sk=(struct sock*)mpcb;
+	struct tcp_sock *mpcb_tp=tcp_sk(mpcb_sk);
+#else
+#define mpcb tp
+#endif
 	int copied = 0;
 	u32 peek_seq;
 	u32 *seq;
@@ -1412,15 +1664,16 @@ int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 
 		tcp_cleanup_rbuf(sk, copied);
 
-		if (!sysctl_tcp_low_latency && tp->ucopy.task == user_recv) {
+		if (!sysctl_tcp_low_latency && mpcb->ucopy.task == user_recv) {
+
 			/* Install new reader */
 			if (!user_recv && !(flags & (MSG_TRUNC | MSG_PEEK))) {
 				user_recv = current;
-				tp->ucopy.task = user_recv;
-				tp->ucopy.iov = msg->msg_iov;
+				mpcb->ucopy.task = user_recv;
+				mpcb->ucopy.iov = msg->msg_iov;
 			}
-
-			tp->ucopy.len = len;
+			
+			mpcb->ucopy.len = len;
 
 			WARN_ON(tp->copied_seq != tp->rcv_nxt &&
 				!(flags & (MSG_PEEK | MSG_TRUNC)));
@@ -1473,7 +1726,7 @@ int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 
 			/* __ Restore normal policy in scheduler __ */
 
-			if ((chunk = len - tp->ucopy.len) != 0) {
+			if ((chunk = len - mpcb->ucopy.len) != 0) {
 				NET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMBACKLOG, chunk);
 				len -= chunk;
 				copied += chunk;
@@ -1484,7 +1737,7 @@ int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 do_prequeue:
 				tcp_prequeue_process(sk);
 
-				if ((chunk = len - tp->ucopy.len) != 0) {
+				if ((chunk = len - mpcb->ucopy.len) != 0) {
 					NET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);
 					len -= chunk;
 					copied += chunk;
@@ -1495,7 +1748,7 @@ do_prequeue:
 			if (net_ratelimit())
 				printk(KERN_DEBUG "TCP(%s:%d): Application bug, race in MSG_PEEK.\n",
 				       current->comm, task_pid_nr(current));
-			peek_seq = tp->copied_seq;
+			peek_seq = mpcb_tp->copied_seq;
 		}
 		continue;
 
@@ -1549,7 +1802,8 @@ do_prequeue:
 #endif
 			{
 				err = skb_copy_datagram_iovec(skb, offset,
-						msg->msg_iov, used);
+							      msg->msg_iov, 
+							      used);
 				if (err) {
 					/* Exception. Bailout! */
 					if (!copied)
@@ -1595,19 +1849,20 @@ skip_copy:
 		if (!skb_queue_empty(&tp->ucopy.prequeue)) {
 			int chunk;
 
-			tp->ucopy.len = copied > 0 ? len : 0;
+			mpcb->ucopy.len = copied > 0 ? len : 0;
 
 			tcp_prequeue_process(sk);
 
-			if (copied > 0 && (chunk = len - tp->ucopy.len) != 0) {
+			if (copied > 0 && (chunk = len - mpcb->ucopy.len) 
+			    != 0) {
 				NET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);
 				len -= chunk;
 				copied += chunk;
 			}
 		}
 
-		tp->ucopy.task = NULL;
-		tp->ucopy.len = 0;
+		mpcb->ucopy.task = NULL;
+		mpcb->ucopy.len = 0;
 	}
 
 #ifdef CONFIG_NET_DMA
@@ -1660,16 +1915,390 @@ recv_urg:
 	goto out;
 }
 
+#ifdef CONFIG_MTCP
+
+/*
+ *	This routine copies from a sock struct into the user buffer.
+ *
+ *	Technical note: in 2.3 we work on _locked_ socket, so that
+ *	tricks with *seq access order and skb->users are not required.
+ *	Probably, code can be easily improved even more.
+ *
+ *      Completely modified for MTCP: subflow demultiplexing is done here.
+ *      Note that we have removed NET_DMA support at the moment.
+ */
+
+int tcp_recvmsg(struct kiocb *iocb, struct sock *master_sk, struct msghdr *msg,
+		size_t len, int nonblock, int flags, int *addr_len)
+{
+	struct tcp_sock *master_tp = tcp_sk(master_sk);
+	struct sock *sk;
+	struct tcp_sock *tp;
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(master_tp);
+	struct sock *mpcb_sk=(struct sock*) mpcb;
+	struct tcp_sock *mpcb_tp=tcp_sk(mpcb_sk);
+	int copied = 0;
+	u32 peek_data_seq;
+	u32 *data_seq;
+	int err;
+	int target;		/* Read at least this many bytes */
+	long timeo;
+	struct task_struct *user_recv = NULL;
+	
+	if (!master_tp->mpc)
+		return tcp_recvmsg_fallback(iocb,master_sk,msg,len,nonblock,
+					    flags,addr_len);
+#ifdef CONFIG_MTCP_PM
+	/*Received a new list of addresses recently ?
+	  announce corresponding path indices to the
+	  mpcb, and start new subflows*/
+	mtcp_check_new_subflow(mpcb); 
+#endif
+	
+	/*We listen on every subflow.
+	 * Here we are awoken each time
+	 * any subflow wants to give work to tcp_recvmsg. To be more clear,
+	 * we behave here somewhat like doing a select, but as seen by bottom 
+	 * halves we are expecting data from every subflow at once.
+	 */
+		
+	/*Locking metasocket*/
+	mutex_lock(&mpcb->mutex);
+
+	/*For every subflow, init copied to 0*/
+	mtcp_for_each_tp(mpcb,tp) 
+		tp->copied=0;
+
+	/*Locking all subsockets*/
+	mtcp_for_each_sk(mpcb,sk,tp) lock_sock(sk);
+
+	err = -ENOTCONN;
+	if (master_sk->sk_state == TCP_LISTEN)
+		goto out; 
+
+	/*Receive timeout, set by application. This is the same for 
+	  all subflows, and the real value is stored in the master socket.*/
+	timeo = sock_rcvtimeo(master_sk, nonblock);
+
+	/* Urgent data needs to be handled specially. */
+	if (flags & MSG_OOB)
+		goto recv_urg; 
+
+	/*Setting global and local seq pointer*/
+	if (flags & MSG_PEEK) {	
+		/*We put this because it is not sure at all that MSG_PEEK
+		  works correctly.*/
+		printk(KERN_ERR "Warning: MSG_PEEK is set...\n");
+		peek_data_seq = mpcb_tp->copied_seq;
+		data_seq = &peek_data_seq; /*global pointer*/
+		mtcp_for_each_tp(mpcb,tp) {
+			tp->peek_seq=tp->copied_seq;
+			tp->seq=&tp->peek_seq; /*local pointer*/
+		}
+	}
+	else {
+		data_seq = &mpcb_tp->copied_seq; /*global pointer*/
+		mtcp_for_each_tp(mpcb,tp)
+			tp->seq=&tp->copied_seq; /*local pointer*/
+	}
+	
+	/*low water test: minimal number of bytes that must be consumed before
+	  tcp_recvmsg completes*/
+	target = sock_rcvlowat(master_sk, flags & MSG_WAITALL, len);
+	if (target!=1) {
+		printk(KERN_ERR "SO_RCVLOWAT != 1 not yet supported\n");
+		BUG();
+	}
+
+	do {
+		int empty_prequeues=0;
+
+		/*Start by checking if skbs are waiting on the mpcb 
+		  receive queue*/
+		err=mtcp_check_rcv_queue(mpcb,msg, &len, data_seq, &copied, flags);
+		if (err<0) {
+			printk(KERN_ERR "error in mtcp_check_rcv_queue\n");
+			/* Exception. Bailout! */
+			if (!copied)
+				copied = -EFAULT;
+			break;
+		}
+		
+		/* Are we at urgent data ? 
+		   Stop if we have read anything 
+		   or have SIGURG pending. Note that we only accept Urgent 
+		   data on the master subflow at the moment*/
+		if (master_tp->urg_data && 
+		    master_tp->urg_seq == master_tp->copied_seq) {
+			/*urg data not managed currently*/
+			BUG();
+			if (copied)
+				break;
+			if (signal_pending(current)) {
+				BUG_ON(copied);
+				copied = timeo ? 
+					sock_intr_errno(timeo) : 
+					-EAGAIN;
+				break;
+			}
+		}
+		
+		/* Well, if we have backlog, try to process it now yet. */
+		if (copied >= target && 
+		    !mtcp_test_any_sk(mpcb,sk,sk->sk_backlog.tail))
+			break;
+
+		/*Here we test a set of conditions to return immediately to
+		  the user*/
+		if (copied) {
+			/*Error on any subsocket, shutdown on all subsocks,
+			  timeout or pending signal*/
+			if (mpcb_sk->sk_err ||
+			    mpcb_sk->sk_state==TCP_CLOSE ||
+			    (mpcb_sk->sk_shutdown & RCV_SHUTDOWN) ||
+			    !timeo ||
+			    signal_pending(current))
+				break;
+		} else {
+			if (sock_flag(mpcb_sk,SOCK_DONE))
+				break;
+			
+			if (mpcb_sk->sk_err) {
+				copied = sock_error(mpcb_sk);
+				break;
+			}
+			
+			if (mpcb_sk->sk_shutdown & RCV_SHUTDOWN)
+				break;
+
+			if (mpcb_sk->sk_state == TCP_CLOSE) {
+				if (!sock_flag(mpcb_sk,SOCK_DONE)) {
+					/* This occurs when user tries to read
+                                         * from never connected socket.
+					 */
+					copied = -ENOTCONN;
+					break;
+				}
+				break;
+			}
+
+			if (!timeo) {
+				copied = -EAGAIN;
+				break;
+			}
+
+			if (signal_pending(current)) {
+				copied = sock_intr_errno(timeo);
+				break;
+			}
+		}
+
+		mtcp_for_each_sk(mpcb,sk,tp)
+			tcp_cleanup_rbuf(sk, tp->copied);
+
+		if (!sysctl_tcp_low_latency && mpcb->ucopy.task == 
+		    user_recv) {
+			/* Install new reader */
+			if (!user_recv && !(flags & (MSG_TRUNC | MSG_PEEK))) {
+				user_recv = current;
+				mpcb->ucopy.task = user_recv;
+				mpcb->ucopy.iov = msg->msg_iov;
+			}
+
+			mpcb->ucopy.len = len;
+			
+			mtcp_for_each_tp(mpcb,tp) {
+				WARN_ON(tp->copied_seq != tp->rcv_nxt &&
+					!(flags & (MSG_PEEK | MSG_TRUNC)));
+			}
+			
+			/* Ugly... If prequeue is not empty, we have to
+			 * process it before releasing socket, otherwise
+			 * order will be broken at second iteration.
+			 * More elegant solution is required!!!
+			 *
+			 * Look: we have the following (pseudo)queues:
+			 *
+			 * 1. packets in flight
+			 * 2. backlog
+			 * 3. prequeue
+			 * 4. receive_queue
+			 *
+			 * Each queue can be processed only if the next ones
+			 * are empty. At this point we have empty receive_queue.
+			 * But prequeue _can_ be not empty after 2nd iteration,
+			 * when we jumped to start of loop because backlog
+			 * processing added something to receive_queue.
+			 * We cannot release_sock(), because backlog contains
+			 * packets arrived _after_ prequeued ones.
+			 *
+			 * Shortly, algorithm is clear --- to process all
+			 * the queues in order. We could make it more directly,
+			 * requeueing packets from backlog to prequeue, if
+			 * is not empty. It is more elegant, but eats cycles,
+			 * unfortunately.
+			 */
+			if (mtcp_test_any_tp(mpcb,tp,
+					     !skb_queue_empty(
+						     &tp->ucopy.prequeue))) {
+				empty_prequeues=1;
+				goto do_prequeue;
+			}
+			/* __ Set realtime policy in scheduler __ */
+		}
+
+		if (copied >= target) {
+			/* Do not sleep, just process backlog. */
+			mtcp_for_each_sk(mpcb,sk,tp) {
+				release_sock(sk);
+				lock_sock(sk);
+			}
+		} else {
+			/*Wait for data arriving on any subsocket*/
+			mtcp_wait_data(mpcb,master_sk, &timeo,flags);
+		}
+
+		if (user_recv) {
+			int chunk;
+			mtcp_debug("At line %d\n",__LINE__);
+			
+			/* __ Restore normal policy in scheduler __ */
+
+			if ((chunk = len - mpcb->ucopy.len) != 0) {		
+				NET_ADD_STATS_USER(sock_net(master_sk), LINUX_MIB_TCPDIRECTCOPYFROMBACKLOG, chunk);
+				/*TODEL*/
+				mtcp_debug("backlog copy: %d\n",chunk);
+				len -= chunk;
+				copied += chunk;
+			}
+			
+		do_prequeue:			
+			mtcp_for_each_tp(mpcb,tp) {
+				mtcp_debug("Checking prequeue for pi %d,"
+				           "prequeue len:%d\n",
+				           tp->path_index,
+				           skb_queue_len(&tp->ucopy.prequeue));
+				if ((empty_prequeues ||
+				     (tp->rcv_nxt == tp->copied_seq)) &&
+				    !skb_queue_empty(
+					    &tp->ucopy.prequeue)) {
+					
+					sk=(struct sock*) tp;
+					tcp_prequeue_process(sk);
+					
+					if ((chunk = len - mpcb->ucopy.len)
+					    != 0) {
+						mtcp_debug("prequeue "
+						           "copy :%d, len %d,"
+						           "ucopy.len %d\n",
+						           chunk,(int)len,
+						           mpcb->ucopy.len);/*TODEL*/
+						NET_ADD_STATS_USER(
+							sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);
+						BUG_ON(chunk<0);
+						len -= chunk;
+						copied += chunk;
+					}
+				}
+			}
+			empty_prequeues=0;
+		}
+		mtcp_for_each_tp(mpcb,tp) {
+			if ((flags & MSG_PEEK) && 
+			    tp->peek_seq != tp->copied_seq) {
+				if (net_ratelimit())
+					printk(KERN_ERR "TCP(%s:%d): "
+					       "Application bug, race in "
+					       "MSG_PEEK.\n",
+					       current->comm, 
+					       task_pid_nr(current));
+				tp->peek_seq = tp->copied_seq;
+			}
+		}
+	} while (len > 0);
+	
+	if (user_recv) {
+		mtcp_for_each_sk(mpcb,sk,tp)
+			if (!skb_queue_empty(&tp->ucopy.prequeue)) {
+				int chunk;
+				
+				mpcb->ucopy.len = copied > 0 ? len : 0;
+				
+				tcp_prequeue_process(sk);
+				
+				if (copied > 0 && (chunk = len - 
+						   mpcb->ucopy.len) != 0) {
+					NET_ADD_STATS_USER(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);
+					mtcp_debug("prequeue2 copy :%d\n",
+					           chunk); /*TODEL*/
+					len -= chunk;
+					copied += chunk;
+				}
+			}
+		
+		mpcb->ucopy.task = NULL;
+		mpcb->ucopy.len = 0;
+	}
+	
+	/* According to UNIX98, msg_name/msg_namelen are ignored
+	 * on connected socket. I was just happy when found this 8) --ANK
+	 */
+	
+	/* Clean up data we have read: This will do ACK frames. */
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		tcp_cleanup_rbuf(sk, tp->copied);
+	}
+	
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		release_sock(sk);
+	}
+	mutex_unlock(&mpcb->mutex);
+
+	return copied;
+
+out:
+	mtcp_for_each_sk(mpcb,sk,tp) release_sock(sk);
+	mutex_unlock(&mpcb->mutex);
+	mtcp_debug("At line %d\n",__LINE__);
+	return err;
+
+recv_urg:
+	/*At the moment we only allow receiving urgent data on the master
+	  subsocket. Makes sense ?*/
+	err = tcp_recv_urg(master_sk, timeo, msg, len, flags, addr_len);
+	mtcp_debug("At line %d\n",__LINE__);
+	goto out;
+}
+
+#endif /*CONFIG_MTCP*/
+
 void tcp_set_state(struct sock *sk, int state)
 {
 	int oldstate = sk->sk_state;
+	struct tcp_sock *tp=tcp_sk(sk);
 
 	switch (state) {
 	case TCP_ESTABLISHED:
-		if (oldstate != TCP_ESTABLISHED)
+		if (oldstate != TCP_ESTABLISHED) {
 			TCP_INC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);
+#ifdef CONFIG_MTCP
+			/*mpcb is NULL if the socket is in the accept
+			  queue of the mpcb.*/
+			BUG_ON(!tp->mpcb && !tp->pending);
+			if (tcp_sk(sk)->mpcb) {
+				struct sock *mpcb_sk=
+					(struct sock*)(tcp_sk(sk)->mpcb);
+				if (tcp_sk(sk)->mpc && 
+				    is_master_sk(tcp_sk(sk))) 
+					mtcp_ask_update(sk);
+				tcp_sk(sk)->mpcb->cnt_established++;
+				mtcp_update_sndbuf(tcp_sk(sk)->mpcb);
+				mpcb_sk->sk_state=TCP_ESTABLISHED;
+			}
+#endif
+		}
 		break;
-
+		
 	case TCP_CLOSE:
 		if (oldstate == TCP_CLOSE_WAIT || oldstate == TCP_ESTABLISHED)
 			TCP_INC_STATS(sock_net(sk), TCP_MIB_ESTABRESETS);
@@ -1718,7 +2347,7 @@ static const unsigned char new_state[16] = {
   /* TCP_CLOSING	*/ TCP_CLOSING,
 };
 
-static int tcp_close_state(struct sock *sk)
+int tcp_close_state(struct sock *sk)
 {
 	int next = (int)new_state[sk->sk_state];
 	int ns = next & TCP_STATE_MASK;
@@ -1741,7 +2370,27 @@ void tcp_shutdown(struct sock *sk, int how)
 	 */
 	if (!(how & SEND_SHUTDOWN))
 		return;
-
+#ifdef CONFIG_MTCP
+	/*if this is the master subsocket, we must first close the
+	  slave subsockets*/
+	if (tcp_sk(sk)->mpc && is_master_sk(tcp_sk(sk))) {
+		struct multipath_pcb *mpcb=mpcb_from_tcpsock(tcp_sk(sk));
+		struct sock *mpcb_sk=(struct sock*)mpcb;
+		
+		mtcp_debug("%s: Shutdown of master_sk\n",__FUNCTION__);
+		
+		lock_sock(mpcb_sk);		
+
+		if ((1 << mpcb_sk->sk_state) &
+		    (TCPF_ESTABLISHED | TCPF_SYN_SENT |
+		     TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {
+			if (tcp_close_state(mpcb_sk))
+				mtcp_send_fin(mpcb_sk);
+		}
+		release_sock(mpcb_sk);
+		return;
+	}
+#endif
 	/* If we've already sent a FIN, or it's a closed state, skip this. */
 	if ((1 << sk->sk_state) &
 	    (TCPF_ESTABLISHED | TCPF_SYN_SENT |
@@ -1752,13 +2401,23 @@ void tcp_shutdown(struct sock *sk, int how)
 	}
 }
 
+
+/**
+ * MPTCP modif: tcp_close can now be called from tcp_write_xmit()
+ * In that case, timeout must be set to -1.
+ * This prevents the locks from being taken, and enforces atomic operation
+ * everywhere, because in tcp_write_xmit, the sk is already locked, and
+ * and we may be in soft interrupt context.
+ */
 void tcp_close(struct sock *sk, long timeout)
 {
 	struct sk_buff *skb;
 	int data_was_unread = 0;
 	int state;
+	int locked=(timeout==-1)?1:0;
+		
+	if (!locked) lock_sock(sk);
 
-	lock_sock(sk);
 	sk->sk_shutdown = SHUTDOWN_MASK;
 
 	if (sk->sk_state == TCP_LISTEN) {
@@ -1794,7 +2453,7 @@ void tcp_close(struct sock *sk, long timeout)
 		/* Unread data was tossed, zap the connection. */
 		NET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);
 		tcp_set_state(sk, TCP_CLOSE);
-		tcp_send_active_reset(sk, GFP_KERNEL);
+		tcp_send_active_reset(sk, (locked)?GFP_ATOMIC:GFP_KERNEL);
 	} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {
 		/* Check zero linger _after_ checking for unread data. */
 		sk->sk_prot->disconnect(sk, 0);
@@ -1828,7 +2487,7 @@ void tcp_close(struct sock *sk, long timeout)
 		tcp_send_fin(sk);
 	}
 
-	sk_stream_wait_close(sk, timeout);
+	if (!locked) sk_stream_wait_close(sk, timeout);
 
 adjudge_to_death:
 	state = sk->sk_state;
@@ -1837,14 +2496,16 @@ adjudge_to_death:
 	atomic_inc(sk->sk_prot->orphan_count);
 
 	/* It is the last release_sock in its life. It will remove backlog. */
-	release_sock(sk);
-
-
+	if (!locked)
+		release_sock(sk);
+	
 	/* Now socket is owned by kernel and we acquire BH lock
 	   to finish close. No need to check for user refs.
 	 */
-	local_bh_disable();
-	bh_lock_sock(sk);
+	if (!locked) {
+		local_bh_disable();
+		bh_lock_sock(sk);
+	}
 	WARN_ON(sock_owned_by_user(sk));
 
 	/* Have we already been destroyed by a softirq or backlog? */
@@ -1903,8 +2564,10 @@ adjudge_to_death:
 	/* Otherwise, socket is reprieved until protocol close. */
 
 out:
-	bh_unlock_sock(sk);
-	local_bh_enable();
+	if (!locked){
+		bh_unlock_sock(sk);
+		local_bh_enable();
+	}
 	sock_put(sk);
 }
 
@@ -1937,7 +2600,10 @@ int tcp_disconnect(struct sock *sk, int flags)
 		/* The last check adjusts for discrepancy of Linux wrt. RFC
 		 * states
 		 */
-		tcp_send_active_reset(sk, gfp_any());
+		/*MTCP: No need to reset half established slave subflows,
+		  since a reset on any flow resets everything*/
+		if (tp->mpc) BUG_ON(!tp->mpcb && !tp->pending);
+		if (!tp->mpc || tp->mpcb) tcp_send_active_reset(sk, gfp_any());
 		sk->sk_err = ECONNRESET;
 	} else if (old_state == TCP_SYN_SENT)
 		sk->sk_err = ECONNRESET;
@@ -2652,7 +3318,7 @@ void tcp_done(struct sock *sk)
 {
 	if(sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)
 		TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_ATTEMPTFAILS);
-
+    
 	tcp_set_state(sk, TCP_CLOSE);
 	tcp_clear_xmit_timers(sk);
 
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index d77c0d2..eedf5b4 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -70,6 +70,9 @@
 #include <linux/ipsec.h>
 #include <asm/unaligned.h>
 #include <net/netdma.h>
+#include <net/mtcp.h>
+#include <linux/tcp_probe.h>
+#include <linux/completion.h>
 
 int sysctl_tcp_timestamps __read_mostly = 1;
 int sysctl_tcp_window_scaling __read_mostly = 1;
@@ -79,12 +82,22 @@ int sysctl_tcp_reordering __read_mostly = TCP_FASTRETRANS_THRESH;
 int sysctl_tcp_ecn __read_mostly;
 int sysctl_tcp_dsack __read_mostly = 1;
 int sysctl_tcp_app_win __read_mostly = 31;
-int sysctl_tcp_adv_win_scale __read_mostly = 2;
+int sysctl_tcp_adv_win_scale __read_mostly = 1;
 
 int sysctl_tcp_stdurg __read_mostly;
 int sysctl_tcp_rfc1337 __read_mostly;
 int sysctl_tcp_max_orphans __read_mostly = NR_FILE;
+#ifdef CONFIG_MTCP
+/*At the moment we disable frto, because it creates problems
+  with failure recovery: It waits for the next ack before to decide
+  whether it enters the Loss state. But in case of failure, 
+  the next ack never arrives of course. When we have several paths this is
+  a problem because we do want to retransmit on another working subflow
+  in that case.*/
+int sysctl_tcp_frto __read_mostly = 0;
+#else
 int sysctl_tcp_frto __read_mostly = 2;
+#endif
 int sysctl_tcp_frto_response __read_mostly;
 int sysctl_tcp_nometrics_save __read_mostly;
 
@@ -308,7 +321,7 @@ static void tcp_grow_window(struct sock *sk, struct sk_buff *skb)
 	    (int)tp->rcv_ssthresh < tcp_space(sk) &&
 	    !tcp_memory_pressure) {
 		int incr;
-
+		
 		/* Check #2. Increase window, if skb with such overhead
 		 * will fit to rcvbuf in future.
 		 */
@@ -316,11 +329,14 @@ static void tcp_grow_window(struct sock *sk, struct sk_buff *skb)
 			incr = 2 * tp->advmss;
 		else
 			incr = __tcp_grow_window(sk, skb);
-
+		
 		if (incr) {
 			tp->rcv_ssthresh = min(tp->rcv_ssthresh + incr,
 					       tp->window_clamp);
 			inet_csk(sk)->icsk_ack.quick |= 1;
+#ifdef CONFIG_MTCP
+			mtcp_update_window_clamp(tp->mpcb);
+#endif
 		}
 	}
 }
@@ -376,6 +392,14 @@ static void tcp_init_buffer_space(struct sock *sk)
 
 	tp->rcv_ssthresh = min(tp->rcv_ssthresh, tp->window_clamp);
 	tp->snd_cwnd_stamp = tcp_time_stamp;
+
+#ifdef CONFIG_MTCP
+	/*mpcb is NULL if the subsock is in the mpcb accept queue.
+	  In that case, the mtcp_update_window_clamp() is called 
+	  later, from mtcp_add_sock()*/
+	BUG_ON(!tp->mpcb && !tp->pending);
+	if (tp->mpcb) mtcp_update_window_clamp(tp->mpcb);
+#endif
 }
 
 /* 5. Recalculate window clamp after socket hit its memory bounds. */
@@ -395,6 +419,10 @@ static void tcp_clamp_window(struct sock *sk)
 	}
 	if (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf)
 		tp->rcv_ssthresh = min(tp->window_clamp, 2U * tp->advmss);
+
+#ifdef CONFIG_MTCP
+	mtcp_update_window_clamp(tp->mpcb);
+#endif	
 }
 
 /* Initialize RCV_MSS value.
@@ -448,9 +476,9 @@ static void tcp_rcv_rtt_update(struct tcp_sock *tp, u32 sample, int win_dep)
 		 */
 		if (!win_dep) {
 			m -= (new_sample >> 3);
-			new_sample += m;
-		} else if (m < new_sample)
-			new_sample = m << 3;
+			new_sample += m; /* new=old*7/8+new*1/8 */
+		} else if (m < new_sample) /* if new < 8*old */
+			new_sample = m << 3; 
 	} else {
 		/* No previous measure. */
 		new_sample = m << 3;
@@ -462,6 +490,13 @@ static void tcp_rcv_rtt_update(struct tcp_sock *tp, u32 sample, int win_dep)
 
 static inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)
 {
+	/*If MPTCP is used, timestamps are mandatory. Hence, 
+	  we can use always tcp_rcv_rtt_measure_ts. Moreover,
+	  this simply CANNOT be used with MPTCP, since 
+	  this function uses tp->rcv_wnd, which is NOT updated 
+	  at all. Using that function with MPTCP, would lead
+	  to underestimate the RTT, and hence undersize the receive window.*/
+	if (tp->mpc) return;
 	if (tp->rcv_rtt_est.time == 0)
 		goto new_measure;
 	if (before(tp->rcv_nxt, tp->rcv_rtt_est.seq))
@@ -492,13 +527,34 @@ void tcp_rcv_space_adjust(struct sock *sk)
 	struct tcp_sock *tp = tcp_sk(sk);
 	int time;
 	int space;
-
+#ifdef CONFIG_MTCP
+	struct multipath_pcb *mpcb=tp->mpcb;
+	if (tp->mpc && tp->pending)
+		mpcb=mtcp_hash_find(tp->mtcp_loc_token);		
+#endif
 	if (tp->rcvq_space.time == 0)
 		goto new_measure;
 
 	time = tcp_time_stamp - tp->rcvq_space.time;
-	if (time < (tp->rcv_rtt_est.rtt >> 3) || tp->rcv_rtt_est.rtt == 0)
-		return;
+
+	if (tp->mpc && tp->mpcb) { 
+#ifdef CONFIG_MTCP
+		struct multipath_pcb *mpcb=tp->mpcb;
+		struct tcp_sock *tp_it;
+		u32 rtt_max=0;
+
+		/*In MPTCP, we take the max delay across all flows,
+		  in order to take into account meta-reordering buffers.*/
+		mtcp_for_each_tp(mpcb,tp_it) {
+			if (rtt_max<(tp_it->rcv_rtt_est.rtt >> 3))
+				rtt_max=(tp_it->rcv_rtt_est.rtt >> 3);
+		}
+		if (time < rtt_max || !rtt_max)
+			goto out;
+#endif
+	}
+	else if (time < (tp->rcv_rtt_est.rtt >> 3) || tp->rcv_rtt_est.rtt == 0)
+		goto out;
 
 	space = 2 * (tp->copied_seq - tp->rcvq_space.seq);
 
@@ -531,6 +587,10 @@ void tcp_rcv_space_adjust(struct sock *sk)
 
 				/* Make the window clamp follow along.  */
 				tp->window_clamp = new_clamp;
+#ifdef CONFIG_MTCP
+				mtcp_update_window_clamp(mpcb);
+#endif
+					
 			}
 		}
 	}
@@ -538,6 +598,11 @@ void tcp_rcv_space_adjust(struct sock *sk)
 new_measure:
 	tp->rcvq_space.seq = tp->copied_seq;
 	tp->rcvq_space.time = tcp_time_stamp;
+out:
+#ifdef CONFIG_MTCP
+	if (tp->mpc && tp->pending && mpcb)
+		mpcb_put(mpcb);
+#endif
 }
 
 /* There is something which you must keep in mind when you analyze the
@@ -592,8 +657,11 @@ static void tcp_event_data_recv(struct sock *sk, struct sk_buff *skb)
 
 	TCP_ECN_check_ce(tp, skb);
 
-	if (skb->len >= 128)
+	/*MPTCP: if the subsock is not yet attached to the mpcb,
+	  we cannot grow the window.*/
+	if (skb->len >= 128 && (!tp->mpc || tp->mpcb))
 		tcp_grow_window(sk, skb);
+	if (tp->mpc) BUG_ON(!tp->mpcb && !tp->pending);
 }
 
 static u32 tcp_rto_min(struct sock *sk)
@@ -620,6 +688,11 @@ static void tcp_rtt_estimator(struct sock *sk, const __u32 mrtt)
 	struct tcp_sock *tp = tcp_sk(sk);
 	long m = mrtt; /* RTT */
 
+	if (m > HZ) {
+		printk(KERN_ERR "pi %d:measured rtt is %ld ms\n",
+		       tp->path_index,m*1000/HZ);
+	}
+
 	/*	The following amusing code comes from Jacobson's
 	 *	article in SIGCOMM '88.  Note that rtt and mdev
 	 *	are scaled versions of rtt and mean deviation.
@@ -2553,6 +2626,9 @@ static void tcp_mtup_probe_success(struct sock *sk, struct sk_buff *skb)
 	tp->snd_cwnd_cnt = 0;
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 	tp->rcv_ssthresh = tcp_current_ssthresh(sk);
+#ifdef CONFIG_MTCP
+	mtcp_update_window_clamp(tp->mpcb);
+#endif
 
 	icsk->icsk_mtup.search_low = icsk->icsk_mtup.probe_size;
 	icsk->icsk_mtup.probe_size = 0;
@@ -2752,6 +2828,11 @@ static void tcp_ack_saw_tstamp(struct sock *sk, int flag)
 	 */
 	struct tcp_sock *tp = tcp_sk(sk);
 	const __u32 seq_rtt = tcp_time_stamp - tp->rx_opt.rcv_tsecr;
+	if (seq_rtt > HZ) {
+		printk(KERN_ERR "1 - pi %d:measured rtt is %d ms\n",
+		       tp->path_index,seq_rtt*1000/HZ);
+	}
+
 	tcp_rtt_estimator(sk, seq_rtt);
 	tcp_set_rto(sk);
 	inet_csk(sk)->icsk_backoff = 0;
@@ -2771,6 +2852,10 @@ static void tcp_ack_no_tstamp(struct sock *sk, u32 seq_rtt, int flag)
 
 	if (flag & FLAG_RETRANS_DATA_ACKED)
 		return;
+	if (seq_rtt > HZ) {
+		printk(KERN_ERR "2 - pi %d:measured rtt is %d ms\n",
+		       tcp_sk(sk)->path_index,seq_rtt*1000/HZ);
+	}
 
 	tcp_rtt_estimator(sk, seq_rtt);
 	tcp_set_rto(sk);
@@ -2803,12 +2888,11 @@ static void tcp_rearm_rto(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	if (!tp->packets_out) {
+	if (!tp->packets_out)
 		inet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);
-	} else {
+	else
 		inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
 					  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);
-	}
 }
 
 /* If we get here, the whole TSO packet has not been acked. */
@@ -2820,6 +2904,7 @@ static u32 tcp_tso_acked(struct sock *sk, struct sk_buff *skb)
 	BUG_ON(!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una));
 
 	packets_acked = tcp_skb_pcount(skb);
+	
 	if (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))
 		return 0;
 	packets_acked -= tcp_skb_pcount(skb);
@@ -2851,13 +2936,30 @@ static int tcp_clean_rtx_queue(struct sock *sk, int prior_fackets,
 	s32 seq_rtt = -1;
 	s32 ca_seq_rtt = -1;
 	ktime_t last_ackt = net_invalid_timestamp();
+#ifdef MTCP_DEBUG_PKTS_OUT
+	int orig_packets, orig_qsize,orig_outsize;
+#endif
+
+
+	BUG_ON(is_meta_sk(sk));
+	/*Cannot have more packets in flight than packets in the
+	  rexmit queue*/
+	BUG_ON(tp->packets_out>skb_queue_len(&sk->sk_write_queue));
 
+	if (tcp_write_queue_empty(sk) && 
+	    icsk->icsk_pending==ICSK_TIME_RETRANS)
+		BUG();
+
+	check_pkts_out(sk);
+		
 	while ((skb = tcp_write_queue_head(sk)) && skb != tcp_send_head(sk)) {
 		struct tcp_skb_cb *scb = TCP_SKB_CB(skb);
 		u32 end_seq;
 		u32 acked_pcount;
 		u8 sacked = scb->sacked;
 
+		check_pkts_out(sk);
+
 		/* Determine how many packets and what bytes were acked, tso and else */
 		if (after(scb->end_seq, tp->snd_una)) {
 			if (tcp_skb_pcount(skb) == 1 ||
@@ -2873,7 +2975,9 @@ static int tcp_clean_rtx_queue(struct sock *sk, int prior_fackets,
 		} else {
 			acked_pcount = tcp_skb_pcount(skb);
 			end_seq = scb->end_seq;
+			BUG_ON(!acked_pcount);
 		}
+		check_pkts_out(sk);
 
 		/* MTU probing checks */
 		if (fully_acked && icsk->icsk_mtup.probe_size &&
@@ -2904,6 +3008,13 @@ static int tcp_clean_rtx_queue(struct sock *sk, int prior_fackets,
 		if (sacked & TCPCB_LOST)
 			tp->lost_out -= acked_pcount;
 
+#ifdef MTCP_DEBUG_PKTS_OUT
+		orig_outsize=check_pkts_out(sk);
+		
+		orig_packets=tp->packets_out;
+		orig_qsize=skb_queue_len(&sk->sk_write_queue);
+#endif
+
 		tp->packets_out -= acked_pcount;
 		pkts_acked += acked_pcount;
 
@@ -2924,7 +3035,62 @@ static int tcp_clean_rtx_queue(struct sock *sk, int prior_fackets,
 		if (!fully_acked)
 			break;
 
+		/*Before we remove the skb, we update the meta-ack count*/
+#ifdef CONFIG_MTCP
+		{
+			if (!tp->mpc || !skb->len) goto no_mptcp_update;
+			
+			/*Since we are about to remove this segment from the
+			  retransmit queue, we know for sure that is has been
+			  acked, note that we check the end_data_seq, not the
+			  data_seq, since data_seq is 0 for the first data 
+			  segment (currently)*/
+			BUG_ON(!scb->end_data_seq);
+			skb->count_dsn=0;
+			if (!tp->bw_est.time) {
+				/*bootstrap bw estimation*/
+				tp->bw_est.space=(tp->snd_cwnd*tp->mss_cache)<<
+					tp->bw_est.shift;
+				tp->bw_est.seq=tp->snd_una+tp->bw_est.space;
+				tp->bw_est.time=tcp_time_stamp;
+			}
+			else if (after(tp->snd_una,tp->bw_est.seq)) {
+				/*update the bw estimate for this
+				  subflow*/
+				if (tcp_time_stamp-tp->bw_est.time==0)
+					tp->bw_est.shift++;
+				else {
+					tp->cur_bw_est=tp->bw_est.space/
+						(tcp_time_stamp-
+						 tp->bw_est.time);
+				}
+				tp->bw_est.space=(tp->snd_cwnd*tp->mss_cache)<<
+					tp->bw_est.shift;
+				tp->bw_est.seq=tp->snd_una+tp->bw_est.space;
+				tp->bw_est.time=tcp_time_stamp;
+			}
+		}		
+	no_mptcp_update:
+#endif
+		
 		tcp_unlink_write_queue(skb, sk);
+
+		
+		/*Cannot have more packets in flight than packets in the
+		  rexmit queue*/
+		if(tp->packets_out>skb_queue_len(&sk->sk_write_queue)) {
+			printk(KERN_ERR "acked_pcount:%d\n", acked_pcount);
+#ifdef MTCP_DEBUG_PKTS_OUT
+			printk(KERN_ERR "orig_packets:%d,orig_qsize:%d,"
+			       "orig_outsize:%d\n"
+			       "packets:%d,qsize:%d\n",orig_packets,orig_qsize,
+			       orig_outsize,
+			       tp->packets_out,
+			       skb_queue_len(&sk->sk_write_queue));
+#endif
+			BUG();
+		}
+
 		sk_wmem_free_skb(sk, skb);
 		tp->scoreboard_skb_hint = NULL;
 		if (skb == tp->retransmit_skb_hint)
@@ -2944,6 +3110,7 @@ static int tcp_clean_rtx_queue(struct sock *sk, int prior_fackets,
 			= inet_csk(sk)->icsk_ca_ops;
 
 		tcp_ack_update_rtt(sk, flag, seq_rtt);
+		BUG_ON(tcp_write_queue_empty(sk) && tp->packets_out);
 		tcp_rearm_rto(sk);
 
 		if (tcp_is_reno(tp)) {
@@ -2985,9 +3152,9 @@ static int tcp_clean_rtx_queue(struct sock *sk, int prior_fackets,
 	}
 
 #if FASTRETRANS_DEBUG > 0
-	WARN_ON((int)tp->sacked_out < 0);
-	WARN_ON((int)tp->lost_out < 0);
-	WARN_ON((int)tp->retrans_out < 0);
+	BUG_ON((int)tp->sacked_out < 0);
+	BUG_ON((int)tp->lost_out < 0);
+	BUG_ON((int)tp->retrans_out < 0);
 	if (!tp->packets_out && tcp_is_sack(tp)) {
 		icsk = inet_csk(sk);
 		if (tp->lost_out) {
@@ -3007,6 +3174,13 @@ static int tcp_clean_rtx_queue(struct sock *sk, int prior_fackets,
 		}
 	}
 #endif
+	
+	if (tcp_write_queue_empty(sk) && 
+	    icsk->icsk_pending==ICSK_TIME_RETRANS) {
+		printk(KERN_ERR "packets_out:%d, flag_acked:%d,flag:%#x\n",
+		       tp->packets_out, flag & FLAG_ACKED,flag);
+		BUG();
+	}
 	return flag;
 }
 
@@ -3014,10 +3188,21 @@ static void tcp_ack_probe(struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
 	struct inet_connection_sock *icsk = inet_csk(sk);
+	int usable_wopen;
 
 	/* Was it a usable window open? */
 
-	if (!after(TCP_SKB_CB(tcp_send_head(sk))->end_seq, tcp_wnd_end(tp))) {
+	if (tp->mpc) 
+		usable_wopen=(!after(
+				      TCP_SKB_CB(
+					      tcp_send_head(sk))->end_data_seq, 
+				      tcp_wnd_end(tp,1)));
+	else
+		usable_wopen=(!after(TCP_SKB_CB(
+					     tcp_send_head(sk))->end_seq, 
+				     tcp_wnd_end(tp,0)));
+	
+	if (usable_wopen) {
 		icsk->icsk_backoff = 0;
 		inet_csk_clear_xmit_timer(sk, ICSK_TIME_PROBE0);
 		/* Socket must be waked up by subsequent tcp_data_snd_check().
@@ -3050,7 +3235,14 @@ static inline int tcp_may_update_window(const struct tcp_sock *tp,
 					const u32 ack, const u32 ack_seq,
 					const u32 nwin)
 {
-	return (after(ack, tp->snd_una) ||
+/*the variable snd_wl1 tracks the
+  newest sequence number that we've seen.  It helps prevent snd_wnd from
+  being reopened on re-transmitted data.  If snd_wl1 is greater than
+  received sequence #, we skip it.*/
+/*MPTCP note: added check for ack_seq != 0, because the data seqnum can be 0
+  if the dataseq option is not present. This is the case if we received a pure
+  ack with no data. Since */
+	return (after(ack, tp->snd_una) || !ack_seq ||
 		after(ack_seq, tp->snd_wl1) ||
 		(ack_seq == tp->snd_wl1 && nwin > tp->snd_wnd));
 }
@@ -3066,16 +3258,31 @@ static int tcp_ack_update_window(struct sock *sk, struct sk_buff *skb, u32 ack,
 	struct tcp_sock *tp = tcp_sk(sk);
 	int flag = 0;
 	u32 nwin = ntohs(tcp_hdr(skb)->window);
+	u32 *snd_wnd=(tp->mpc && tp->mpcb)?&tp->mpcb->tp.snd_wnd:&tp->snd_wnd;
+	struct tcp_sock *mpcb_tp;
+	u32 data_ack,data_ack_seq;
+
+	if (tp->mpc && tp->mpcb) {
+		mpcb_tp=&tp->mpcb->tp;
+		data_ack=TCP_SKB_CB(skb)->data_ack;
+		data_ack_seq=TCP_SKB_CB(skb)->data_seq;
+	}
+	else {
+		mpcb_tp=tp;
+		data_ack=ack;
+		data_ack_seq=ack_seq;
+	}
 
 	if (likely(!tcp_hdr(skb)->syn))
 		nwin <<= tp->rx_opt.snd_wscale;
 
-	if (tcp_may_update_window(tp, ack, ack_seq, nwin)) {
+	if (tcp_may_update_window(mpcb_tp, data_ack, data_ack_seq, nwin)) {
+		u32 *max_window=(tp->mpc && tp->mpcb)?&tp->mpcb->tp.max_window:
+			&tp->max_window;
 		flag |= FLAG_WIN_UPDATE;
-		tcp_update_wl(tp, ack, ack_seq);
-
-		if (tp->snd_wnd != nwin) {
-			tp->snd_wnd = nwin;
+		tcp_update_wl(mpcb_tp, data_ack_seq);
+		if (*snd_wnd != nwin) {
+			*snd_wnd = nwin;
 
 			/* Note, it is the only place, where
 			 * fast path is recovered for sending TCP.
@@ -3083,14 +3290,23 @@ static int tcp_ack_update_window(struct sock *sk, struct sk_buff *skb, u32 ack,
 			tp->pred_flags = 0;
 			tcp_fast_path_check(sk);
 
-			if (nwin > tp->max_window) {
-				tp->max_window = nwin;
+			if (nwin > *max_window) {
+				*max_window = nwin;
 				tcp_sync_mss(sk, inet_csk(sk)->icsk_pmtu_cookie);
 			}
 		}
 	}
 
 	tp->snd_una = ack;
+	if (data_ack && tp->mpc && tp->mpcb) {
+		int old_snd_una=mpcb_tp->snd_una;
+		mpcb_tp->snd_una=data_ack;
+		if (old_snd_una!=data_ack)
+			mtcp_clean_rtx_queue((struct sock*)mpcb_tp);
+	}
+	if (tp->pf==1)
+		tcpprobe_logmsg(sk,"pi %d: leaving pf state",tp->path_index);
+	tp->pf=0;
 
 	return flag;
 }
@@ -3240,6 +3456,7 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp_sock *mpcb_tp=(tp->mpc && tp->mpcb)?&tp->mpcb->tp:tp;
 	u32 prior_snd_una = tp->snd_una;
 	u32 ack_seq = TCP_SKB_CB(skb)->seq;
 	u32 ack = TCP_SKB_CB(skb)->ack_seq;
@@ -3248,6 +3465,8 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 	int prior_packets;
 	int frto_cwnd = 0;
 
+	check_pkts_out(sk);
+
 	/* If the ack is newer than sent or older than previous acks
 	 * then we can probably ignore it.
 	 */
@@ -3277,7 +3496,8 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 		 * No more checks are required.
 		 * Note, we use the fact that SND.UNA>=SND.WL2.
 		 */
-		tcp_update_wl(tp, ack, ack_seq);
+		tcp_update_wl(mpcb_tp, (tp->mpc)?TCP_SKB_CB(skb)->data_seq:
+			      ack_seq);
 		tp->snd_una = ack;
 		flag |= FLAG_WIN_UPDATE;
 
@@ -3304,6 +3524,10 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 	/* We passed data and got it acked, remove any soft error
 	 * log. Something worked...
 	 */
+	if (tp->pf==1)
+		tcpprobe_logmsg(sk,"pi %d: leaving pf state",tp->path_index);
+	tp->pf=0;
+
 	sk->sk_err_soft = 0;
 	icsk->icsk_probes_out = 0;
 	tp->rcv_tstamp = tcp_time_stamp;
@@ -3334,7 +3558,7 @@ static int tcp_ack(struct sock *sk, struct sk_buff *skb, int flag)
 
 	if ((flag & FLAG_FORWARD_PROGRESS) || !(flag & FLAG_NOT_DUP))
 		dst_confirm(sk->sk_dst_cache);
-
+	check_pkts_out(sk);
 	return 1;
 
 no_queue:
@@ -3344,6 +3568,7 @@ no_queue:
 	 */
 	if (tcp_send_head(sk))
 		tcp_ack_probe(sk);
+	check_pkts_out(sk);
 	return 1;
 
 old_ack:
@@ -3354,7 +3579,13 @@ old_ack:
 	}
 
 uninteresting_ack:
+	printk(KERN_ERR "received uninteresting ack\n");
+	printk(KERN_ERR "pi %d:Ack %#x out of %#x:%#x, addr "
+	       NIPQUAD_FMT "->" NIPQUAD_FMT "\n", 
+	       tp->path_index,ack, tp->snd_una, tp->snd_nxt,
+	       NIPQUAD(inet_sk(sk)->saddr),NIPQUAD(inet_sk(sk)->daddr));
 	SOCK_DEBUG(sk, "Ack %u out of %u:%u\n", ack, tp->snd_una, tp->snd_nxt);
+	check_pkts_out(sk);
 	return 0;
 }
 
@@ -3363,15 +3594,16 @@ uninteresting_ack:
  * the fast version below fails.
  */
 void tcp_parse_options(struct sk_buff *skb, struct tcp_options_received *opt_rx,
-		       int estab)
+		       struct multipath_options *mopt, int estab)
 {
-	unsigned char *ptr;
+	unsigned char *ptr,*ptr8;
 	struct tcphdr *th = tcp_hdr(skb);
 	int length = (th->doff * 4) - sizeof(struct tcphdr);
+	int saw_dsn=0;
 
 	ptr = (unsigned char *)(th + 1);
 	opt_rx->saw_tstamp = 0;
-
+	
 	while (length > 0) {
 		int opcode = *ptr++;
 		int opsize;
@@ -3390,7 +3622,8 @@ void tcp_parse_options(struct sk_buff *skb, struct tcp_options_received *opt_rx,
 				return;	/* don't parse partial options */
 			switch (opcode) {
 			case TCPOPT_MSS:
-				if (opsize == TCPOLEN_MSS && th->syn && !estab) {
+				if (opsize == TCPOLEN_MSS && 
+				    th->syn && !estab) {
 					u16 in_mss = get_unaligned_be16(ptr);
 					if (in_mss) {
 						if (opt_rx->user_mss &&
@@ -3407,8 +3640,13 @@ void tcp_parse_options(struct sk_buff *skb, struct tcp_options_received *opt_rx,
 					opt_rx->wscale_ok = 1;
 					if (snd_wscale > 14) {
 						if (net_ratelimit())
-							printk(KERN_INFO "tcp_parse_options: Illegal window "
-							       "scaling value %d >14 received.\n",
+							printk(KERN_INFO 
+							       "tcp_parse_"
+							       "options: "
+							       "Illegal window "
+							       "scaling value "
+							       "%d >14 "
+							       "received.\n",
 							       snd_wscale);
 						snd_wscale = 14;
 					}
@@ -3433,10 +3671,15 @@ void tcp_parse_options(struct sk_buff *skb, struct tcp_options_received *opt_rx,
 				break;
 
 			case TCPOPT_SACK:
-				if ((opsize >= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK)) &&
-				   !((opsize - TCPOLEN_SACK_BASE) % TCPOLEN_SACK_PERBLOCK) &&
-				   opt_rx->sack_ok) {
-					TCP_SKB_CB(skb)->sacked = (ptr - 2) - (unsigned char *)th;
+				if ((opsize >= 
+				     (TCPOLEN_SACK_BASE + 
+				      TCPOLEN_SACK_PERBLOCK)) &&
+				    !((opsize - TCPOLEN_SACK_BASE) % 
+				      TCPOLEN_SACK_PERBLOCK) &&
+				    opt_rx->sack_ok) {
+					
+					TCP_SKB_CB(skb)->sacked = 
+						(ptr - 2) - (unsigned char *)th;
 				}
 				break;
 #ifdef CONFIG_TCP_MD5SIG
@@ -3447,12 +3690,109 @@ void tcp_parse_options(struct sk_buff *skb, struct tcp_options_received *opt_rx,
 				 */
 				break;
 #endif
-			}
 
+#ifdef CONFIG_MTCP
+			case TCPOPT_MPC:
+				if (opsize!=TCPOLEN_MPC) {
+					mtcp_debug("multipath opt:bad option "
+					           "size\n");
+					break;
+				}
+				mtcp_debug("recvd multipath opt\n");
+				opt_rx->saw_mpc=1;
+				if (mopt)
+					mopt->list_rcvd=1;
+#ifdef CONFIG_MTCP_PM
+				opt_rx->mtcp_rem_token=
+					ntohl(*((u32*)(ptr+1)));
+#endif
+				break;
+				
+#ifdef CONFIG_MTCP_PM
+			case TCPOPT_ADDR:
+				if (!mopt) {
+					printk(KERN_ERR "MPTCP addresses "
+					       "received, but no mptcp state"
+					       "found, using sock struct\n");
+					break;
+				}
+				
+				for (ptr8=ptr; ptr8<ptr+opsize-2;) {
+					if ((*(ptr8+1))>>4==4) {
+						mtcp_v4_add_raddress(
+							mopt,
+							(struct in_addr*) 
+							(ptr8+2),
+							*ptr8
+							);
+						ptr8+=2+sizeof(struct in_addr);
+					}
+					/*Add IPv6 stuff here*/
+				}
+				break;
+
+			case TCPOPT_JOIN:
+				break;
+#endif /*CONFIG_MTCP_PM*/				
+			case TCPOPT_DSN:
+				if (opsize!=TCPOLEN_DSN) {
+					mtcp_debug("dataseq opt:bad option "
+					           "size\n");
+					break;
+				}
+				
+				TCP_SKB_CB(skb)->data_len = 
+					ntohs(*(uint16_t*)ptr);			
+				TCP_SKB_CB(skb)->sub_seq = 
+					ntohl(*(uint32_t*)(ptr+2))+
+					opt_rx->rcv_isn;
+				TCP_SKB_CB(skb)->data_seq = 
+					ntohl(*(uint32_t*)(ptr+6));
+				TCP_SKB_CB(skb)->end_data_seq=
+					TCP_SKB_CB(skb)->data_seq+
+					TCP_SKB_CB(skb)->end_seq-
+					TCP_SKB_CB(skb)->seq;
+				saw_dsn=1;
+				break;
+			case TCPOPT_DFIN:
+				/*the dsn opt MUST be put
+				  before the dfin (to know 
+				  the its data seqnum*/
+				BUG_ON(!saw_dsn);
+				if (opsize!=TCPOLEN_DFIN) {
+					mtcp_debug("dfin opt:bad option "
+						   "size\n");
+					break;
+				}
+				TCP_SKB_CB(skb)->end_data_seq++;
+				if (mopt) {
+					mopt->dfin_rcvd=opt_rx->saw_dfin=1;
+					mopt->fin_dsn=TCP_SKB_CB(skb)->data_seq+
+						TCP_SKB_CB(skb)->data_len;
+				}
+				break;
+			case TCPOPT_DATA_ACK:
+				if (opsize!=TCPOLEN_DATA_ACK) {
+					mtcp_debug("data_ack opt:bad option "
+					       "size\n");
+					break;
+				}
+				TCP_SKB_CB(skb)->data_ack =
+					ntohl(*(uint32_t*)ptr);
+				break;				
+#endif /* CONFIG_MTCP */
+			}
+			
 			ptr += opsize-2;
 			length -= opsize;
 		}
 	}
+#ifdef CONFIG_MTCP
+	if (!saw_dsn)
+		TCP_SKB_CB(skb)->data_len=
+			TCP_SKB_CB(skb)->data_seq=
+			TCP_SKB_CB(skb)->end_data_seq=0;
+#endif
 }
 
 static int tcp_parse_aligned_timestamp(struct tcp_sock *tp, struct tcphdr *th)
@@ -3477,6 +3817,8 @@ static int tcp_parse_aligned_timestamp(struct tcp_sock *tp, struct tcphdr *th)
 static int tcp_fast_parse_options(struct sk_buff *skb, struct tcphdr *th,
 				  struct tcp_sock *tp)
 {
+	struct multipath_pcb* mpcb;
+	struct multipath_options *mopt;
 	if (th->doff == sizeof(struct tcphdr) >> 2) {
 		tp->rx_opt.saw_tstamp = 0;
 		return 0;
@@ -3485,7 +3827,26 @@ static int tcp_fast_parse_options(struct sk_buff *skb, struct tcphdr *th,
 		if (tcp_parse_aligned_timestamp(tp, th))
 			return 1;
 	}
-	tcp_parse_options(skb, &tp->rx_opt, 1);
+	mpcb = mpcb_from_tcpsock(tp);
+	if (tp->pending)
+		mpcb=mtcp_hash_find(tp->mtcp_loc_token);
+	if (mpcb) mopt=&mpcb->received_options;
+	else {
+		mtcp_debug("mpcb null in fast parse options\n");
+		mopt=&tp->mopt;
+	}
+	tcp_parse_options(skb, &tp->rx_opt,mopt,1);
+	if (unlikely(mpcb && tp->rx_opt.saw_mpc && is_master_sk(tp))) {
+		/*Transfer sndwnd control to the mpcb*/
+		mpcb->tp.snd_wnd=tp->snd_wnd;
+		mpcb->tp.max_window=tp->max_window;		
+		tp->mpc=1;		
+		tp->rx_opt.saw_mpc=0; /*reset that field, it has been read*/
+	}
+	/*It can be that mpcb is NULL while tp is pending
+	  if tp is the master sk.*/
+	if (tp->pending && mpcb) 
+		mpcb_put(mpcb);
 	return 1;
 }
 
@@ -3577,6 +3938,19 @@ static int tcp_disordered_ack(const struct sock *sk, const struct sk_buff *skb)
 	struct tcphdr *th = tcp_hdr(skb);
 	u32 seq = TCP_SKB_CB(skb)->seq;
 	u32 ack = TCP_SKB_CB(skb)->ack_seq;
+	struct tcp_sock *mpcb_tp;
+	u32 data_ack,data_seq;
+
+	if (tp->mpc && tp->mpcb) {
+		mpcb_tp=&tp->mpcb->tp;
+		data_ack=TCP_SKB_CB(skb)->data_ack;
+		data_seq=TCP_SKB_CB(skb)->data_seq;
+	}
+	else {
+		mpcb_tp=tp;
+		data_ack=ack;
+		data_seq=seq;
+	}
 
 	return (/* 1. Pure ACK with correct sequence number. */
 		(th->ack && seq == TCP_SKB_CB(skb)->end_seq && seq == tp->rcv_nxt) &&
@@ -3585,7 +3959,7 @@ static int tcp_disordered_ack(const struct sock *sk, const struct sk_buff *skb)
 		ack == tp->snd_una &&
 
 		/* 3. ... and does not update window. */
-		!tcp_may_update_window(tp, ack, seq, ntohs(th->window) << tp->rx_opt.snd_wscale) &&
+		!tcp_may_update_window(mpcb_tp, data_ack, data_seq, ntohs(th->window) << tp->rx_opt.snd_wscale) &&
 
 		/* 4. ... and sits in replay window. */
 		(s32)(tp->rx_opt.ts_recent - tp->rx_opt.rcv_tsval) <= (inet_csk(sk)->icsk_rto * 1024) / HZ);
@@ -3924,6 +4298,7 @@ static void tcp_ofo_queue(struct sock *sk)
 	struct tcp_sock *tp = tcp_sk(sk);
 	__u32 dsack_high = tp->rcv_nxt;
 	struct sk_buff *skb;
+	int mtcp_eaten=0;
 
 	while ((skb = skb_peek(&tp->out_of_order_queue)) != NULL) {
 		if (after(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))
@@ -3947,32 +4322,143 @@ static void tcp_ofo_queue(struct sock *sk)
 			   TCP_SKB_CB(skb)->end_seq);
 
 		__skb_unlink(skb, &tp->out_of_order_queue);
-		__skb_queue_tail(&sk->sk_receive_queue, skb);
+		mtcp_eaten=mtcp_queue_skb(sk,skb);
+		
 		tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
+
 		if (tcp_hdr(skb)->fin)
 			tcp_fin(skb, sk, tcp_hdr(skb));
+#ifdef CONFIG_MTCP
+		if (tp->mpc && mtcp_eaten==MTCP_EATEN)
+			__kfree_skb(skb);
+#endif				
 	}
 }
 
 static int tcp_prune_ofo_queue(struct sock *sk);
 static int tcp_prune_queue(struct sock *sk);
 
-static inline int tcp_try_rmem_schedule(struct sock *sk, unsigned int size)
+static void check_buffers(struct multipath_pcb *mpcb)
 {
-	if (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||
-	    !sk_rmem_schedule(sk, size)) {
+	struct sock *sk;
+	struct tcp_sock *tp;
+	struct sk_buff *skb;
 
-		if (tcp_prune_queue(sk) < 0)
-			return -1;
+	mtcp_for_each_sk(mpcb,sk,tp) {
+		int ofo_size=0,rcv_size=0;
+		if (sk->sk_state!=TCP_ESTABLISHED)
+			continue;
+		for(skb=skb_peek(&tp->out_of_order_queue);
+		    skb ;skb=(skb_queue_is_last(&tp->out_of_order_queue,skb)?
+			      NULL:
+			      skb_queue_next(&tp->out_of_order_queue,skb)))
+			ofo_size+=skb->truesize;
+		for(skb=skb_peek(&sk->sk_receive_queue);
+		    skb;skb=(skb_queue_is_last(&sk->sk_receive_queue,skb)?NULL:
+			     skb_queue_next(&sk->sk_receive_queue,skb)))
+			rcv_size+=skb->truesize;
+		skb=skb_peek(&sk->sk_receive_queue);
+		printk(KERN_ERR "pi %d, ofo_size:%d,rcv_size:%d, waiting:%d,"
+		       "next dsn:%#x\n",
+		       tp->path_index, ofo_size,rcv_size,tp->wait_data_bit_set,
+		       (skb?TCP_SKB_CB(skb)->data_seq:0));
+	}
+}
 
-		if (!sk_rmem_schedule(sk, size)) {
-			if (!tcp_prune_ofo_queue(sk))
-				return -1;
 
-			if (!sk_rmem_schedule(sk, size))
-				return -1;
+static inline int tcp_try_rmem_schedule(struct sock *sk, unsigned int size)
+{
+	struct tcp_sock *tp=tcp_sk(sk);
+	struct sk_buff *skb;
+
+	if (tp->mpc && tp->mpcb) {
+		struct tcp_sock *mpcb_tp=&tp->mpcb->tp;
+		struct sock *mpcb_sk=(struct sock*)mpcb_tp;
+		if (atomic_read(&mpcb_sk->sk_rmem_alloc) > 
+		    mpcb_sk->sk_rcvbuf) {
+			tcpprobe_logmsg(mpcb_sk,"PROBLEM NOW");
+			printk(KERN_ERR "not enough rcvbuf\n");
+			printk(KERN_ERR "mpcb rcvbuf:%d - rmem_alloc:%d\n",
+			       mpcb_sk->sk_rcvbuf,atomic_read(
+				       &mpcb_sk->sk_rmem_alloc));
+			check_buffers(tp->mpcb);
+			printk(KERN_ERR "mpcb copied seq:%#x\n",
+			       mpcb_tp->copied_seq);
+			mtcp_for_each_sk(tp->mpcb,sk,tp) {
+				if (sk->sk_state!=TCP_ESTABLISHED)
+					continue;
+				printk(KERN_ERR "pi %d,rcvbuf:%d,"
+				       "rmem_alloc:%d\n",
+				       tp->path_index,
+				       sk->sk_rcvbuf,
+				       atomic_read(&sk->sk_rmem_alloc));
+				printk(KERN_ERR "pi %d receive queue:\n",
+				       tp->path_index);
+				printk(KERN_ERR "used mss for wnd "
+				       "computation:%d\n",
+				       inet_csk(sk)->icsk_ack.rcv_mss);
+				skb_queue_walk(&sk->sk_receive_queue, skb) {
+					printk(KERN_ERR "  dsn:%#x, "
+					       "skb->len:%d,truesize:%d,"
+					       "prop:%d /1000\n",
+					       TCP_SKB_CB(skb)->data_seq,
+					       skb->len, skb->truesize,
+					       skb->len*1000/skb->truesize);
+				}
+				printk(KERN_ERR "pi %d ofo queue:\n",
+				       tp->path_index);
+				skb_queue_walk(&tp->out_of_order_queue, skb) {
+					printk(KERN_ERR "  dsn:%#x, "
+					       "skb->len:%d,truesize:%d,"
+					       "prop:%d /1000\n",
+					       TCP_SKB_CB(skb)->data_seq,
+					       skb->len, skb->truesize,
+					       skb->len*1000/skb->truesize);
+				}
+			}
+			printk(KERN_ERR "meta-receive queue:\n");
+			skb_queue_walk(&mpcb_sk->sk_receive_queue, skb) {
+				printk(KERN_ERR "  dsn:%#x, "
+				       "skb->len:%d,truesize:%d,"
+				       "prop:%d /1000\n",
+				       TCP_SKB_CB(skb)->data_seq,
+				       skb->len, skb->truesize,
+				       skb->len*1000/skb->truesize);
+			}
+			printk(KERN_ERR "meta-ofo queue:\n");
+			skb_queue_walk(&mpcb_tp->out_of_order_queue, skb) {
+				printk(KERN_ERR "  dsn:%#x, "
+				       "skb->len:%d,truesize:%d,"
+				       "prop:%d /1000\n",
+				       TCP_SKB_CB(skb)->data_seq,
+				       skb->len, skb->truesize,
+				       skb->len*1000/skb->truesize);
+			}
+			
+			return 0;
+		}
+		else if (!sk_rmem_schedule(sk,size)) {
+			printk(KERN_ERR "impossible to alloc memory\n");
+		}
+		if (atomic_read(&mpcb_sk->sk_rmem_alloc) <= mpcb_sk->sk_rcvbuf 
+		    && sk_rmem_schedule(sk,size)) {
+			return 0;
 		}
 	}
+	else if (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&
+		 sk_rmem_schedule(sk, size))
+		return 0;
+	
+	if (tcp_prune_queue(sk) < 0)
+		return -1;
+	
+	if (!sk_rmem_schedule(sk, size)) {
+		if (!tcp_prune_ofo_queue(sk))
+			return -1;
+		
+		if (!sk_rmem_schedule(sk, size))
+			return -1;
+	}
 	return 0;
 }
 
@@ -3980,7 +4466,12 @@ static void tcp_data_queue(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcphdr *th = tcp_hdr(skb);
 	struct tcp_sock *tp = tcp_sk(sk);
+#ifdef CONFIG_MTCP
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+	int mapping=0;
+#endif
 	int eaten = -1;
+	int mtcp_eaten=0;
 
 	if (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq)
 		goto drop;
@@ -3993,7 +4484,7 @@ static void tcp_data_queue(struct sock *sk, struct sk_buff *skb)
 		tp->rx_opt.dsack = 0;
 		tp->rx_opt.eff_sacks = tp->rx_opt.num_sacks;
 	}
-
+	
 	/*  Queue data for delivery to the user.
 	 *  Packets in sequence go to the receive queue.
 	 *  Out of sequence packets to the out_of_order_queue.
@@ -4003,16 +4494,71 @@ static void tcp_data_queue(struct sock *sk, struct sk_buff *skb)
 			goto out_of_window;
 
 		/* Ok. In sequence. In window. */
+#ifdef CONFIG_MTCP
+		if (tp->mpc) {
+			mapping=mtcp_get_dataseq_mapping(tp,skb);
+			if (mapping==-1) goto drop;
+		}
+		if (mpcb && mpcb->ucopy.task == current &&
+		    tp->copied_seq == tp->rcv_nxt && mpcb->ucopy.len &&
+		    sock_owned_by_user(sk) && !tp->urg_data) {
+			if (tp->mpc) {
+				if (mapping==1) { /*in meta-order*/
+					int chunk = min_t(unsigned int, skb->len,
+							  mpcb->ucopy.len);
+					
+					__set_current_state(TASK_RUNNING);
+					
+					local_bh_enable();
+					if (!skb_copy_datagram_iovec(skb, 0, 
+								     mpcb->ucopy.iov, 
+								     chunk)) {
+						
+						mtcp_check_seqnums(mpcb,1);
+						
+						mpcb->ucopy.len -= chunk;
+						tp->copied_seq += chunk;
+						mpcb->tp.copied_seq += chunk;
+						tp->copied += chunk;
+						eaten = (chunk == skb->len && !th->fin);
+						tcp_rcv_space_adjust(sk);
+						
+						mtcp_check_seqnums(mpcb,0);
+					}
+					local_bh_disable();
+				}
+			}
+			else {
+				int chunk = min_t(unsigned int, skb->len,
+						  mpcb->ucopy.len);
+				
+				__set_current_state(TASK_RUNNING);
+				
+				local_bh_enable();
+				if (!skb_copy_datagram_iovec(skb, 0, mpcb->ucopy.iov, 
+							     chunk)) {
+					mpcb->ucopy.len -= chunk;
+					tp->copied_seq += chunk;
+					eaten = (chunk == skb->len && !th->fin);
+					tcp_rcv_space_adjust(sk);
+				}
+				local_bh_disable();
+			}
+		}
+		
+
+#else
 		if (tp->ucopy.task == current &&
 		    tp->copied_seq == tp->rcv_nxt && tp->ucopy.len &&
 		    sock_owned_by_user(sk) && !tp->urg_data) {
 			int chunk = min_t(unsigned int, skb->len,
 					  tp->ucopy.len);
-
+			
 			__set_current_state(TASK_RUNNING);
-
+			
 			local_bh_enable();
-			if (!skb_copy_datagram_iovec(skb, 0, tp->ucopy.iov, chunk)) {
+			if (!skb_copy_datagram_iovec(skb, 0, tp->ucopy.iov, 
+						     chunk)) {
 				tp->ucopy.len -= chunk;
 				tp->copied_seq += chunk;
 				eaten = (chunk == skb->len && !th->fin);
@@ -4020,22 +4566,30 @@ static void tcp_data_queue(struct sock *sk, struct sk_buff *skb)
 			}
 			local_bh_disable();
 		}
-
+#endif
+		
 		if (eaten <= 0) {
-queue_and_out:
+		queue_and_out:
 			if (eaten < 0 &&
-			    tcp_try_rmem_schedule(sk, skb->truesize))
+			    tcp_try_rmem_schedule(sk, skb->truesize)) {
+				printk(KERN_ERR "dropping seg after"
+				       " tcp_try_rmem_schedule\n");
 				goto drop;
-
+			}
+			
 			skb_set_owner_r(skb, sk);
-			__skb_queue_tail(&sk->sk_receive_queue, skb);
+			mtcp_eaten=mtcp_queue_skb(sk,skb);
 		}
 		tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 		if (skb->len)
 			tcp_event_data_recv(sk, skb);
+
 		if (th->fin)
 			tcp_fin(skb, sk, th);
 
+		if (mtcp_eaten==MTCP_EATEN)
+			__kfree_skb(skb);
+		
 		if (!skb_queue_empty(&tp->out_of_order_queue)) {
 			tcp_ofo_queue(sk);
 
@@ -4050,11 +4604,27 @@ queue_and_out:
 			tcp_sack_remove(tp);
 
 		tcp_fast_path_check(sk);
-
+		
 		if (eaten > 0)
 			__kfree_skb(skb);
-		else if (!sock_flag(sk, SOCK_DEAD))
+		else if (!sock_flag(sk, SOCK_DEAD)) {
+#ifdef CONFIG_MTCP
+			/*If mapping is 1, we know that the segment is
+			  in order and in meta-order.
+			  So we can wake up the app. Further, we know that 
+			  eaten is not >0, thus it has not been completely
+			  eaten by the prequeue (otherwise, no need to call
+			  mtcp_data_ready, the prequeue does it).*/
+			if (tp->mpc) {
+				if (mapping==1)
+					mtcp_data_ready(sk);
+			}
+			else sk->sk_data_ready(sk,0);
+#else
 			sk->sk_data_ready(sk, 0);
+#endif
+		}
+
 		return;
 	}
 
@@ -4066,11 +4636,11 @@ queue_and_out:
 out_of_window:
 		tcp_enter_quickack_mode(sk);
 		inet_csk_schedule_ack(sk);
-drop:
+drop:		
 		__kfree_skb(skb);
 		return;
 	}
-
+	
 	/* Out of window. F.e. zero window probe. */
 	if (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt + tcp_receive_window(tp)))
 		goto out_of_window;
@@ -4182,6 +4752,7 @@ add_sack:
 	}
 }
 
+#ifndef CONFIG_MTCP
 static struct sk_buff *tcp_collapse_one(struct sock *sk, struct sk_buff *skb,
 					struct sk_buff_head *list)
 {
@@ -4193,19 +4764,28 @@ static struct sk_buff *tcp_collapse_one(struct sock *sk, struct sk_buff *skb,
 
 	return next;
 }
+#endif
 
 /* Collapse contiguous sequence of skbs head..tail with
  * sequence numbers start..end.
  * Segments with FIN/SYN are not collapsed (only because this
  * simplifies code)
+ *
+ * TODO: for MPTCP, we CANNOT collapse segments that have non contiguous 
+ * dataseq numbers. It is possible the seq numbers are contiguous but not
+ * dataseq. In that case we must keep the segments separated. Until this
+ * is supported, we disable the tcp_collapse function.
+ * NOTE that when supporting this, we will need to ensure that the path_index
+ * field is copied when creating the new skbuff.
  */
+#ifndef CONFIG_MTCP
 static void
 tcp_collapse(struct sock *sk, struct sk_buff_head *list,
 	     struct sk_buff *head, struct sk_buff *tail,
 	     u32 start, u32 end)
 {
 	struct sk_buff *skb;
-
+	
 	/* First, check that queue is collapsible and find
 	 * the point where collapsing can be useful. */
 	for (skb = head; skb != tail;) {
@@ -4271,6 +4851,7 @@ tcp_collapse(struct sock *sk, struct sk_buff_head *list,
 				if (skb_copy_bits(skb, offset, skb_put(nskb, size), size))
 					BUG();
 				TCP_SKB_CB(nskb)->end_seq += size;
+				TCP_SKB_CB(nskb)->end_data_seq += size;
 				copy -= size;
 				start += size;
 			}
@@ -4281,13 +4862,15 @@ tcp_collapse(struct sock *sk, struct sk_buff_head *list,
 				    tcp_hdr(skb)->fin)
 					return;
 			}
-		}
+		}		
 	}
 }
+#endif
 
 /* Collapse ofo queue. Algorithm: select contiguous sequence of skbs
  * and tcp_collapse() them until all the queue is collapsed.
  */
+#ifndef CONFIG_MTCP
 static void tcp_collapse_ofo_queue(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -4319,13 +4902,17 @@ static void tcp_collapse_ofo_queue(struct sock *sk)
 			start = TCP_SKB_CB(skb)->seq;
 			end = TCP_SKB_CB(skb)->end_seq;
 		} else {
-			if (before(TCP_SKB_CB(skb)->seq, start))
+			if (before(TCP_SKB_CB(skb)->seq, start)) {
 				start = TCP_SKB_CB(skb)->seq;
-			if (after(TCP_SKB_CB(skb)->end_seq, end))
+			}
+			if (after(TCP_SKB_CB(skb)->end_seq, end)) {
 				end = TCP_SKB_CB(skb)->end_seq;
+			}
 		}
 	}
 }
+#endif
+
 
 /*
  * Purge the out-of-order queue.
@@ -4373,11 +4960,15 @@ static int tcp_prune_queue(struct sock *sk)
 	else if (tcp_memory_pressure)
 		tp->rcv_ssthresh = min(tp->rcv_ssthresh, 4U * tp->advmss);
 
+#ifndef CONFIG_MTCP
 	tcp_collapse_ofo_queue(sk);
 	tcp_collapse(sk, &sk->sk_receive_queue,
 		     sk->sk_receive_queue.next,
 		     (struct sk_buff *)&sk->sk_receive_queue,
 		     tp->copied_seq, tp->rcv_nxt);
+#else
+	mtcp_update_window_clamp(tp->mpcb);
+#endif
 	sk_mem_reclaim(sk);
 
 	if (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)
@@ -4411,7 +5002,7 @@ void tcp_cwnd_application_limited(struct sock *sk)
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	if (inet_csk(sk)->icsk_ca_state == TCP_CA_Open &&
-	    sk->sk_socket && !test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {
+	    sk->sk_socket && !test_bit(SOCK_NOSPACE, &sk->sock_flags)) {
 		/* Limited by application or receiver window. */
 		u32 init_win = tcp_init_cwnd(tp, __sk_dst_get(sk));
 		u32 win_used = max(tp->snd_cwnd_used, init_win);
@@ -4424,14 +5015,14 @@ void tcp_cwnd_application_limited(struct sock *sk)
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 }
 
-static int tcp_should_expand_sndbuf(struct sock *sk)
+static int tcp_should_expand_sndbuf(struct sock *sk, struct sock *mpcb_sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	/* If the user specified a specific send buffer setting, do
 	 * not modify it.
 	 */
-	if (sk->sk_userlocks & SOCK_SNDBUF_LOCK)
+	if (mpcb_sk->sk_userlocks & SOCK_SNDBUF_LOCK)
 		return 0;
 
 	/* If we are under global TCP memory pressure, do not expand.  */
@@ -4442,8 +5033,12 @@ static int tcp_should_expand_sndbuf(struct sock *sk)
 	if (atomic_read(&tcp_memory_allocated) >= sysctl_tcp_mem[0])
 		return 0;
 
-	/* If we filled the congestion window, do not expand.  */
-	if (tp->packets_out >= tp->snd_cwnd)
+	/* If we filled the congestion window, do not expand.  
+	   MPTCP note: at the moment, we do not take this case into account.
+	   In the future, if we want to do so, we'll need to maintain
+	   packet_out counter at the mpcb_tp level, as well as maintain a 
+	   mpcb_tp->snd_cwnd=sum(sub_tp->snd_cwnd)*/
+	if (!tp->mpc && tp->packets_out >= tp->snd_cwnd)
 		return 0;
 
 	return 1;
@@ -4455,38 +5050,99 @@ static int tcp_should_expand_sndbuf(struct sock *sk)
  *
  * PROBLEM: sndbuf expansion does not work well with largesend.
  */
-static void tcp_new_space(struct sock *sk)
+static void tcp_new_space(struct sock *sk, struct sock *mpcb_sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	if (tcp_should_expand_sndbuf(sk)) {
+	if (tcp_should_expand_sndbuf(sk,mpcb_sk)) {
 		int sndmem = max_t(u32, tp->rx_opt.mss_clamp, tp->mss_cache) +
 			MAX_TCP_HEADER + 16 + sizeof(struct sk_buff);
+#ifndef CONFIG_MTCP
 		int demanded = max_t(unsigned int, tp->snd_cwnd,
 				     tp->reordering + 1);
+#else
+		int demanded;
+		struct multipath_pcb *mpcb=tp->mpcb;
+		struct tcp_sock *tp_it;
+		u32 rtt_max=tp->srtt;
+		mtcp_for_each_tp(mpcb,tp_it)
+			if (rtt_max<tp_it->srtt)
+				rtt_max=tp_it->srtt;
+
+		/* Normally the send buffer is computed as twice the BDP
+		 * However in multipath, a fast path may need more buffer
+		 * for the following reason:
+		 * Imagine 2 flows with same bw b, and delay 10 and 100, resp.
+		 * Normally flow 10 will have send buffer 2*b*10
+		 *               100 will have send buffer 2*b*100
+		 * In order to minimize reordering at the receiver, the sender
+		 * must ensure that all consecutive packets are sent as close to
+		 * each other as possible, even when spread across several 
+		 * subflows. If we represent a buffer as having a "height" in 
+		 * time units, and a "width" in bandwidth units, we must ensure 
+		 * that each segment is sent on the buffer with smallest 
+		 * "current height". (lowest filling related to his height).
+		 * The subflow max height, given that its width is its bw,
+		 * is computed as 2d traditionnally, thus 20 and 200 resp. here.
+		 * The problem is that if buffer with delay 10, is kept
+		 * at size 2*b*10, the scheduler will be able to schedule
+		 * segments until height=20 maximum. In summary, the use of 
+		 * all buffers is reduced to the hight of the smallest one.
+		 * This is why all buffers must be arranged to have equal
+		 * height, that height being the highest height needed by the
+		 * network, that is 2*max(delays).
+		 */
+		/*If cur_bw_est not yet computed, just delay the re-evaluation
+		  of sndbuf*/
+		if (!tp->cur_bw_est)
+			demanded=0;
+		else 
+			demanded = max_t(unsigned int, 
+					 (tp->cur_bw_est>>tp->bw_est.shift)*
+					 (rtt_max>>3),
+					 tp->reordering + 1);
+#endif
+		/*After this, sndmem is the new contribution of the
+		  current subflow to the aggregate sndbuf*/
 		sndmem *= 2 * demanded;
-		if (sndmem > sk->sk_sndbuf)
+		if (sndmem > sk->sk_sndbuf) {
+			int old_sndbuf=sk->sk_sndbuf;
 			sk->sk_sndbuf = min(sndmem, sysctl_tcp_wmem[2]);
+			/*ok, the subflow sndbuf has grown, reflect this in
+			  the aggregate buffer.*/
+			if (old_sndbuf!=sk->sk_sndbuf && tp->mpcb)
+				mtcp_update_sndbuf(tp->mpcb);
+		}
 		tp->snd_cwnd_stamp = tcp_time_stamp;
 	}
 
-	sk->sk_write_space(sk);
+	sk->sk_write_space(mpcb_sk);
 }
 
-static void tcp_check_space(struct sock *sk)
+
+/*If the flow is MPTCP, sk is the subsock, and mpcb_sk is the mpcb.
+  Otherwise both are the regular TCP socket.*/
+void tcp_check_space(struct sock *sk,struct sock *mpcb_sk)
 {
-	if (sock_flag(sk, SOCK_QUEUE_SHRUNK)) {
-		sock_reset_flag(sk, SOCK_QUEUE_SHRUNK);
-		if (sk->sk_socket &&
-		    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags))
-			tcp_new_space(sk);
+	if (sock_flag(mpcb_sk, SOCK_QUEUE_SHRUNK)) {
+		sock_reset_flag(mpcb_sk, SOCK_QUEUE_SHRUNK);
+		if (mpcb_sk->sk_socket &&
+		    test_bit(SOCK_NOSPACE, &mpcb_sk->sock_flags))
+			tcp_new_space(sk,mpcb_sk);
 	}
 }
 
 static inline void tcp_data_snd_check(struct sock *sk)
 {
-	tcp_push_pending_frames(sk);
-	tcp_check_space(sk);
+	struct sock *mpcb_sk;
+	
+	BUG_ON(is_meta_sk(sk));
+	if (tcp_sk(sk)->mpc && tcp_sk(sk)->mpcb)
+		mpcb_sk=((struct sock*)tcp_sk(sk)->mpcb);
+	else
+		mpcb_sk=sk;
+	tcp_push_pending_frames(mpcb_sk);
+	tcp_check_space(sk,mpcb_sk);
 }
 
 /*
@@ -4516,10 +5172,8 @@ static void __tcp_ack_snd_check(struct sock *sk, int ofo_possible)
 
 static inline void tcp_ack_snd_check(struct sock *sk)
 {
-	if (!inet_csk_ack_scheduled(sk)) {
-		/* We sent a data segment already. */
+	if (!inet_csk_ack_scheduled(sk))
 		return;
-	}
 	__tcp_ack_snd_check(sk, 1);
 }
 
@@ -4604,14 +5258,19 @@ static void tcp_urg(struct sock *sk, struct sk_buff *skb, struct tcphdr *th)
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	/* Check if we get a new urgent pointer - normally not. */
-	if (th->urg)
+	if (tp->mpc && th->urg) {
+		/*Not supported yet*/
+		BUG();
 		tcp_check_urg(sk, th);
+	}
 
 	/* Do we wait for any urgent data? - normally not... */
-	if (tp->urg_data == TCP_URG_NOTYET) {
+	if (tp->urg_data == TCP_URG_NOTYET) {		
 		u32 ptr = tp->urg_seq - ntohl(th->seq) + (th->doff * 4) -
 			  th->syn;
 
+		/*Not supported yet*/
+		if (tp->mpc) BUG();
 		/* Is the urgent pointer pointing into this packet? */
 		if (ptr < skb->len) {
 			u8 tmp;
@@ -4624,6 +5283,40 @@ static void tcp_urg(struct sock *sk, struct sk_buff *skb, struct tcphdr *th)
 	}
 }
 
+#ifdef CONFIG_MTCP
+static int tcp_copy_to_iovec(struct sock *sk, struct sk_buff *skb, int hlen)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+	int chunk = skb->len - hlen;
+	int err;
+
+	mtcp_debug("Entering %s\n",__FUNCTION__);
+
+	local_bh_enable();
+	if (skb_csum_unnecessary(skb))
+		err = skb_copy_datagram_iovec(skb, hlen, mpcb->ucopy.iov, 
+					      chunk);
+	else
+		err = skb_copy_and_csum_datagram_iovec(skb, hlen,
+						       mpcb->ucopy.iov);
+
+	if (!err) {
+		mtcp_check_seqnums(mpcb,1);
+
+		mpcb->ucopy.len -= chunk;
+		tp->copied_seq += chunk;
+		mpcb->tp.copied_seq += chunk;
+		tp->copied += chunk;
+		tcp_rcv_space_adjust(sk);
+
+		mtcp_check_seqnums(mpcb,0);
+	}
+
+	local_bh_disable();
+	return err;
+}
+#else
 static int tcp_copy_to_iovec(struct sock *sk, struct sk_buff *skb, int hlen)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -4646,6 +5339,7 @@ static int tcp_copy_to_iovec(struct sock *sk, struct sk_buff *skb, int hlen)
 	local_bh_disable();
 	return err;
 }
+#endif /*CONFIG_MTCP*/
 
 static __sum16 __tcp_checksum_complete_user(struct sock *sk,
 					    struct sk_buff *skb)
@@ -4800,6 +5494,232 @@ discard:
  *	the rest is checked inline. Fast processing is turned on in
  *	tcp_data_queue when everything is OK.
  */
+#ifdef CONFIG_MTCP
+int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
+			struct tcphdr *th, unsigned len)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct multipath_pcb *mpcb = mpcb_from_tcpsock(tp);
+	int res;
+	int mtcp_eaten=0;
+	
+	tcpprobe_rcv_established(sk,skb,th,len);
+
+	/*
+	 *	Header prediction.
+	 *	The code loosely follows the one in the famous
+	 *	"30 instruction TCP receive" Van Jacobson mail.
+	 *
+	 *	Van's trick is to deposit buffers into socket queue
+	 *	on a device interrupt, to call tcp_recv function
+	 *	on the receive process context and checksum and copy
+	 *	the buffer to user space. smart...
+	 *
+	 *	Our current scheme is not silly either but we take the
+	 *	extra cost of the net_bh soft interrupt processing...
+	 *	We do checksum and copy also but from device to kernel.
+	 */
+	
+	tp->rx_opt.saw_tstamp = 0;
+	/*sbarre: force slowpath at the moment. Will carefully check
+	  fast path for mptcp later.*/
+	goto slow_path;
+
+	/*	pred_flags is 0xS?10 << 16 + snd_wnd
+	 *	if header_prediction is to be made
+	 *	'S' will always be tp->tcp_header_len >> 2
+	 *	'?' will be 0 for the fast path, otherwise pred_flags is 0 to
+	 *  turn it off	(when there are holes in the receive
+	 *	 space for instance)
+	 *	PSH flag is ignored.
+	 */
+
+	if ((tcp_flag_word(th) & TCP_HP_BITS) == tp->pred_flags &&
+	    TCP_SKB_CB(skb)->seq == tp->rcv_nxt) {
+		int tcp_header_len = tp->tcp_header_len;
+
+		/* Timestamp header prediction: tcp_header_len
+		 * is automatically equal to th->doff*4 due to pred_flags
+		 * match.
+		 */
+
+		/* Check timestamp */
+		if (tcp_header_len == sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) {
+			/* No? Slow path! */
+			if (!tcp_parse_aligned_timestamp(tp, th))
+				goto slow_path;
+
+			/* If PAWS failed, check it more carefully in slow 
+			   path */
+			if ((s32)(tp->rx_opt.rcv_tsval - tp->rx_opt.ts_recent) 
+			    < 0)
+				goto slow_path;
+
+			/* DO NOT update ts_recent here, if checksum fails
+			 * and timestamp was corrupted part, it will result
+			 * in a hung connection since we will drop all
+			 * future packets due to the PAWS test.
+			 */
+		}
+
+		if (len <= tcp_header_len) {
+			/* Bulk data transfer: sender */
+			if (len == tcp_header_len) {
+				/* Predicted packet is in window by definition.
+				 * seq == rcv_nxt and rcv_wup <= rcv_nxt.
+				 * Hence, check seq<=rcv_wup reduces to:
+				 */
+				if (tcp_header_len ==
+				    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&
+				    tp->rcv_nxt == tp->rcv_wup)
+					tcp_store_ts_recent(tp);
+
+				/* We know that such packets are checksummed
+				 * on entry.
+				 */
+				tcp_ack(sk, skb, 0);
+				__kfree_skb(skb);
+				tcp_data_snd_check(sk);
+				return 0;
+			} else { /* Header too small */
+				TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
+				goto discard;
+			}
+		} else {
+			int eaten = 0;
+
+			if (tp->copied_seq == tp->rcv_nxt &&
+			    len - tcp_header_len <= mpcb->ucopy.len) {
+				if (mpcb->ucopy.task == current &&
+				    sock_owned_by_user(sk)) {
+					/*We have not yet finished to adapt
+					  the code for the fast path, thus
+					  we put this BUG to ensure that it
+					  is not accidentally taken.
+					  When we implement this, we must
+					  ensure that this iovec copy is
+					  replaced with a call to 
+					  mtcp_queue_skb, and that the skb is
+					  only destroyed when that function
+					  returns MTCP_EATEN (here, the 
+					  skb is always destroyed if eaten
+					  is 1, which is no longer
+					  correct with MTCP)*/
+					if (tp->mpc) BUG();
+					__set_current_state(TASK_RUNNING);
+
+					if (!tcp_copy_to_iovec(sk, skb, 
+							       tcp_header_len))
+						eaten = 1;
+				}
+				if (eaten) {
+					/* Predicted packet is in window by 
+					 * definition.seq == rcv_nxt and 
+					 * rcv_wup <= rcv_nxt.
+					 * Hence, check seq<=rcv_wup reduces to:
+					 */
+					if (tcp_header_len ==
+					    (sizeof(struct tcphdr) +
+					     TCPOLEN_TSTAMP_ALIGNED) &&
+					    tp->rcv_nxt == tp->rcv_wup)
+						tcp_store_ts_recent(tp);
+
+					tcp_rcv_rtt_measure_ts(sk, skb);
+
+					__skb_pull(skb, tcp_header_len);
+					tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
+					NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPHPHITSTOUSER);
+				}
+			}
+			if (!eaten) {
+				if (tcp_checksum_complete_user(sk, skb))
+					goto csum_error;
+				
+				/* Predicted packet is in window by definition.
+				 * seq == rcv_nxt and rcv_wup <= rcv_nxt.
+				 * Hence, check seq<=rcv_wup reduces to:
+				 */
+				if (tcp_header_len ==
+				    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&
+				    tp->rcv_nxt == tp->rcv_wup)
+					tcp_store_ts_recent(tp);
+
+				tcp_rcv_rtt_measure_ts(sk, skb);
+
+				if ((int)skb->truesize > sk->sk_forward_alloc)
+					goto step5;
+
+				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPHPHITS);
+
+				/* Bulk data transfer: receiver */
+				__skb_pull(skb, tcp_header_len);
+				mtcp_eaten=mtcp_queue_skb(sk,skb);
+				skb_set_owner_r(skb, sk);
+				tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
+			}
+			
+			tcp_event_data_recv(sk, skb);
+
+			if (TCP_SKB_CB(skb)->ack_seq != tp->snd_una) {
+				/* Well, only one small jumplet in fast path... */
+				tcp_ack(sk, skb, FLAG_DATA);
+				tcp_data_snd_check(sk);
+				if (!inet_csk_ack_scheduled(sk))
+					goto no_ack;
+			}
+
+			if (tp->rcv_nxt != tp->rcv_wup)
+				__tcp_ack_snd_check(sk, 0);
+no_ack:
+			if (eaten || mtcp_eaten)
+				__kfree_skb(skb);
+			else {
+				mtcp_debug("Will wake the master sk up");
+				mpcb->master_sk->sk_data_ready(sk, 0);
+			}
+			return 0;
+		}
+	}
+	
+slow_path:
+	if (len < (th->doff << 2) || tcp_checksum_complete_user(sk, skb))
+		goto csum_error;
+
+	/*
+	 *	Standard slow path.
+	 */
+
+	res = tcp_validate_incoming(sk, skb, th, 1);
+	if (res <= 0)
+		return -res;
+
+
+step5:
+	if (th->ack)
+		tcp_ack(sk, skb, FLAG_SLOWPATH);
+
+	tcp_rcv_rtt_measure_ts(sk, skb);
+
+	/* Process urgent data. */
+	tcp_urg(sk, skb, th);
+
+	/* step 7: process the segment text */
+	tcp_data_queue(sk, skb);
+
+	if (tp->mpcb)
+		tcp_data_snd_check(sk);
+	BUG_ON(!tp->mpcb && !tp->pending);
+	tcp_ack_snd_check(sk);
+	return 0;
+
+csum_error:
+	TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);
+
+discard:
+	__kfree_skb(skb);
+	return 0;
+}
+#else
 int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 			struct tcphdr *th, unsigned len)
 {
@@ -5008,6 +5928,7 @@ discard:
 	__kfree_skb(skb);
 	return 0;
 }
+#endif /*CONFIG_MTCP*/
 
 static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 					 struct tcphdr *th, unsigned len)
@@ -5015,9 +5936,25 @@ static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	int saved_clamp = tp->rx_opt.mss_clamp;
+	struct multipath_pcb* mpcb;
+
+	mpcb = mpcb_from_tcpsock(tp);
+	if (mpcb==NULL){
+		printk(KERN_ERR "MPCB null in synsent state process\n");
+		BUG();
+	}
 
-	tcp_parse_options(skb, &tp->rx_opt, 0);
+	tcp_parse_options(skb, &tp->rx_opt,&mpcb->received_options, 0);
 
+	if (unlikely(mpcb && tp->rx_opt.saw_mpc && is_master_sk(tp))) {
+		/*Transfer sndwnd control to the mpcb*/
+		mpcb->tp.snd_wnd=tp->snd_wnd;
+		mpcb->tp.max_window=tp->max_window;
+		/*We can do multipath with that socket*/
+		tp->mpc=1;
+		tp->rx_opt.saw_mpc=0; /*reset that field, it has been read*/
+	}
+	
 	if (th->ack) {
 		/* rfc793:
 		 * "If the state is SYN-SENT then
@@ -5072,24 +6009,39 @@ static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 
 		TCP_ECN_rcv_synack(tp, th);
 
-		tp->snd_wl1 = TCP_SKB_CB(skb)->seq;
+		if (tp->mpc && tp->mpcb)
+			tp->mpcb->tp.snd_wl1 = TCP_SKB_CB(skb)->data_seq;
+		else
+			tp->snd_wl1=TCP_SKB_CB(skb)->seq;
 		tcp_ack(sk, skb, FLAG_SLOWPATH);
 
 		/* Ok.. it's good. Set up sequence numbers and
 		 * move to established.
 		 */
+#ifdef CONFIG_MTCP
+		tp->rx_opt.rcv_isn = TCP_SKB_CB(skb)->seq;
+#endif
 		tp->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;
 		tp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;
 
 		/* RFC1323: The window in SYN & SYN/ACK segments is
 		 * never scaled.
 		 */
-		tp->snd_wnd = ntohs(th->window);
-		tcp_init_wl(tp, TCP_SKB_CB(skb)->ack_seq, TCP_SKB_CB(skb)->seq);
+		if (tp->mpc && tp->mpcb) {
+			tp->mpcb->tp.snd_wnd = ntohs(th->window);
+			tcp_init_wl(&tp->mpcb->tp, tp->mpcb->tp.rcv_nxt);
+		}
+		else {
+			tp->snd_wnd = ntohs(th->window);
+			tcp_init_wl(tp, TCP_SKB_CB(skb)->seq);
+		}
 
 		if (!tp->rx_opt.wscale_ok) {
 			tp->rx_opt.snd_wscale = tp->rx_opt.rcv_wscale = 0;
 			tp->window_clamp = min(tp->window_clamp, 65535U);
+#ifdef CONFIG_MTCP
+			mtcp_update_window_clamp(tp->mpcb);
+#endif
 		}
 
 		if (tp->rx_opt.saw_tstamp) {
@@ -5114,6 +6066,7 @@ static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 		 * is initialized. */
 		tp->copied_seq = tp->rcv_nxt;
 		smp_mb();
+
 		tcp_set_state(sk, TCP_ESTABLISHED);
 
 		security_inet_conn_established(sk, skb);
@@ -5136,7 +6089,9 @@ static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 			inet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp));
 
 		if (!tp->rx_opt.snd_wscale)
-			__tcp_fast_path_on(tp, tp->snd_wnd);
+			__tcp_fast_path_on(tp, (tp->mpc && tp->mpcb)?
+					   tp->mpcb->tp.snd_wnd:
+					   tp->snd_wnd);
 		else
 			tp->pred_flags = 0;
 
@@ -5211,9 +6166,16 @@ discard:
 		/* RFC1323: The window in SYN & SYN/ACK segments is
 		 * never scaled.
 		 */
-		tp->snd_wnd    = ntohs(th->window);
-		tp->snd_wl1    = TCP_SKB_CB(skb)->seq;
-		tp->max_window = tp->snd_wnd;
+		if (tp->mpc && tp->mpcb) {
+			tp->mpcb->tp.snd_wl1    = TCP_SKB_CB(skb)->data_seq;
+			tp->mpcb->tp.snd_wnd    = ntohs(th->window);
+			tp->mpcb->tp.max_window = tp->snd_wnd;
+		}
+		else {
+			tp->snd_wl1    = TCP_SKB_CB(skb)->seq;
+			tp->snd_wnd    = ntohs(th->window);
+			tp->max_window = tp->snd_wnd;
+		}
 
 		TCP_ECN_rcv_syn(tp, th);
 
@@ -5258,7 +6220,6 @@ reset_and_undo:
  *	It's called from both tcp_v4_rcv and tcp_v6_rcv and should be
  *	address independent.
  */
-
 int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 			  struct tcphdr *th, unsigned len)
 {
@@ -5344,10 +6305,18 @@ int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 						      SOCK_WAKE_IO, POLL_OUT);
 
 				tp->snd_una = TCP_SKB_CB(skb)->ack_seq;
-				tp->snd_wnd = ntohs(th->window) <<
-					      tp->rx_opt.snd_wscale;
-				tcp_init_wl(tp, TCP_SKB_CB(skb)->ack_seq,
-					    TCP_SKB_CB(skb)->seq);
+				if (tp->mpc && tp->mpcb) {
+					tp->mpcb->tp.snd_wnd = 
+						ntohs(th->window) <<
+						tp->rx_opt.snd_wscale;
+					tcp_init_wl(&tp->mpcb->tp, 
+						    tp->mpcb->tp.rcv_nxt);
+				}
+				else {
+					tp->snd_wnd = ntohs(th->window) <<
+						tp->rx_opt.snd_wscale;
+					tcp_init_wl(tp, TCP_SKB_CB(skb)->seq);
+				}
 
 				/* tcp_ack considers this ACK as duplicate
 				 * and does not calculate rtt.
@@ -5384,7 +6353,7 @@ int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 			break;
 
 		case TCP_FIN_WAIT1:
-			if (tp->snd_una == tp->write_seq) {
+			if (tp->snd_una == tp->write_seq) {			
 				tcp_set_state(sk, TCP_FIN_WAIT2);
 				sk->sk_shutdown |= SEND_SHUTDOWN;
 				dst_confirm(sk->sk_dst_cache);
@@ -5394,28 +6363,40 @@ int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 					sk->sk_state_change(sk);
 				else {
 					int tmo;
-
+					
 					if (tp->linger2 < 0 ||
-					    (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&
-					     after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt))) {
+					    (TCP_SKB_CB(skb)->end_seq != 
+					     TCP_SKB_CB(skb)->seq &&
+					     after(TCP_SKB_CB(skb)->end_seq - 
+						   th->fin, tp->rcv_nxt))) {
 						tcp_done(sk);
-						NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
+						NET_INC_STATS_BH(sock_net(sk), 
+								 LINUX_MIB_TCPABORTONDATA);
 						return 1;
 					}
 
 					tmo = tcp_fin_time(sk);
 					if (tmo > TCP_TIMEWAIT_LEN) {
-						inet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);
-					} else if (th->fin || sock_owned_by_user(sk)) {
-						/* Bad case. We could lose such FIN otherwise.
-						 * It is not a big problem, but it looks confusing
-						 * and not so rare event. We still can lose it now,
-						 * if it spins in bh_lock_sock(), but it is really
-						 * marginal case.
+						inet_csk_reset_keepalive_timer(
+							sk, tmo - 
+							TCP_TIMEWAIT_LEN);
+					} else if (th->fin || 
+						   sock_owned_by_user(sk)) {
+						/* Bad case. We could lose 
+						 * such FIN otherwise.
+						 * It is not a big problem, 
+						 * but it looks confusing
+						 * and not so rare event. We 
+						 * still can lose it now,
+						 * if it spins in 
+						 * bh_lock_sock(), but it is 
+						 * really marginal case.
 						 */
-						inet_csk_reset_keepalive_timer(sk, tmo);
+						inet_csk_reset_keepalive_timer(
+							sk, tmo);
 					} else {
-						tcp_time_wait(sk, TCP_FIN_WAIT2, tmo);
+						tcp_time_wait(sk, TCP_FIN_WAIT2,
+							      tmo);
 						goto discard;
 					}
 				}
@@ -5471,8 +6452,21 @@ int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 		break;
 	}
 
+	/*MPTCP note: here we cannot enter the if body if mpcb is not defined,
+	  because the following functions need the mpcb. Anyway, if the mpcb
+	  is NULL, it means that the socket is not yet attached to the 
+	  mpcb, and thus no data can be given to it.
+	  Note that it can happen that the state is already TCP_CLOSE, e.g. if
+	  we are here due to a FIN reception, that FIN triggered a change to
+	  TIMEWAIT state in the context of this function, then the timewait 
+	  timer expires before we reach this point in this same function,
+	  making the socket go to TCP_CLOSE even before we leave. That's very
+	  fast (or this fct is slow), but I have seen it already.*/
+
+	BUG_ON(sk->sk_state != TCP_CLOSE &&
+	       tp->mpc && !tp->mpcb && !tp->pending);
 	/* tcp_data could move socket to TIME-WAIT */
-	if (sk->sk_state != TCP_CLOSE) {
+	if (sk->sk_state != TCP_CLOSE && (!tp->mpc || tp->mpcb)) {
 		tcp_data_snd_check(sk);
 		tcp_ack_snd_check(sk);
 	}
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index 5c8fa7f..ecfcd28 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -81,7 +81,7 @@
 #include <linux/scatterlist.h>
 
 int sysctl_tcp_tw_reuse __read_mostly;
-int sysctl_tcp_low_latency __read_mostly;
+int sysctl_tcp_low_latency __read_mostly=1;
 
 
 #ifdef CONFIG_TCP_MD5SIG
@@ -247,6 +247,10 @@ int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
 							   inet->sport,
 							   usin->sin_port);
 
+#ifdef CONFIG_MTCP
+	tp->snt_isn=tp->write_seq;
+#endif
+
 	inet->id = tp->write_seq ^ jiffies;
 
 	err = tcp_connect(sk);
@@ -254,6 +258,9 @@ int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
 	if (err)
 		goto failure;
 
+#ifdef CONFIG_MTCP
+	mtcp_update_metasocket(sk);
+#endif
 	return 0;
 
 failure:
@@ -605,10 +612,10 @@ static void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb)
    outside socket context is ugly, certainly. What can I do?
  */
 
-static void tcp_v4_send_ack(struct sk_buff *skb, u32 seq, u32 ack,
-			    u32 win, u32 ts, int oif,
-			    struct tcp_md5sig_key *key,
-			    int reply_flags)
+void tcp_v4_send_ack(struct sk_buff *skb, u32 seq, u32 ack,
+		     u32 win, u32 ts, int oif,
+		     struct tcp_md5sig_key *key,
+		     int reply_flags)
 {
 	struct tcphdr *th = tcp_hdr(skb);
 	struct {
@@ -769,8 +776,8 @@ static void syn_flood_warning(struct sk_buff *skb)
 /*
  * Save and compile IPv4 options into the request_sock if needed.
  */
-static struct ip_options *tcp_v4_save_options(struct sock *sk,
-					      struct sk_buff *skb)
+struct ip_options *tcp_v4_save_options(struct sock *sk,
+				       struct sk_buff *skb)
 {
 	struct ip_options *opt = &(IPCB(skb)->opt);
 	struct ip_options *dopt = NULL;
@@ -1167,7 +1174,7 @@ static struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {
 };
 #endif
 
-static struct timewait_sock_ops tcp_timewait_sock_ops = {
+struct timewait_sock_ops tcp_timewait_sock_ops = {
 	.twsk_obj_size	= sizeof(struct tcp_timewait_sock),
 	.twsk_unique	= tcp_twsk_unique,
 	.twsk_destructor= tcp_twsk_destructor,
@@ -1225,7 +1232,7 @@ int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
 	tmp_opt.mss_clamp = 536;
 	tmp_opt.user_mss  = tcp_sk(sk)->rx_opt.user_mss;
 
-	tcp_parse_options(skb, &tmp_opt, 0);
+	tcp_parse_options(skb, &tmp_opt, NULL, 0);
 
 	if (want_cookie && !tmp_opt.saw_tstamp)
 		tcp_clear_options(&tmp_opt);
@@ -1241,6 +1248,12 @@ int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
 	}
 	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
 
+#ifdef CONFIG_MTCP_PM
+	/*Must be set to NULL before calling openreq init.
+	  tcp_openreq_init() uses this to know whether the request
+	  is join request or a conn request.*/
+	req->mpcb=NULL;
+#endif
 	tcp_openreq_init(req, &tmp_opt, skb);
 
 	if (security_inet_conn_request(sk, skb, req))
@@ -1417,9 +1430,10 @@ static struct sock *tcp_v4_hnd_req(struct sock *sk, struct sk_buff *skb)
 						       iph->saddr, iph->daddr);
 	if (req)
 		return tcp_check_req(sk, skb, req, prev);
-
+	
 	nsk = inet_lookup_established(sock_net(sk), &tcp_hashinfo, iph->saddr,
-			th->source, iph->daddr, th->dest, inet_iif(skb));
+				      th->source, iph->daddr, th->dest, 
+				      inet_iif(skb));
 
 	if (nsk) {
 		if (nsk->sk_state != TCP_TIME_WAIT) {
@@ -1495,10 +1509,13 @@ int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
 		goto csum_err;
 
 	if (sk->sk_state == TCP_LISTEN) {
-		struct sock *nsk = tcp_v4_hnd_req(sk, skb);
-		if (!nsk)
+		struct sock *nsk;
+		nsk= tcp_v4_hnd_req(sk, skb);
+		if (!nsk) {
 			goto discard;
-
+		}
+		
+		BUG_ON(skb->len>3000); /*Try to force the GPF*/
 		if (nsk != sk) {
 			if (tcp_child_process(sk, nsk, skb)) {
 				rsk = nsk;
@@ -1540,12 +1557,13 @@ int tcp_v4_rcv(struct sk_buff *skb)
 {
 	const struct iphdr *iph;
 	struct tcphdr *th;
-	struct sock *sk;
+	struct sock *sk, *mpcb_sk=NULL;
 	int ret;
 	struct net *net = dev_net(skb->dev);
+	struct multipath_pcb *mpcb=NULL;
 
 	if (skb->pkt_type != PACKET_HOST)
-		goto discard_it;
+		goto discard_it;	
 
 	/* Count it even if it's bad */
 	TCP_INC_STATS_BH(net, TCP_MIB_INSEGS);
@@ -1573,30 +1591,69 @@ int tcp_v4_rcv(struct sk_buff *skb)
 	TCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +
 				    skb->len - th->doff * 4);
 	TCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);
+#ifdef CONFIG_MTCP
+	/*Init to zero, will be set upon option parsing.*/
+	TCP_SKB_CB(skb)->data_seq = 0;
+	TCP_SKB_CB(skb)->end_data_seq = 0;
+#endif
 	TCP_SKB_CB(skb)->when	 = 0;
 	TCP_SKB_CB(skb)->flags	 = iph->tos;
 	TCP_SKB_CB(skb)->sacked	 = 0;
+	
+#ifdef CONFIG_MTCP_PM
+	/*We must absolutely check for subflow related segments
+	  before the normal sock lookup, because otherwise subflow
+	  segments could be understood as associated to some listening
+	  socket.*/	
+
+	/*Is there a pending request sock for this segment ?*/
+	if (mtcp_syn_recv_sock(skb)) return 0;
+	/*Is this a new syn+join ?*/
+	if (th->syn && mtcp_lookup_join(skb)) return 0;
+
+	/*OK, this segment is not related to subflow initiation,
+	  we can proceed to normal lookup*/
+#endif
 
-	sk = __inet_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest);
+	sk = __inet_lookup_skb(&tcp_hashinfo, skb, th->source, 
+			       th->dest);
 	if (!sk)
 		goto no_tcp_socket;
-
+	
 process:
 	if (sk->sk_state == TCP_TIME_WAIT)
 		goto do_time_wait;
-
+	
 	if (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))
 		goto discard_and_relse;
 	nf_reset(skb);
-
+	
 	if (sk_filter(sk, skb))
 		goto discard_and_relse;
-
+	
 	skb->dev = NULL;
+	
+	if (tcp_sk(sk)->mpcb) {
+		mpcb=tcp_sk(sk)->mpcb;
+		kref_get(&mpcb->kref);
+		mpcb_sk=(struct sock*)(tcp_sk(sk)->mpcb);
+	}
 
 	bh_lock_sock_nested(sk);
+	if (mpcb_sk)
+		bh_lock_sock(mpcb_sk);
 	ret = 0;
-	if (!sock_owned_by_user(sk)) {
+
+	if (mpcb_sk) {
+		if (!sock_owned_by_user(mpcb_sk) &&
+		    !sock_owned_by_user(sk)) {
+			if (!tcp_prequeue(sk, skb))
+				ret = tcp_v4_do_rcv(sk, skb);
+		}
+		else 
+			sk_add_backlog(sk, skb);
+	}
+	else if (!sock_owned_by_user(sk)) {
 #ifdef CONFIG_NET_DMA
 		struct tcp_sock *tp = tcp_sk(sk);
 		if (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)
@@ -1607,13 +1664,16 @@ process:
 #endif
 		{
 			if (!tcp_prequeue(sk, skb))
-			ret = tcp_v4_do_rcv(sk, skb);
+				ret = tcp_v4_do_rcv(sk, skb);
 		}
 	} else
 		sk_add_backlog(sk, skb);
-	bh_unlock_sock(sk);
 
+	if (mpcb_sk)
+		bh_unlock_sock(mpcb_sk);
+	bh_unlock_sock(sk);
 	sock_put(sk);
+	if (mpcb) kref_put(&mpcb->kref,mpcb_release);
 
 	return ret;
 
@@ -1804,6 +1864,16 @@ static int tcp_v4_init_sock(struct sock *sk)
 	sk->sk_sndbuf = sysctl_tcp_wmem[1];
 	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
 
+#ifdef CONFIG_MTCP
+	/*Init the MPTCP mpcb*/
+	{
+		struct multipath_pcb *mpcb;		
+		mpcb=mtcp_alloc_mpcb(sk);
+		tp->path_index=0;
+		mtcp_add_sock(mpcb,tp);
+	}
+#endif
+
 	atomic_inc(&tcp_sockets_allocated);
 
 	return 0;
@@ -2353,7 +2423,11 @@ void tcp4_proc_exit(void)
 struct proto tcp_prot = {
 	.name			= "TCP",
 	.owner			= THIS_MODULE,
+#ifdef CONFIG_MTCP
+	.close			= mtcp_close,
+#else
 	.close			= tcp_close,
+#endif
 	.connect		= tcp_v4_connect,
 	.disconnect		= tcp_disconnect,
 	.accept			= inet_csk_accept,
diff --git a/net/ipv4/tcp_minisocks.c b/net/ipv4/tcp_minisocks.c
index 779f2e9..836b294 100644
--- a/net/ipv4/tcp_minisocks.c
+++ b/net/ipv4/tcp_minisocks.c
@@ -98,11 +98,12 @@ tcp_timewait_state_process(struct inet_timewait_sock *tw, struct sk_buff *skb,
 {
 	struct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);
 	struct tcp_options_received tmp_opt;
+	struct multipath_options mopt;
 	int paws_reject = 0;
 
 	tmp_opt.saw_tstamp = 0;
 	if (th->doff > (sizeof(*th) >> 2) && tcptw->tw_ts_recent_stamp) {
-		tcp_parse_options(skb, &tmp_opt, 0);
+		tcp_parse_options(skb, &tmp_opt, &mopt, 0);
 
 		if (tmp_opt.saw_tstamp) {
 			tmp_opt.ts_recent	= tcptw->tw_ts_recent;
@@ -396,10 +397,17 @@ struct sock *tcp_create_openreq_child(struct sock *sk, struct request_sock *req,
 		newtp->rcv_wup = newtp->copied_seq = newtp->rcv_nxt = treq->rcv_isn + 1;
 		newtp->snd_sml = newtp->snd_una = newtp->snd_nxt = treq->snt_isn + 1;
 		newtp->snd_up = treq->snt_isn + 1;
+#ifdef CONFIG_MTCP
+		newtp->rx_opt.rcv_isn=treq->rcv_isn;
+		newtp->snt_isn=treq->snt_isn;
+		newtp->rcv_isn=treq->rcv_isn;
+		memset(&newtp->rcvq_space,0,sizeof(newtp->rcvq_space));
+		newtp->mopt.list_rcvd=0;
+#endif
 
 		tcp_prequeue_init(newtp);
 
-		tcp_init_wl(newtp, treq->snt_isn, treq->rcv_isn);
+		tcp_init_wl(newtp, treq->rcv_isn);
 
 		newtp->srtt = 0;
 		newtp->mdev = TCP_TIMEOUT_INIT;
@@ -499,11 +507,14 @@ struct sock *tcp_check_req(struct sock *sk,struct sk_buff *skb,
 	__be32 flg = tcp_flag_word(th) & (TCP_FLAG_RST|TCP_FLAG_SYN|TCP_FLAG_ACK);
 	int paws_reject = 0;
 	struct tcp_options_received tmp_opt;
+	struct multipath_options mtp;
 	struct sock *child;
 
 	tmp_opt.saw_tstamp = 0;
+	mtcp_init_addr_list(&mtp);
+	
 	if (th->doff > (sizeof(struct tcphdr)>>2)) {
-		tcp_parse_options(skb, &tmp_opt, 0);
+		tcp_parse_options(skb, &tmp_opt, &mtp, 0);
 
 		if (tmp_opt.saw_tstamp) {
 			tmp_opt.ts_recent = req->ts_recent;
@@ -654,7 +665,9 @@ struct sock *tcp_check_req(struct sock *sk,struct sk_buff *skb,
 	 * ESTABLISHED STATE. If it will be dropped after
 	 * socket is created, wait for troubles.
 	 */
+	BUG_ON(skb->len>3000); /*Try to force the GPF*/
 	child = inet_csk(sk)->icsk_af_ops->syn_recv_sock(sk, skb, req, NULL);
+	BUG_ON(skb->len>3000); /*Try to force the GPF*/
 	if (child == NULL)
 		goto listen_overflow;
 #ifdef CONFIG_TCP_MD5SIG
@@ -681,19 +694,42 @@ struct sock *tcp_check_req(struct sock *sk,struct sk_buff *skb,
 	}
 #endif
 
+#ifdef CONFIG_MTCP
+	{
+		/*Copy mptcp related info from req to child
+		  we do this here because this is shared between
+		  ipv4 and ipv6*/
+		struct tcp_sock *child_tp = tcp_sk(child);
+		child_tp->rx_opt.saw_mpc=req->saw_mpc;
+		if (child_tp->rx_opt.saw_mpc)
+			child_tp->mpc=1;
+#ifdef CONFIG_MTCP_PM
+		child_tp->rx_opt.mtcp_rem_token=req->mtcp_rem_token;
+		child_tp->mtcp_loc_token=req->mtcp_loc_token;
+		child_tp->mpcb=NULL;
+		child_tp->pending=1;
+		if (mtp.list_rcvd)
+			memcpy(&child_tp->mopt,&mtp,sizeof(mtp));
+#endif
+	}
+#endif
+	
 	inet_csk_reqsk_queue_unlink(sk, req, prev);
 	inet_csk_reqsk_queue_removed(sk, req);
-
+	
 	inet_csk_reqsk_queue_add(sk, req, child);
+	BUG_ON(skb->len>3000); /*Try to force the GPF*/
 	return child;
-
+	
 listen_overflow:
+	BUG_ON(skb->len>3000); /*Try to force the GPF*/
 	if (!sysctl_tcp_abort_on_overflow) {
 		inet_rsk(req)->acked = 1;
 		return NULL;
 	}
-
+	
 embryonic_reset:
+	BUG_ON(skb->len>3000); /*Try to force the GPF*/
 	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_EMBRYONICRSTS);
 	if (!(flg & TCP_FLAG_RST))
 		req->rsk_ops->send_reset(sk, skb);
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index fe3b4bd..14688d6 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -38,6 +38,7 @@
 
 #include <linux/compiler.h>
 #include <linux/module.h>
+#include <linux/tcp_probe.h>
 
 /* People can turn this off for buggy TCP's found in printers etc. */
 int sysctl_tcp_retrans_collapse __read_mostly = 1;
@@ -59,22 +60,48 @@ int sysctl_tcp_base_mss __read_mostly = 512;
 /* By default, RFC2861 behavior.  */
 int sysctl_tcp_slow_start_after_idle __read_mostly = 1;
 
+/*TODEL*/
+int tocheck=0;
+struct sk_buff *check_skb;
+struct sock *check_sk;
+
 static void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	unsigned int prior_packets = tp->packets_out;
+	int meta_sk=is_meta_tp(tp);
+
+	if (tocheck) {
+		BUG_ON(check_skb==skb);
+		BUG_ON(tcp_send_head(check_sk)!=check_skb);
+	}
 
+	check_send_head(sk,2);
+	BUG_ON(tcp_send_head(sk)!=skb);
+	check_pkts_out(sk);
 	tcp_advance_send_head(sk, skb);
-	tp->snd_nxt = TCP_SKB_CB(skb)->end_seq;
+	check_send_head(sk,3);
+	if (tocheck)
+		BUG_ON(tcp_send_head(check_sk)!=check_skb);
+	tp->snd_nxt = meta_sk?TCP_SKB_CB(skb)->end_data_seq:
+		TCP_SKB_CB(skb)->end_seq;
 
-	/* Don't override Nagle indefinately with F-RTO */
+	/* Don't override Nagle indefinitely with F-RTO */
 	if (tp->frto_counter == 2)
 		tp->frto_counter = 3;
 
 	tp->packets_out += tcp_skb_pcount(skb);
-	if (!prior_packets)
+	if (!prior_packets && !meta_sk) {
+		tcpprobe_logmsg(sk,"setting RTO to %d ms",
+				inet_csk(sk)->icsk_rto*1000/HZ);
 		inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
 					  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);
+	}
+	if (tocheck)
+		BUG_ON(tcp_send_head(check_sk)!=check_skb);
+	
+	check_pkts_out(sk);
+	check_send_head(sk,5);
 }
 
 /* SND.NXT, if window was not shrunk.
@@ -87,10 +114,20 @@ static inline __u32 tcp_acceptable_seq(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	if (!before(tcp_wnd_end(tp), tp->snd_nxt))
+	/*We do not call tcp_wnd_end(..,1) here, 
+	  because even when MPTCP is used, 
+	  we exceptionnaly want here to consider the send window as related to
+	  the seqnums, not the dataseqs. The reason is that we have no dataseq
+	  nums in non-data segments (this function is only called for the
+	  construction of non-data segments, e.g. acks), and the dataseq is now
+	  the only field that can be checked by the receiver. The seqnum we
+	  choose here ensure that we are accepted as well by middleboxes
+	  that are not aware of MPTCP stuff.*/
+	
+	if (!before(tcp_wnd_end(tp,0), tp->snd_nxt))
 		return tp->snd_nxt;
 	else
-		return tcp_wnd_end(tp);
+		return tcp_wnd_end(tp,0);
 }
 
 /* Calculate mss to advertise in SYN segment.
@@ -116,6 +153,9 @@ static __u16 tcp_advertise_mss(struct sock *sk)
 	if (dst && dst_metric(dst, RTAX_ADVMSS) < mss) {
 		mss = dst_metric(dst, RTAX_ADVMSS);
 		tp->advmss = mss;
+#ifdef CONFIG_MTCP
+		tp->mss_too_low=1;
+#endif
 	}
 
 	return (__u16)mss;
@@ -244,6 +284,8 @@ static u16 tcp_select_window(struct sock *sk)
 	u32 cur_win = tcp_receive_window(tp);
 	u32 new_win = __tcp_select_window(sk);
 
+	BUG_ON(is_meta_sk(sk));
+
 	/* Never shrink the offered window */
 	if (new_win < cur_win) {
 		/* Danger Will Robinson!
@@ -255,8 +297,19 @@ static u16 tcp_select_window(struct sock *sk)
 		 */
 		new_win = ALIGN(cur_win, 1 << tp->rx_opt.rcv_wscale);
 	}
-	tp->rcv_wnd = new_win;
-	tp->rcv_wup = tp->rcv_nxt;
+	if (tp->mpcb && tp->mpc) {
+		struct tcp_sock *mpcb_tp=(struct tcp_sock*)(tp->mpcb);
+		mpcb_tp->rcv_wnd = new_win;
+		mpcb_tp->rcv_wup = mpcb_tp->rcv_nxt;
+		/*the subsock rcv_wup must still be updated,
+		  because it is used to decide when to echo the timestamp
+		  and when to delay the acks*/
+		tp->rcv_wup=tp->rcv_nxt;
+	}
+	else {
+		tp->rcv_wnd = new_win;
+		tp->rcv_wup = tp->rcv_nxt;
+	}
 
 	/* Make sure we do not exceed the maximum possible
 	 * scaled window.
@@ -273,6 +326,7 @@ static u16 tcp_select_window(struct sock *sk)
 	if (new_win == 0)
 		tp->pred_flags = 0;
 
+	sk->sk_debug=0;
 	return new_win;
 }
 
@@ -328,7 +382,7 @@ static inline void TCP_ECN_send(struct sock *sk, struct sk_buff *skb,
 /* Constructs common control bits of non-data skb. If SYN/FIN is present,
  * auto increment end seqno.
  */
-static void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)
+void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)
 {
 	skb->csum = 0;
 
@@ -350,17 +404,6 @@ static inline int tcp_urg_mode(const struct tcp_sock *tp)
 	return tp->snd_una != tp->snd_up;
 }
 
-#define OPTION_SACK_ADVERTISE	(1 << 0)
-#define OPTION_TS		(1 << 1)
-#define OPTION_MD5		(1 << 2)
-
-struct tcp_out_options {
-	u8 options;		/* bit field of OPTION_* */
-	u8 ws;			/* window scale, 0 to disable */
-	u8 num_sack_blocks;	/* number of SACK blocks to include */
-	u16 mss;		/* 0 to disable */
-	__u32 tsval, tsecr;	/* need to include OPTION_TS */
-};
 
 /* Beware: Something in the Internet is very sensitive to the ordering of
  * TCP options, we learned this through the hard way, so be careful here.
@@ -373,9 +416,9 @@ struct tcp_out_options {
  * At least SACK_PERM as the first option is known to lead to a disaster
  * (but it may well be that other scenarios fail similarly).
  */
-static void tcp_options_write(__be32 *ptr, struct tcp_sock *tp,
-			      const struct tcp_out_options *opts,
-			      __u8 **md5_hash) {
+void tcp_options_write(__be32 *ptr, struct tcp_sock *tp,
+		       const struct tcp_out_options *opts,
+		       __u8 **md5_hash) {
 	if (unlikely(OPTION_MD5 & opts->options)) {
 		*ptr++ = htonl((TCPOPT_NOP << 24) |
 			       (TCPOPT_NOP << 16) |
@@ -446,11 +489,73 @@ static void tcp_options_write(__be32 *ptr, struct tcp_sock *tp,
 			tp->rx_opt.eff_sacks = tp->rx_opt.num_sacks;
 		}
 	}
+#ifdef CONFIG_MTCP
+	if (unlikely(OPTION_MPC & opts->options)) {
+#ifdef CONFIG_MTCP_PM
+		*ptr++ = htonl((TCPOPT_NOP  << 24) |
+			       (TCPOPT_MPC << 16) |
+			       (TCPOLEN_MPC << 8));
+		*ptr++ = htonl(opts->token);
+#else
+		*ptr++ = htonl((TCPOPT_MPC << 24) |
+			       (TCPOLEN_MPC << 16));
+#endif
+	}
+
+#ifdef CONFIG_MTCP_PM
+	if (unlikely((OPTION_ADDR & opts->options) && opts->num_addr4)) {
+		uint8_t *ptr8=(uint8_t*)ptr; /*We need a per-byte pointer here*/
+		int i;
+		for (i=TCPOLEN_ADDR(opts->num_addr4);
+		     i<TCPOLEN_ADDR_ALIGNED(opts->num_addr4);i++)
+			*ptr8++ = TCPOPT_NOP;
+		*ptr8++ = TCPOPT_ADDR;
+		*ptr8++ = TCPOLEN_ADDR(opts->num_addr4);
+		for (i=0;i<opts->num_addr4;i++) {
+			*ptr8++ = opts->addr4[i].id;
+			*ptr8++ = 64;
+			*((__be32*)ptr8)=opts->addr4[i].addr.s_addr;
+			ptr8+=sizeof(struct in_addr);
+		}
+		ptr = (__be32*)ptr8;
+	}
+
+	if (unlikely(OPTION_JOIN & opts->options)) {
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_JOIN << 16) |
+			       (TCPOLEN_JOIN << 8) |
+			       (opts->token >> 24));
+		*ptr++ = htonl((opts->token<<8) |
+			       opts->addr_id);
+	}
+#endif
+	if (OPTION_DSN & opts->options) {
+		*ptr++ = htonl((TCPOPT_DSN << 24) |
+			       (TCPOLEN_DSN << 16) |
+			       opts->data_len);
+		*ptr++ = htonl(opts->sub_seq);
+		*ptr++ = htonl(opts->data_seq);
+	}
+	if (OPTION_DATA_ACK & opts->options) {
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_NOP << 16) |
+			       (TCPOPT_DATA_ACK << 8) |
+			       (TCPOLEN_DATA_ACK));
+		*ptr++ = htonl(opts->data_ack);
+	}
+	if (OPTION_DFIN & opts->options) {
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_NOP << 16) |
+			       (TCPOPT_DFIN << 8) |
+			       (TCPOLEN_DFIN));
+	}
+#endif
 }
 
 static unsigned tcp_syn_options(struct sock *sk, struct sk_buff *skb,
 				struct tcp_out_options *opts,
-				struct tcp_md5sig_key **md5) {
+				struct tcp_md5sig_key **md5) 
+{
 	struct tcp_sock *tp = tcp_sk(sk);
 	unsigned size = 0;
 
@@ -492,6 +597,36 @@ static unsigned tcp_syn_options(struct sock *sk, struct sk_buff *skb,
 		if (unlikely(!(OPTION_TS & opts->options)))
 			size += TCPOLEN_SACKPERM_ALIGNED;
 	}
+#ifdef CONFIG_MTCP	
+	if (is_master_sk(tp)) {
+		struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+
+		opts->options |= OPTION_MPC;
+		size+=TCPOLEN_MPC_ALIGNED;
+#ifdef CONFIG_MTCP_PM
+		opts->token=tp->mtcp_loc_token;
+#endif
+		
+		/*We arrive here either when sending a SYN or a
+		  SYN+ACK when in SYN_SENT state (that is, tcp_synack_options
+		  is only called for syn+ack replied by a server, while this
+		  function is called when SYNs are sent by both parties and 
+		  are crossed)
+		  Due to this possibility, a slave subsocket may arrive here,
+		  and does not need to set the dataseq options, since
+		  there is no data in the segment*/
+		BUG_ON(!mpcb);
+	}
+#ifdef CONFIG_MTCP_PM
+	else {
+		struct multipath_pcb *mpcb=mpcb_from_tcpsock(tp);
+		opts->options |= OPTION_JOIN;
+		size+=TCPOLEN_JOIN_ALIGNED;
+		opts->token=tp->rx_opt.mtcp_rem_token;
+		opts->addr_id=mtcp_get_loc_addrid(mpcb, tp->path_index);
+	}
+#endif
+#endif
 
 	return size;
 }
@@ -500,7 +635,8 @@ static unsigned tcp_synack_options(struct sock *sk,
 				   struct request_sock *req,
 				   unsigned mss, struct sk_buff *skb,
 				   struct tcp_out_options *opts,
-				   struct tcp_md5sig_key **md5) {
+				   struct tcp_md5sig_key **md5)
+{
 	unsigned size = 0;
 	struct inet_request_sock *ireq = inet_rsk(req);
 	char doing_ts;
@@ -541,15 +677,37 @@ static unsigned tcp_synack_options(struct sock *sk,
 			size += TCPOLEN_SACKPERM_ALIGNED;
 	}
 
+
+#ifdef CONFIG_MTCP
+/*For the SYNACK, the mpcb is normally not yet initialized
+  (to protect against SYN DoS attack)
+  So we cannot use it here.*/
+	
+	opts->options |= OPTION_MPC;
+	size+=TCPOLEN_MPC_ALIGNED;
+#ifdef CONFIG_MTCP_PM
+	opts->token = req->mtcp_loc_token;
+#endif
+	opts->options |= OPTION_DSN;
+	size+=TCPOLEN_DSN_ALIGNED;
+	opts->data_seq=0;
+#endif
 	return size;
 }
 
+
+/*if skb is NULL, then we are evaluating the MSS, thus, we take into account
+ * ALL potential options. */
 static unsigned tcp_established_options(struct sock *sk, struct sk_buff *skb,
 					struct tcp_out_options *opts,
 					struct tcp_md5sig_key **md5) {
 	struct tcp_skb_cb *tcb = skb ? TCP_SKB_CB(skb) : NULL;
 	struct tcp_sock *tp = tcp_sk(sk);
 	unsigned size = 0;
+#ifdef CONFIG_MTCP
+	struct multipath_pcb *mpcb;
+	int release_mpcb=0;
+#endif
 
 #ifdef CONFIG_TCP_MD5SIG
 	*md5 = tp->af_specific->md5_lookup(sk, sk);
@@ -568,16 +726,102 @@ static unsigned tcp_established_options(struct sock *sk, struct sk_buff *skb,
 		size += TCPOLEN_TSTAMP_ALIGNED;
 	}
 
+#ifdef CONFIG_MTCP
+	mpcb = tp->mpcb;
+	if (tp->pending && !is_master_sk(tp) && tp->mpc) {
+		mpcb=mtcp_hash_find(tp->mtcp_loc_token);
+		if (!mpcb) {
+			printk(KERN_ERR "mpcb not found, token %#x,"
+			       "master_sk:%d,pending:%d," NIPQUAD_FMT 
+			       "->" NIPQUAD_FMT "\n",
+			       tp->mtcp_loc_token,is_master_sk(tp), 
+			       tp->pending, NIPQUAD(inet_sk(sk)->saddr),
+			       NIPQUAD(inet_sk(sk)->daddr));
+			BUG();
+		}
+		else release_mpcb=1;
+	}
+
+	if (tp->mpc && (!skb || skb->len!=0 ||  
+			(tcb->flags & TCPCB_FLAG_FIN))) {
+		if (tcb && tcb->data_len) { /*Ignore dataseq if data_len is 0*/
+			opts->data_seq=tcb->data_seq;
+			opts->data_len=tcb->data_len;
+			opts->sub_seq=tcb->sub_seq-tp->snt_isn;
+		}
+		opts->options |= OPTION_DSN;
+		size += TCPOLEN_DSN_ALIGNED;		
+	}
+	/*we can have mpc==1 and mpcb==NULL if tp is the master_sk
+	  and is established but not yet accepted.*/
+	if (tp->mpc && mpcb && test_bit(MPCB_FLAG_FIN_ENQUEUED,
+				&mpcb->flags) &&
+	    (!skb || TCP_SKB_CB(skb)->end_data_seq==mpcb->tp.write_seq)) {
+		opts->options |= OPTION_DFIN;
+		size += TCPOLEN_DFIN_ALIGNED;		
+	}
+	if (tp->mpc) {
+		/*If we are at the server side, and the accept syscall has not
+		  yet been called, the received data is still enqueued in the 
+		  subsock receive queue, but we must still
+		  send a data ack. The value of the ack is based on the 
+		  subflow ack since at this step there is necessarily only 
+		  one subflow.*/
+		u32 rcv_nxt=(tp->pending && is_master_sk(tp))?
+			tp->rcv_nxt-tp->rcv_isn-1:
+			mpcb->tp.rcv_nxt;		
+		opts->data_ack=rcv_nxt;
+		opts->options |= OPTION_DATA_ACK;
+		size += TCPOLEN_DATA_ACK_ALIGNED;
+	}
+#ifdef CONFIG_MTCP_PM
+	if (tp->mpc && mpcb) {
+		if (unlikely(mpcb->addr_unsent)) {
+			const unsigned remaining = MAX_TCP_OPTION_SPACE - size;
+			if (remaining<TCPOLEN_ADDR_BASE)
+				opts->num_addr4=0;
+			else
+				opts->num_addr4=min_t(unsigned, 
+						      mpcb->addr_unsent,
+						      (remaining-
+						       TCPOLEN_ADDR_BASE) /
+						      TCPOLEN_ADDR_PERBLOCK);
+			/*If no space to send the option, just wait next
+			  segment*/
+			if (opts->num_addr4) {
+				opts->options |= OPTION_ADDR;
+				opts->addr4=mpcb->addr4+mpcb->num_addr4-
+					mpcb->addr_unsent;
+				if (skb) mpcb->addr_unsent-=opts->num_addr4;
+				size += TCPOLEN_ADDR_ALIGNED(opts->num_addr4);
+			}
+		}
+	}
+	BUG_ON(!mpcb && !tp->pending);
+#endif
+	if (release_mpcb)
+		mpcb_put(mpcb);
+#endif
+
 	if (unlikely(tp->rx_opt.eff_sacks)) {
 		const unsigned remaining = MAX_TCP_OPTION_SPACE - size;
-		opts->num_sack_blocks =
-			min_t(unsigned, tp->rx_opt.eff_sacks,
-			      (remaining - TCPOLEN_SACK_BASE_ALIGNED) /
-			      TCPOLEN_SACK_PERBLOCK);
-		size += TCPOLEN_SACK_BASE_ALIGNED +
-			opts->num_sack_blocks * TCPOLEN_SACK_PERBLOCK;
+		if (remaining<TCPOLEN_SACK_BASE_ALIGNED)
+			opts->num_sack_blocks=0;
+		else
+			opts->num_sack_blocks =
+				min_t(unsigned, tp->rx_opt.eff_sacks,
+				      (remaining - TCPOLEN_SACK_BASE_ALIGNED) /
+				      TCPOLEN_SACK_PERBLOCK);
+		if (opts->num_sack_blocks)
+			size += TCPOLEN_SACK_BASE_ALIGNED +
+				opts->num_sack_blocks * TCPOLEN_SACK_PERBLOCK;
 	}
 
+	if (size>MAX_TCP_OPTION_SPACE) {
+		printk(KERN_ERR "exceeded option space, options:%#x\n",
+		       opts->options);
+		BUG();
+	}
 	return size;
 }
 
@@ -606,7 +850,16 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 	struct tcphdr *th;
 	int err;
 
-	BUG_ON(!skb || !tcp_skb_pcount(skb));
+	BUG_ON(is_meta_sk(sk));
+	check_pkts_out(sk);
+
+	if(!skb || !tcp_skb_pcount(skb)) {
+		printk(KERN_ERR "tcp_skb_pcount:%d,skb->len:%d\n",
+		       tcp_skb_pcount(skb),skb->len);
+		BUG();
+	}
+
+	tcpprobe_transmit_skb(sk,skb,clone_it,gfp_mask);
 
 	/* If congestion control is doing timestamping, we must
 	 * take such a timestamp before we potentially clone/copy.
@@ -619,8 +872,10 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 			skb = pskb_copy(skb, gfp_mask);
 		else
 			skb = skb_clone(skb, gfp_mask);
-		if (unlikely(!skb))
+		if (unlikely(!skb)) {
+			printk(KERN_ERR "transmit_skb, clone failed\n");
 			return -ENOBUFS;
+		}
 	}
 
 	inet = inet_sk(sk);
@@ -628,6 +883,9 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 	tcb = TCP_SKB_CB(skb);
 	memset(&opts, 0, sizeof(opts));
 
+	if (tp->mpc)
+		skb->count_dsn=1;
+	
 	if (unlikely(tcb->flags & TCPCB_FLAG_SYN))
 		tcp_options_size = tcp_syn_options(sk, skb, &opts, &md5);
 	else
@@ -656,9 +914,8 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 		 * is never scaled.
 		 */
 		th->window	= htons(min(tp->rcv_wnd, 65535U));
-	} else {
+	} else
 		th->window	= htons(tcp_select_window(sk));
-	}
 	th->check		= 0;
 	th->urg_ptr		= 0;
 
@@ -668,7 +925,7 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 		th->urg_ptr		= htons(tp->snd_up - tcb->seq);
 		th->urg			= 1;
 	}
-
+	
 	tcp_options_write((__be32 *)(th + 1), tp, &opts, &md5_hash_location);
 	if (likely((tcb->flags & TCPCB_FLAG_SYN) == 0))
 		TCP_ECN_send(sk, skb, tcp_header_size);
@@ -693,9 +950,17 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 	if (after(tcb->end_seq, tp->snd_nxt) || tcb->seq == tcb->end_seq)
 		TCP_INC_STATS(sock_net(sk), TCP_MIB_OUTSEGS);
 
+	skb->path_index=tp->path_index;
+	
 	err = icsk->icsk_af_ops->queue_xmit(skb, 0);
-	if (likely(err <= 0))
+
+	check_pkts_out(sk);
+
+	if (likely(err <= 0)) {
+		if (err<0) 
+			mtcp_debug("%s:error %d\n",__FUNCTION__,err);
 		return err;
+	}
 
 	tcp_enter_cwr(sk, 1);
 
@@ -707,12 +972,15 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
  * NOTE: probe0 timer is not checked, do not forget tcp_push_pending_frames,
  * otherwise socket can stall.
  */
-static void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)
+void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	/* Advance write_seq and place onto the write_queue. */
-	tp->write_seq = TCP_SKB_CB(skb)->end_seq;
+	if (is_meta_sk(sk))
+		tp->write_seq = TCP_SKB_CB(skb)->end_data_seq;
+	else
+		tp->write_seq = TCP_SKB_CB(skb)->end_seq;
 	skb_header_release(skb);
 	tcp_add_write_queue_tail(sk, skb);
 	sk->sk_wmem_queued += skb->truesize;
@@ -792,6 +1060,14 @@ int tcp_fragment(struct sock *sk, struct sk_buff *skb, u32 len,
 	TCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;
 	TCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;
 	TCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;
+#ifdef CONFIG_MTCP
+	TCP_SKB_CB(buff)->data_seq=TCP_SKB_CB(skb)->data_seq + len;
+	TCP_SKB_CB(buff)->end_data_seq = TCP_SKB_CB(skb)->end_data_seq;
+	TCP_SKB_CB(buff)->sub_seq = TCP_SKB_CB(skb)->sub_seq + len;
+	TCP_SKB_CB(buff)->data_len=TCP_SKB_CB(skb)->data_len - len;
+	TCP_SKB_CB(skb)->data_len=len;
+	TCP_SKB_CB(skb)->end_data_seq = TCP_SKB_CB(buff)->data_seq;
+#endif
 
 	/* PSH and FIN should only be set in the second packet. */
 	flags = TCP_SKB_CB(skb)->flags;
@@ -902,6 +1178,12 @@ int tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)
 		__pskb_trim_head(skb, len - skb_headlen(skb));
 
 	TCP_SKB_CB(skb)->seq += len;
+#ifdef CONFIG_MTCP
+	TCP_SKB_CB(skb)->data_seq += len;
+	TCP_SKB_CB(skb)->sub_seq += len;
+	TCP_SKB_CB(skb)->data_len -= len;
+#endif
+
 	skb->ip_summed = CHECKSUM_PARTIAL;
 
 	skb->truesize	     -= len;
@@ -1040,6 +1322,9 @@ unsigned int tcp_current_mss(struct sock *sk, int large_allowed)
 	struct tcp_out_options opts;
 	struct tcp_md5sig_key *md5;
 
+	/*if sk is the meta-socket, return the common MSS*/
+	if (is_meta_tp(tp)) return sysctl_mptcp_mss;
+
 	mss_now = tp->mss_cache;
 
 	if (large_allowed && sk_can_gso(sk))
@@ -1050,7 +1335,7 @@ unsigned int tcp_current_mss(struct sock *sk, int large_allowed)
 		if (mtu != inet_csk(sk)->icsk_pmtu_cookie)
 			mss_now = tcp_sync_mss(sk, mtu);
 	}
-
+	memset(&opts, 0, sizeof(opts));
 	header_len = tcp_established_options(sk, NULL, &opts, &md5) +
 		     sizeof(struct tcphdr);
 	/* The mss_cache is sized based on tp->tcp_header_len, which assumes
@@ -1084,7 +1369,7 @@ static void tcp_cwnd_validate(struct sock *sk)
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	if (tp->packets_out >= tp->snd_cwnd) {
-		/* Network is feed fully. */
+		/* Network is fed fully. */
 		tp->snd_cwnd_used = 0;
 		tp->snd_cwnd_stamp = tcp_time_stamp;
 	} else {
@@ -1098,45 +1383,17 @@ static void tcp_cwnd_validate(struct sock *sk)
 	}
 }
 
-/* Returns the portion of skb which can be sent right away without
- * introducing MSS oddities to segment boundaries. In rare cases where
- * mss_now != mss_cache, we will request caller to create a small skb
- * per input skb which could be mostly avoided here (if desired).
- *
- * We explicitly want to create a request for splitting write queue tail
- * to a small skb for Nagle purposes while avoiding unnecessary modulos,
- * thus all the complexity (cwnd_len is always MSS multiple which we
- * return whenever allowed by the other factors). Basically we need the
- * modulo only when the receiver window alone is the limiting factor or
- * when we would be allowed to send the split-due-to-Nagle skb fully.
- */
-static unsigned int tcp_mss_split_point(struct sock *sk, struct sk_buff *skb,
-					unsigned int mss_now, unsigned int cwnd)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	u32 needed, window, cwnd_len;
-
-	window = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;
-	cwnd_len = mss_now * cwnd;
-
-	if (likely(cwnd_len <= window && skb != tcp_write_queue_tail(sk)))
-		return cwnd_len;
-
-	needed = min(skb->len, window);
-
-	if (cwnd_len <= needed)
-		return cwnd_len;
-
-	return needed - needed % mss_now;
-}
-
 /* Can at least one segment of SKB be sent right now, according to the
  * congestion window rules?  If so, return how many segments are allowed.
  */
 static inline unsigned int tcp_cwnd_test(struct tcp_sock *tp,
-					 struct sk_buff *skb)
+				  struct sk_buff *skb)
 {
 	u32 in_flight, cwnd;
+	struct sock *sk=(struct sock*)tp;
+	struct inet_connection_sock *icsk=inet_csk(sk);
+
+	BUG_ON(is_meta_tp(tp));
 
 	/* Don't be strict about the congestion window for the final FIN.  */
 	if ((TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN) &&
@@ -1144,10 +1401,13 @@ static inline unsigned int tcp_cwnd_test(struct tcp_sock *tp,
 		return 1;
 
 	in_flight = tcp_packets_in_flight(tp);
+	if (icsk->icsk_ca_state==TCP_CA_Loss)
+		tcpprobe_logmsg(sk,"tp %d: in_flight is %d",tp->path_index,
+				in_flight);
 	cwnd = tp->snd_cwnd;
 	if (in_flight < cwnd)
 		return (cwnd - in_flight);
-
+	
 	return 0;
 }
 
@@ -1220,31 +1480,49 @@ static inline int tcp_nagle_test(struct tcp_sock *tp, struct sk_buff *skb,
 static inline int tcp_snd_wnd_test(struct tcp_sock *tp, struct sk_buff *skb,
 				   unsigned int cur_mss)
 {
-	u32 end_seq = TCP_SKB_CB(skb)->end_seq;
-
+	u32 end_seq = (tp->mpc)?TCP_SKB_CB(skb)->end_data_seq:
+		TCP_SKB_CB(skb)->end_seq;
+	
 	if (skb->len > cur_mss)
-		end_seq = TCP_SKB_CB(skb)->seq + cur_mss;
-
-	return !after(end_seq, tcp_wnd_end(tp));
+		end_seq = ((tp->mpc)?TCP_SKB_CB(skb)->data_seq:
+			   TCP_SKB_CB(skb)->seq) + cur_mss;
+	if (after(end_seq, tcp_wnd_end(tp,tp->mpc)) && 
+	    (TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN)) {
+		mtcp_debug("FIN refused for sndwnd, fin end dsn %#x,"
+			   "tcp_wnd_end: %#x, mpc:%d,mpcb:%p,snd_una:%#x,"
+			   "snd_wnd:%d, mpcb write_seq:%#x, "
+			   "mpcb queue len:%d\n",
+			   end_seq,tcp_wnd_end(tp,tp->mpc),
+			   tp->mpc,tp->mpcb,tp->mpcb->tp.snd_una,
+			   tp->mpcb->tp.snd_wnd,tp->mpcb->tp.write_seq,
+			   ((struct sock*)&tp->mpcb->tp)->sk_write_queue.qlen);
+	}	
+	
+	return !after(end_seq, tcp_wnd_end(tp,tp->mpc));
 }
 
 /* This checks if the data bearing packet SKB (usually tcp_send_head(sk))
  * should be put on the wire right now.  If so, it returns the number of
  * packets allowed by the congestion window.
  */
-static unsigned int tcp_snd_test(struct sock *sk, struct sk_buff *skb,
+static unsigned int tcp_snd_test(struct sock *subsk, struct sk_buff *skb,
 				 unsigned int cur_mss, int nonagle)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp_sock *subtp = tcp_sk(subsk);
 	unsigned int cwnd_quota;
-
-	tcp_init_tso_segs(sk, skb, cur_mss);
-
-	if (!tcp_nagle_test(tp, skb, cur_mss, nonagle))
+	struct multipath_pcb *mpcb=subtp->mpcb;
+	struct tcp_sock *mpcb_tp=&mpcb->tp;
+	
+	BUG_ON(tcp_skb_pcount(skb)>1);
+	if (!mpcb)
+		mpcb_tp=subtp;
+	
+	if (!tcp_nagle_test(mpcb_tp, skb, cur_mss, nonagle))
 		return 0;
 
-	cwnd_quota = tcp_cwnd_test(tp, skb);
-	if (cwnd_quota && !tcp_snd_wnd_test(tp, skb, cur_mss))
+	cwnd_quota = tcp_cwnd_test(subtp, skb);
+
+	if (cwnd_quota && !tcp_snd_wnd_test(subtp, skb, cur_mss))
 		cwnd_quota = 0;
 
 	return cwnd_quota;
@@ -1254,9 +1532,15 @@ int tcp_may_send_now(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *skb = tcp_send_head(sk);
+	int mss;
+
+	if (tp->mpc)
+		mss=sysctl_mptcp_mss;
+	else
+		mss=tcp_current_mss(sk, 1);
 
 	return (skb &&
-		tcp_snd_test(sk, skb, tcp_current_mss(sk, 1),
+		tcp_snd_test(sk, skb, mss,
 			     (tcp_skb_is_last(sk, skb) ?
 			      tp->nonagle : TCP_NAGLE_PUSH)));
 }
@@ -1275,6 +1559,10 @@ static int tso_fragment(struct sock *sk, struct sk_buff *skb, unsigned int len,
 	int nlen = skb->len - len;
 	u16 flags;
 
+	mtcp_debug("Entering %s\n",__FUNCTION__);
+
+	BUG_ON(len==0); /*This would create an empty segment*/
+
 	/* All of a TSO frame must be composed of paged data.  */
 	if (skb->len != skb->data_len)
 		return tcp_fragment(sk, skb, len, mss_now);
@@ -1292,6 +1580,14 @@ static int tso_fragment(struct sock *sk, struct sk_buff *skb, unsigned int len,
 	TCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;
 	TCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;
 	TCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;
+#ifdef CONFIG_MTCP
+	TCP_SKB_CB(buff)->data_seq=TCP_SKB_CB(skb)->data_seq + len;
+	TCP_SKB_CB(buff)->end_data_seq = TCP_SKB_CB(skb)->end_data_seq;
+	TCP_SKB_CB(buff)->sub_seq = TCP_SKB_CB(skb)->sub_seq + len;
+	TCP_SKB_CB(buff)->data_len=TCP_SKB_CB(skb)->data_len - len;
+	TCP_SKB_CB(skb)->data_len=len;
+	TCP_SKB_CB(skb)->end_data_seq = TCP_SKB_CB(buff)->data_seq;
+#endif
 
 	/* PSH and FIN should only be set in the second packet. */
 	flags = TCP_SKB_CB(skb)->flags;
@@ -1315,72 +1611,6 @@ static int tso_fragment(struct sock *sk, struct sk_buff *skb, unsigned int len,
 	return 0;
 }
 
-/* Try to defer sending, if possible, in order to minimize the amount
- * of TSO splitting we do.  View it as a kind of TSO Nagle test.
- *
- * This algorithm is from John Heffner.
- */
-static int tcp_tso_should_defer(struct sock *sk, struct sk_buff *skb)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	const struct inet_connection_sock *icsk = inet_csk(sk);
-	u32 send_win, cong_win, limit, in_flight;
-
-	if (TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN)
-		goto send_now;
-
-	if (icsk->icsk_ca_state != TCP_CA_Open)
-		goto send_now;
-
-	/* Defer for less than two clock ticks. */
-	if (tp->tso_deferred &&
-	    ((jiffies << 1) >> 1) - (tp->tso_deferred >> 1) > 1)
-		goto send_now;
-
-	in_flight = tcp_packets_in_flight(tp);
-
-	BUG_ON(tcp_skb_pcount(skb) <= 1 || (tp->snd_cwnd <= in_flight));
-
-	send_win = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;
-
-	/* From in_flight test above, we know that cwnd > in_flight.  */
-	cong_win = (tp->snd_cwnd - in_flight) * tp->mss_cache;
-
-	limit = min(send_win, cong_win);
-
-	/* If a full-sized TSO skb can be sent, do it. */
-	if (limit >= sk->sk_gso_max_size)
-		goto send_now;
-
-	if (sysctl_tcp_tso_win_divisor) {
-		u32 chunk = min(tp->snd_wnd, tp->snd_cwnd * tp->mss_cache);
-
-		/* If at least some fraction of a window is available,
-		 * just use it.
-		 */
-		chunk /= sysctl_tcp_tso_win_divisor;
-		if (limit >= chunk)
-			goto send_now;
-	} else {
-		/* Different approach, try not to defer past a single
-		 * ACK.  Receiver should ACK every other full sized
-		 * frame, so if we have space for more than 3 frames
-		 * then send now.
-		 */
-		if (limit > tcp_max_burst(tp) * tp->mss_cache)
-			goto send_now;
-	}
-
-	/* Ok, it looks like it is advisable to defer.  */
-	tp->tso_deferred = 1 | (jiffies << 1);
-
-	return 1;
-
-send_now:
-	tp->tso_deferred = 0;
-	return 0;
-}
-
 /* Create a new MTU probe if we are ready.
  * Returns 0 if we should wait to probe (no cwnd available),
  *         1 if a probe was sent,
@@ -1396,6 +1626,7 @@ static int tcp_mtu_probe(struct sock *sk)
 	int size_needed;
 	int copy;
 	int mss_now;
+	u32 snd_wnd=(tp->mpc)?tp->mpcb->tp.snd_wnd:tp->snd_wnd;
 
 	/* Not currently probing/verifying,
 	 * not in recovery,
@@ -1421,9 +1652,9 @@ static int tcp_mtu_probe(struct sock *sk)
 	if (tp->write_seq - tp->snd_nxt < size_needed)
 		return -1;
 
-	if (tp->snd_wnd < size_needed)
+	if (snd_wnd < size_needed)
 		return -1;
-	if (after(tp->snd_nxt + size_needed, tcp_wnd_end(tp)))
+	if (after(tp->snd_nxt + size_needed, tcp_wnd_end(tp,0)))
 		return 0;
 
 	/* Do we need to wait to drain cwnd? With none in flight, don't stall */
@@ -1480,6 +1711,9 @@ static int tcp_mtu_probe(struct sock *sk)
 				tcp_set_skb_tso_segs(sk, skb, mss_now);
 			}
 			TCP_SKB_CB(skb)->seq += copy;
+#ifdef CONFIG_MTCP
+			TCP_SKB_CB(skb)->data_seq += copy;
+#endif
 		}
 
 		len += copy;
@@ -1508,6 +1742,8 @@ static int tcp_mtu_probe(struct sock *sk)
 	return -1;
 }
 
+extern int tcp_close_state(struct sock *sk);
+
 /* This routine writes packets to the network.  It advances the
  * send_head.  This happens as incoming acks open up the remote
  * window for us.
@@ -1522,77 +1758,241 @@ static int tcp_mtu_probe(struct sock *sk)
 static int tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+	struct sock *mpcb_sk=(struct sock*)tp->mpcb;
 	struct sk_buff *skb;
 	unsigned int tso_segs, sent_pkts;
 	int cwnd_quota;
+	int reinject;
 	int result;
-
+	
+	if (sk->sk_in_write_xmit) {
+		printk(KERN_ERR "sk in write xmit, meta_sk:%d\n",
+		       is_meta_sk(sk));
+		BUG();
+	}
+	/*We can be recursively called only in TCP_FIN_WAIT1 state (because
+	  the very last segment calls tcp_send_fin() on all subflows)*/
+	if(tp->mpcb && mpcb_sk->sk_in_write_xmit
+	   && ((1<<mpcb_sk->sk_state) & ~(TCPF_FIN_WAIT1|TCPF_LAST_ACK))) {
+		printk(KERN_ERR "meta-sk in write xmit, meta-sk:%d,"
+		       "state of mpcb_sk:%d, of subsk:%d\n",
+		       is_meta_sk(sk),((struct sock*)tp->mpcb)->sk_state,
+		       sk->sk_state);
+		BUG();
+	}
+
+	sk->sk_in_write_xmit=1;
+	
+	if (tp->mpc) {
+		if (mss_now!=sysctl_mptcp_mss) {
+			printk(KERN_ERR "write xmit-mss_now %d, mptcp mss:%d\n",
+			       mss_now,sysctl_mptcp_mss);
+			BUG();
+		}
+	}
+	
 	/* If we are closed, the bytes will have to remain here.
 	 * In time closedown will finish, we empty the write queue and all
 	 * will be happy.
 	 */
-	if (unlikely(sk->sk_state == TCP_CLOSE))
+	if (unlikely(sk->sk_state == TCP_CLOSE)) {
+		sk->sk_in_write_xmit=0;
 		return 0;
+	}
 
 	sent_pkts = 0;
 
-	/* Do MTU probing. */
-	if ((result = tcp_mtu_probe(sk)) == 0) {
+	/* Do MTU probing. */	
+	if ((result=tcp_mtu_probe(sk)) == 0) {
+		sk->sk_in_write_xmit=0;
+		tcpprobe_logmsg(sk,"mtu forces us out of write_xmit");
 		return 0;
-	} else if (result > 0) {
+	}
+	else if (result > 0) {
 		sent_pkts = 1;
 	}
 
-	while ((skb = tcp_send_head(sk))) {
+	while ((skb=mtcp_next_segment(sk,&reinject))) {
 		unsigned int limit;
+		int err;
+		struct sock *subsk;
+		struct tcp_sock *subtp;
+		struct sk_buff *subskb;
+
+		if (reinject && !after(TCP_SKB_CB(skb)->end_data_seq,
+				       tp->snd_una)) {
+			/*another copy of the segment already reached
+			  the peer, just discard this one.*/
+			skb_unlink(skb,&tp->mpcb->reinject_queue);
+			kfree_skb(skb);
+			continue;
+		}
+		
+		if (is_meta_tp(tp)) {
+			int pf=0;
+			subsk=get_available_subflow(tp->mpcb,skb,&pf);
+			if (!subsk)
+				break;
+			subtp=tcp_sk(subsk);
+		}
+		else {
+			subsk=sk; subtp=tp;
+		}
 
-		tso_segs = tcp_init_tso_segs(sk, skb, mss_now);
-		BUG_ON(!tso_segs);
+		/*Since all subsocks are locked before calling the scheduler,
+		  the tcp_send_head should not change.*/
+		BUG_ON(!reinject && tcp_send_head(sk)!=skb);
 
-		cwnd_quota = tcp_cwnd_test(tp, skb);
-		if (!cwnd_quota)
+		/*This must be invoked even if we don't want
+		  to support TSO at the moment*/
+		tso_segs=tcp_init_tso_segs(sk,skb,mss_now);
+		BUG_ON(!tso_segs);
+		/*At the moment we do not support tso, hence 
+		  tso_segs must be 1*/
+		BUG_ON(tp->mpc && tso_segs!=1);
+
+		/*decide to which subsocket we give the skb*/
+		
+		cwnd_quota = tcp_cwnd_test(subtp, skb);
+		if (!cwnd_quota) {
+			/*Should not happen, since mptcp must have
+			  chosen a subsock with open cwnd*/
+			if (sk!=subsk) BUG();
+			if (reinject) printk(KERN_ERR "reinj: line %d\n", 
+					     __LINE__);
 			break;
+		}
 
-		if (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now)))
+		if (unlikely(!tcp_snd_wnd_test(subtp, skb, mss_now))) {
+			if (reinject) printk(KERN_ERR "reinj: line %d\n", 
+					     __LINE__);
 			break;
-
-		if (tso_segs == 1) {
-			if (unlikely(!tcp_nagle_test(tp, skb, mss_now,
-						     (tcp_skb_is_last(sk, skb) ?
-						      nonagle : TCP_NAGLE_PUSH))))
-				break;
-		} else {
-			if (tcp_tso_should_defer(sk, skb))
-				break;
 		}
-
+		
+		if (unlikely(!tcp_nagle_test(tp, skb, mss_now,
+					     (tcp_skb_is_last(sk, skb) ?
+					      nonagle : 
+					      TCP_NAGLE_PUSH)))) {
+			if (reinject) printk(KERN_ERR "reinj: line %d\n", 
+					     __LINE__);
+			break;
+		}
+		
 		limit = mss_now;
-		if (tso_segs > 1 && !tcp_urg_mode(tp))
-			limit = tcp_mss_split_point(sk, skb, mss_now,
-						    cwnd_quota);
 
 		if (skb->len > limit &&
-		    unlikely(tso_fragment(sk, skb, limit, mss_now)))
+		    unlikely(tso_fragment(sk, skb, limit, mss_now))) {
+			if (reinject) printk(KERN_ERR "reinj: line %d\n", __LINE__);
 			break;
+		}
 
-		TCP_SKB_CB(skb)->when = tcp_time_stamp;
-
-		if (unlikely(tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC)))
+		if (sk!=subsk) {
+			if (tp->path_index) 
+				skb->path_mask|=PI_TO_FLAG(tp->path_index);
+			/*If the segment is reinjected, the clone is done 
+			  already*/
+			if (!reinject)
+				subskb=skb_clone(skb,GFP_ATOMIC);
+			else {
+				skb_unlink(skb,&tp->mpcb->reinject_queue);
+				subskb=skb;
+			}
+			if (!subskb) {
+				if (reinject) printk(KERN_ERR "reinj: line %d\n", __LINE__);
+				break;
+			}
+			BUG_ON(tcp_send_head(subsk));
+			mtcp_skb_entail(subsk, subskb);
+			if (reinject) {
+				tcpprobe_logmsg(sk,"reinj:seq is %#x",
+						TCP_SKB_CB(subskb)->seq);
+			}
+		}
+		else
+			subskb=skb;
+		
+
+		TCP_SKB_CB(subskb)->when = tcp_time_stamp;
+		if (unlikely(err=tcp_transmit_skb(subsk, subskb, 1, 
+						  GFP_ATOMIC))) {
+ 			if (sk!=subsk) {
+				/*Remove the skb from the subsock*/
+				tcp_advance_send_head(subsk,subskb);
+				tcp_unlink_write_queue(subskb,subsk);
+				subtp->write_seq-=subskb->len;
+				mtcp_wmem_free_skb(subsk, subskb);
+				/*If we entered CWR, just try to give
+				  that same skb to another subflow,
+				  by querying again the scheduler,
+				  we need however to ensure that the
+				  same subflow is not selected again by
+				  the scheduler, to avoid looping*/
+				if (err>0 && tp->mpcb->cnt_subflows>1) {
+					tp->mpcb->noneligible|=
+						PI_TO_FLAG(subtp->path_index);
+					continue;
+				}
+			}
 			break;
-
+		}	
+		
 		/* Advance the send_head.  This one is sent out.
 		 * This call will increment packets_out.
 		 */
-		tcp_event_new_data_sent(sk, skb);
+		if(!reinject && tcp_send_head(sk)!=skb) {
+			printk(KERN_ERR "sock_owned_by_user:%d\n",
+			       sock_owned_by_user(sk));
+			BUG();
+			       
+		}
+		if (sk!=subsk && !reinject)
+			tocheck=1;
+		check_skb=skb;
+		check_sk=sk;
+		tcp_event_new_data_sent(subsk, subskb);
+ 		if (sk!=subsk) BUG_ON(tcp_send_head(subsk));
+		tocheck=0;
+		if (sk!=subsk && !reinject) {
+			BUG_ON(tcp_send_head(sk)!=skb);
+			tcp_event_new_data_sent(sk,skb);
+		}
+		
+		if (sk!=subsk &&
+		    (TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN)) {
+			struct sock *sk_it, *sk_tmp;
+			BUG_ON(!tcp_close_state(subsk));
+			/*App close: we have sent every app-level byte,
+			  send now the FIN on all subflows.
+			  if the FIN was triggered by mtcp_close(),
+			  then the SHUTDOWN_MASK is set and we call
+			  tcp_close() on all subsocks. Otherwise
+			  only sk_shutdown has been called, and 
+			  we just send the fin on all subflows.*/
+			mtcp_for_each_sk_safe(tp->mpcb,sk_it,sk_tmp) {
+				if (sk->sk_shutdown == SHUTDOWN_MASK)
+					tcp_close(sk_it,-1);
+				else if (sk_it!=subsk && 
+					 tcp_close_state(sk_it)) {
+					tcp_send_fin(sk_it);
+				}
+			}
+		}
+		
 
 		tcp_minshall_update(tp, mss_now, skb);
 		sent_pkts++;
+
+		tcp_cwnd_validate(subsk);
 	}
 
+	if (tp->mpcb) tp->mpcb->noneligible=0;
+
 	if (likely(sent_pkts)) {
-		tcp_cwnd_validate(sk);
+		sk->sk_in_write_xmit=0;
 		return 0;
 	}
+
+	sk->sk_in_write_xmit=0;
 	return !tp->packets_out && tcp_send_head(sk);
 }
 
@@ -1603,12 +2003,22 @@ static int tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle)
 void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 			       int nonagle)
 {
-	struct sk_buff *skb = tcp_send_head(sk);
+	struct sk_buff *skb = mtcp_next_segment(sk,NULL);
 
 	if (skb) {
-		if (tcp_write_xmit(sk, cur_mss, nonagle))
-			tcp_check_probe_timer(sk);
+		if (tcp_write_xmit(sk, cur_mss, nonagle)) {
+			if (!is_meta_sk(sk))
+				tcp_check_probe_timer(sk);
+			else {
+				struct sock *sk_it;
+				struct tcp_sock *tp_it;
+				mtcp_for_each_sk(tcp_sk(sk)->mpcb,sk_it,
+						 tp_it)
+					tcp_check_probe_timer(sk_it);
+			}
+		}
 	}
+	else 	tcpprobe_logmsg(sk,"not running write_xmit");
 }
 
 /* Send _single_ skb sitting at the send head. This function requires
@@ -1617,37 +2027,126 @@ void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 void tcp_push_one(struct sock *sk, unsigned int mss_now)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
-	struct sk_buff *skb = tcp_send_head(sk);
+	int reinject;
+	struct sk_buff *skb;
 	unsigned int tso_segs, cwnd_quota;
+	struct sock *subsk;
+	struct tcp_sock *subtp;
+	int err;
+
+again:
+	skb=mtcp_next_segment(sk,&reinject);
+	BUG_ON(!skb);
+
+	while (reinject && !after(TCP_SKB_CB(skb)->end_data_seq,
+			       tp->snd_una)) {
+		/*another copy of the segment already reached
+		  the peer, just discard this one.*/
+		skb_unlink(skb,&tp->mpcb->reinject_queue);
+		kfree_skb(skb);
+		skb=mtcp_next_segment(sk,&reinject);
+	}
+	
+	BUG_ON(!skb);
 
-	BUG_ON(!skb || skb->len < mss_now);
+	if (is_meta_tp(tp)) {
+		subsk=get_available_subflow(tp->mpcb,skb,NULL);
+		subtp=tcp_sk(subsk);
+		if (!subsk)
+			goto out;
+		subsk->sk_debug=4;		
+	}
+	else 
+		subsk=sk; subtp=tp;
+	
+	BUG_ON(!reinject && tcp_send_head(sk)!=skb);
+
+	if (skb->len<mss_now) {
+		printk(KERN_ERR "skb->len:%d,mss_now:%d\n",skb->len,
+		       mss_now);
+		BUG();
+	}
+	
+	tso_segs = tcp_init_tso_segs(sk,skb,mss_now);
 
-	tso_segs = tcp_init_tso_segs(sk, skb, mss_now);
-	cwnd_quota = tcp_snd_test(sk, skb, mss_now, TCP_NAGLE_PUSH);
+	cwnd_quota = tcp_snd_test(subsk, skb, mss_now, TCP_NAGLE_PUSH);
 
 	if (likely(cwnd_quota)) {
 		unsigned int limit;
+		struct sk_buff *subskb;
 
 		BUG_ON(!tso_segs);
+		/*At the moment we do not support tso, hence 
+		  tso_segs must be 1*/
+		BUG_ON(tp->mpc && tso_segs!=1);
 
 		limit = mss_now;
-		if (tso_segs > 1 && !tcp_urg_mode(tp))
-			limit = tcp_mss_split_point(sk, skb, mss_now,
-						    cwnd_quota);
+
+		BUG_ON(tp->mpc && skb->len>limit);
 
 		if (skb->len > limit &&
-		    unlikely(tso_fragment(sk, skb, limit, mss_now)))
-			return;
+		    unlikely(tso_fragment(sk, skb, limit, mss_now))) {
+			mtcp_debug("NOT SENDING TCP SEGMENT\n");
+			goto out;
+		}
 
 		/* Send it out now. */
-		TCP_SKB_CB(skb)->when = tcp_time_stamp;
 
-		if (likely(!tcp_transmit_skb(sk, skb, 1, sk->sk_allocation))) {
-			tcp_event_new_data_sent(sk, skb);
-			tcp_cwnd_validate(sk);
-			return;
+		if (sk!=subsk) {
+			if (tp->path_index)
+				skb->path_mask|=PI_TO_FLAG(tp->path_index);
+			if (!reinject) {
+				subskb=skb_clone(skb,GFP_KERNEL);
+			}
+			else {
+				skb_unlink(skb,&tp->mpcb->reinject_queue);
+				subskb=skb;
+			}
+			if (!subskb) {
+				printk(KERN_ERR "skb_clone failed\n");
+				goto out;
+			}
+			BUG_ON(tcp_send_head(subsk));
+			mtcp_skb_entail(subsk, subskb);
+		}
+		else
+			subskb=skb;
+
+		BUG_ON(tcp_send_head(sk)!=skb);
+
+		TCP_SKB_CB(subskb)->when = tcp_time_stamp;
+		if (likely(!(err=tcp_transmit_skb(subsk, subskb, 1, 
+						  subsk->sk_allocation)))) {
+			if (TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN) {
+				struct sock *sk_it;
+				struct tcp_sock *tp_it;
+				/*App close: we have sent every app-level byte,
+				  send now the FIN on all subflows.*/
+				mtcp_for_each_sk(tp->mpcb,sk_it,tp_it)
+					if (sk_it!=subsk)
+						tcp_send_fin(sk_it);
+			}
+			tcp_event_new_data_sent(subsk, subskb);
+			BUG_ON(tcp_send_head(subsk));
+			if (sk!=subsk && !reinject)
+				tcp_event_new_data_sent(sk,skb);
+			tcp_cwnd_validate(subsk);
+		}
+		else if (sk!=subsk) {
+			/*Remove the skb from the subsock*/
+			tcp_advance_send_head(subsk,subskb);
+			tcp_unlink_write_queue(subskb,subsk);
+			subtp->write_seq-=subskb->len;
+			mtcp_wmem_free_skb(subsk, subskb);
+			if (err>0 && tp->mpcb->cnt_subflows>1) {
+				tp->mpcb->noneligible|=
+					PI_TO_FLAG(subtp->path_index);
+				goto again;
+			}
 		}
 	}
+out:
+	if (tp->mpcb) tp->mpcb->noneligible=0;
 }
 
 /* This function returns the amount that we can raise the
@@ -1702,7 +2201,12 @@ void tcp_push_one(struct sock *sk, unsigned int mss_now)
  * Note, we don't "adjust" for TIMESTAMP or SACK option bytes.
  * Regular options like TIMESTAMP are taken into account.
  */
-u32 __tcp_select_window(struct sock *sk)
+
+#ifndef CONFIG_MTCP
+#define __tcp_select_window_fallback __tcp_select_window
+#endif
+
+u32 __tcp_select_window_fallback(struct sock *sk)
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -1723,16 +2227,92 @@ u32 __tcp_select_window(struct sock *sk)
 	if (free_space < (full_space >> 1)) {
 		icsk->icsk_ack.quick = 0;
 
-		if (tcp_memory_pressure)
+		if (tcp_memory_pressure) {
 			tp->rcv_ssthresh = min(tp->rcv_ssthresh,
 					       4U * tp->advmss);
+		}
 
 		if (free_space < mss)
 			return 0;
 	}
 
-	if (free_space > tp->rcv_ssthresh)
+	if (free_space > tp->rcv_ssthresh) {
 		free_space = tp->rcv_ssthresh;
+	}
+
+	/* Don't do rounding if we are using window scaling, since the
+	 * scaled window will not line up with the MSS boundary anyway.
+	 */
+	window = tp->rcv_wnd;
+	if (tp->rx_opt.rcv_wscale) {
+		window = free_space;
+
+		/* Advertise enough space so that it won't get scaled away.
+		 * Import case: prevent zero window announcement if
+		 * 1<<rcv_wscale > mss.
+		 */
+		if (((window >> tp->rx_opt.rcv_wscale) << tp->rx_opt.rcv_wscale) != window)
+			window = (((window >> tp->rx_opt.rcv_wscale) + 1)
+				  << tp->rx_opt.rcv_wscale);
+	} else {
+		/* Get the largest window that is a nice multiple of mss.
+		 * Window clamp already applied above.
+		 * If our current window offering is within 1 mss of the
+		 * free space we just keep it. This prevents the divide
+		 * and multiply from happening most of the time.
+		 * We also don't do any window rounding when the free space
+		 * is too small.
+		 */
+		if (window <= free_space - mss || window > free_space)
+			window = (free_space / mss) * mss;
+		else if (mss == full_space &&
+			 free_space > window + (full_space >> 1))
+			window = free_space;
+	}
+
+	return window;
+}
+
+#ifdef CONFIG_MTCP
+u32 __tcp_select_window(struct sock *sk)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct multipath_pcb *mpcb = tp->mpcb;
+	int mss,free_space,full_space,window;
+
+	BUG_ON(!tp->mpcb && !tp->pending);
+	if (!tp->mpc || !tp->mpcb) return __tcp_select_window_fallback(sk);
+
+	/* MSS for the peer's data.  Previous versions used mss_clamp
+	 * here.  I don't know if the value based on our guesses
+	 * of peer's MSS is better for the performance.  It's more correct
+	 * but may be worse for the performance because of rcv_mss
+	 * fluctuations.  --SAW  1998/11/1
+	 */
+	mss = icsk->icsk_ack.rcv_mss;
+	free_space = mtcp_space(sk);
+	full_space = min_t(int, mpcb->tp.window_clamp, mtcp_full_space(sk));
+
+	if (mss > full_space)
+		mss = full_space;
+
+	if (free_space < (full_space >> 1)) {
+		icsk->icsk_ack.quick = 0;
+
+		if (tcp_memory_pressure) {
+			tp->rcv_ssthresh = min(tp->rcv_ssthresh,
+					       4U * tp->advmss);
+			mtcp_update_window_clamp(mpcb);
+		}
+
+		if (free_space < mss)
+			return 0;
+	}
+
+	if (free_space > mpcb->tp.rcv_ssthresh) {
+		free_space = mpcb->tp.rcv_ssthresh;
+	}
 
 	/* Don't do rounding if we are using window scaling, since the
 	 * scaled window will not line up with the MSS boundary anyway.
@@ -1766,6 +2346,7 @@ u32 __tcp_select_window(struct sock *sk)
 
 	return window;
 }
+#endif
 
 /* Attempt to collapse two adjacent SKB's during retransmission. */
 static void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *skb,
@@ -1791,9 +2372,12 @@ static void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *skb,
 		return;
 
 	/* Next skb is out of window. */
-	if (after(TCP_SKB_CB(next_skb)->end_seq, tcp_wnd_end(tp)))
+	if (!tp->mpc && after(TCP_SKB_CB(next_skb)->end_seq, tcp_wnd_end(tp,0)))
 		return;
-
+	if (tp->mpc && after(TCP_SKB_CB(next_skb)->end_data_seq, 
+			     tcp_wnd_end(tp,1)))
+		return;
+	
 	/* Punt if not enough space exists in the first SKB for
 	 * the data in the second, or the total combined payload
 	 * would exceed the MSS.
@@ -1820,6 +2404,13 @@ static void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *skb,
 
 	/* Update sequence range on original skb. */
 	TCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;
+	/*For the dsn space, we need to make an addition and not just
+	  copy the end_seq, because if the next_skb is a pure FIN (with
+	  no data), the len is 1 and the data_len is 0, as well as
+	  the end_data_seq of the FIN. Using an addition takes this
+	  difference into account*/
+	TCP_SKB_CB(skb)->end_data_seq += TCP_SKB_CB(next_skb)->data_len;
+	TCP_SKB_CB(skb)->data_len += TCP_SKB_CB(next_skb)->data_len;
 
 	/* Merge over control information. */
 	flags |= TCP_SKB_CB(next_skb)->flags; /* This moves PSH/FIN etc. over */
@@ -1909,7 +2500,16 @@ int tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb)
 	unsigned int cur_mss;
 	int err;
 
-	/* Inconslusive MTU probe */
+	BUG_ON(!skb);
+	
+	/*In case of RTO (loss state), we reinject data on another subflow*/
+	if (icsk->icsk_ca_state == TCP_CA_Loss &&
+	    tp->mpc && sk->sk_state==TCP_ESTABLISHED &&
+	    tp->path_index) {
+		mtcp_reinject_data(sk);
+	}
+	
+	/* Inconclusive MTU probe */
 	if (icsk->icsk_mtup.probe_size) {
 		icsk->icsk_mtup.probe_size = 0;
 	}
@@ -1931,14 +2531,19 @@ int tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb)
 	if (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))
 		return -EHOSTUNREACH; /* Routing failure or similar. */
 
+#ifdef CONFIG_MTCP
+	cur_mss = sysctl_mptcp_mss;
+#else
 	cur_mss = tcp_current_mss(sk, 0);
+#endif
 
 	/* If receiver has shrunk his window, and skb is out of
 	 * new window, do not retransmit it. The exception is the
 	 * case, when window is shrunk to zero. In this case
 	 * our retransmit serves as a zero window probe.
 	 */
-	if (!before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp))
+	if (!before((tp->mpc)?TCP_SKB_CB(skb)->data_seq:
+		    TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp,tp->mpc))
 	    && TCP_SKB_CB(skb)->seq != tp->snd_una)
 		return -EAGAIN;
 
@@ -1987,14 +2592,9 @@ int tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb)
 
 		tp->total_retrans++;
 
-#if FASTRETRANS_DEBUG > 0
-		if (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS) {
-			if (net_ratelimit())
-				printk(KERN_DEBUG "retrans_out leaked.\n");
-		}
-#endif
 		if (!tp->retrans_out)
 			tp->lost_retrans_low = tp->snd_nxt;
+
 		TCP_SKB_CB(skb)->sacked |= TCPCB_RETRANS;
 		tp->retrans_out += tcp_skb_pcount(skb);
 
@@ -2057,6 +2657,8 @@ void tcp_xmit_retransmit_queue(struct sock *sk)
 	int mib_idx;
 	int fwd_rexmitting = 0;
 
+	BUG_ON(is_meta_sk(sk));
+
 	if (!tp->lost_out)
 		tp->retransmit_high = tp->snd_una;
 
@@ -2148,24 +2750,34 @@ void tcp_send_fin(struct sock *sk)
 	 * unsent frames.  But be careful about outgoing SACKS
 	 * and IP options.
 	 */
-	mss_now = tcp_current_mss(sk, 1);
+	if (!tp->mpc) mss_now = tcp_current_mss(sk, 1);
+	else mss_now = sysctl_mptcp_mss;
 
 	if (tcp_send_head(sk) != NULL) {
 		TCP_SKB_CB(skb)->flags |= TCPCB_FLAG_FIN;
 		TCP_SKB_CB(skb)->end_seq++;
-		tp->write_seq++;
-	} else {
-		/* Socket is locked, keep trying until memory is available. */
-		for (;;) {
-			skb = alloc_skb_fclone(MAX_TCP_HEADER, GFP_KERNEL);
-			if (skb)
-				break;
-			yield();
-		}
+		tp->write_seq++;	       
+	}
+	else {
+		/* Socket is locked, keep trying until memory is available. 
+		   Due to the possible call from tcp_write_xmit, we might
+		   be called from interrupt context, hence the following cond.*/
+		if (!in_interrupt())
+			for (;;) {
+				skb = alloc_skb_fclone(MAX_TCP_HEADER, 
+						       GFP_KERNEL);
+				if (skb)
+					break;
+				yield();
+			}
+		else
+			skb = alloc_skb_fclone(MAX_TCP_HEADER, 
+					       GFP_ATOMIC);
 
 		/* Reserve space for headers and prepare control bits. */
 		skb_reserve(skb, MAX_TCP_HEADER);
-		/* FIN eats a sequence byte, write_seq advanced by tcp_queue_skb(). */
+		/* FIN eats a sequence byte, write_seq advanced by 
+		   tcp_queue_skb(). */
 		tcp_init_nondata_skb(skb, tp->write_seq,
 				     TCPCB_FLAG_ACK | TCPCB_FLAG_FIN);
 		tcp_queue_skb(sk, skb);
@@ -2260,8 +2872,12 @@ struct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 	skb_reserve(skb, MAX_TCP_HEADER);
 
 	skb->dst = dst_clone(dst);
-
+#ifdef CONFIG_MTCP
+	mss = sysctl_mptcp_mss;
+#else
 	mss = dst_metric(dst, RTAX_ADVMSS);
+#endif
+
 	if (tp->rx_opt.user_mss && tp->rx_opt.user_mss < mss)
 		mss = tp->rx_opt.user_mss;
 
@@ -2270,12 +2886,23 @@ struct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 		/* Set this up on the first call only */
 		req->window_clamp = tp->window_clamp ? : dst_metric(dst, RTAX_WINDOW);
 		/* tcp_full_space because it is guaranteed to be the first packet */
+#ifdef CONFIG_MTCP
+		tcp_select_initial_window(mtcp_full_space(sk),
+					  mss - (ireq->tstamp_ok ? 
+						 TCPOLEN_TSTAMP_ALIGNED : 0),
+					  &req->rcv_wnd,
+					  &req->window_clamp,
+					  ireq->wscale_ok,
+					  &rcv_wscale);
+#else
 		tcp_select_initial_window(tcp_full_space(sk),
-			mss - (ireq->tstamp_ok ? TCPOLEN_TSTAMP_ALIGNED : 0),
-			&req->rcv_wnd,
-			&req->window_clamp,
-			ireq->wscale_ok,
-			&rcv_wscale);
+					  mss - (ireq->tstamp_ok ? 
+						 TCPOLEN_TSTAMP_ALIGNED : 0),
+					  &req->rcv_wnd,
+					  &req->window_clamp,
+					  ireq->wscale_ok,
+					  &rcv_wscale);
+#endif
 		ireq->rcv_wscale = rcv_wscale;
 	}
 
@@ -2288,8 +2915,8 @@ struct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 	TCP_SKB_CB(skb)->when = tcp_time_stamp;
 	tcp_header_size = tcp_synack_options(sk, req, mss,
 					     skb, &opts, &md5) +
-			  sizeof(struct tcphdr);
-
+		sizeof(struct tcphdr);       
+	
 	skb_push(skb, tcp_header_size);
 	skb_reset_transport_header(skb);
 
@@ -2354,26 +2981,45 @@ static void tcp_connect_init(struct sock *sk)
 
 	if (!tp->window_clamp)
 		tp->window_clamp = dst_metric(dst, RTAX_WINDOW);
+#ifdef CONFIG_MTCP
+	tp->advmss = sysctl_mptcp_mss;
+	if (tp->advmss>dst_metric(dst,RTAX_ADVMSS))
+		tp->mss_too_low=1;
+#else
 	tp->advmss = dst_metric(dst, RTAX_ADVMSS);
+#endif
 	if (tp->rx_opt.user_mss && tp->rx_opt.user_mss < tp->advmss)
 		tp->advmss = tp->rx_opt.user_mss;
 
 	tcp_initialize_rcv_mss(sk);
 
-	tcp_select_initial_window(tcp_full_space(sk),
+#ifdef CONFIG_MTCP
+	tcp_select_initial_window(mtcp_full_space(sk),
 				  tp->advmss - (tp->rx_opt.ts_recent_stamp ? tp->tcp_header_len - sizeof(struct tcphdr) : 0),
 				  &tp->rcv_wnd,
 				  &tp->window_clamp,
 				  sysctl_tcp_window_scaling,
 				  &rcv_wscale);
 
+	mtcp_update_window_clamp(tp->mpcb);
+#else
+	tcp_select_initial_window(tcp_full_space(sk),
+				  tp->advmss - (tp->rx_opt.ts_recent_stamp 
+						? tp->tcp_header_len - 
+						sizeof(struct tcphdr) : 0),
+				  &tp->rcv_wnd,
+				  &tp->window_clamp,
+				  sysctl_tcp_window_scaling,
+				  &rcv_wscale);
+#endif
+
 	tp->rx_opt.rcv_wscale = rcv_wscale;
 	tp->rcv_ssthresh = tp->rcv_wnd;
 
 	sk->sk_err = 0;
 	sock_reset_flag(sk, SOCK_DONE);
 	tp->snd_wnd = 0;
-	tcp_init_wl(tp, tp->write_seq, 0);
+	tcp_init_wl(tp, 0);
 	tp->snd_una = tp->write_seq;
 	tp->snd_sml = tp->write_seq;
 	tp->snd_up = tp->write_seq;
@@ -2415,6 +3061,7 @@ int tcp_connect(struct sock *sk)
 	sk->sk_wmem_queued += buff->truesize;
 	sk_mem_charge(sk, buff->truesize);
 	tp->packets_out += tcp_skb_pcount(buff);
+	
 	tcp_transmit_skb(sk, buff, 1, GFP_KERNEL);
 
 	/* We change tp->snd_nxt after the tcp_transmit_skb() call
@@ -2558,10 +3205,13 @@ int tcp_write_wakeup(struct sock *sk)
 		return -1;
 
 	if ((skb = tcp_send_head(sk)) != NULL &&
-	    before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp))) {
+	    before((tp->mpc)?TCP_SKB_CB(skb)->data_seq:
+		   TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp,tp->mpc))) {
 		int err;
 		unsigned int mss = tcp_current_mss(sk, 0);
-		unsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;
+		unsigned int seg_size = tcp_wnd_end(tp,tp->mpc) - 
+			((tp->mpc)?TCP_SKB_CB(skb)->data_seq:
+			 TCP_SKB_CB(skb)->seq);
 
 		if (before(tp->pushed_seq, TCP_SKB_CB(skb)->end_seq))
 			tp->pushed_seq = TCP_SKB_CB(skb)->end_seq;
diff --git a/net/ipv4/tcp_probe.c b/net/ipv4/tcp_probe.c
index 7ddc30f..20fa98d 100644
--- a/net/ipv4/tcp_probe.c
+++ b/net/ipv4/tcp_probe.c
@@ -19,7 +19,6 @@
  */
 
 #include <linux/kernel.h>
-#include <linux/kprobes.h>
 #include <linux/socket.h>
 #include <linux/tcp.h>
 #include <linux/proc_fs.h>
@@ -28,6 +27,12 @@
 #include <linux/time.h>
 #include <net/net_namespace.h>
 
+#ifdef CONFIG_KPROBES
+#include <linux/kprobes.h>
+#else
+#include <linux/tcp_probe.h>
+#endif
+
 #include <net/tcp.h>
 
 MODULE_AUTHOR("Stephen Hemminger <shemminger@linux-foundation.org>");
@@ -43,13 +48,14 @@ static int bufsize __read_mostly = 4096;
 MODULE_PARM_DESC(bufsize, "Log buffer size in packets (4096)");
 module_param(bufsize, int, 0);
 
-static int full __read_mostly;
+static int full __read_mostly=1;
 MODULE_PARM_DESC(full, "Full log (1=every ack packet received,  0=only cwnd changes)");
 module_param(full, int, 0);
 
 static const char procname[] = "tcpprobe";
 
 struct tcp_log {
+	int     path_index;
 	ktime_t tstamp;
 	__be32	saddr, daddr;
 	__be16	sport, dport;
@@ -60,6 +66,26 @@ struct tcp_log {
 	u32	snd_cwnd;
 	u32	ssthresh;
 	u32	srtt;
+	u32     rcv_nxt;
+	u32     copied_seq;
+	u32     rcv_wnd;
+	u32     rcv_buf;  /*N*/
+	u32     rcv_ssthresh; /*N*/
+	u32     window_clamp; /*N*/
+	char    send; /*1 if sending side, 0 if receive*/
+	int     space;
+	u32     rtt_est;
+	u32     in_flight;
+	u32     mss_cache;
+	int     snd_buf;
+	int     wmem_queued;
+	int     rmem_alloc; /*number of ofo bytes received*/
+	int     dsn;
+        u32     mtcp_snduna;
+	u32     drs_seq;
+	u32     drs_time;
+	int     bw_est;
+	char    mpcb_def;
 };
 
 static struct {
@@ -80,6 +106,8 @@ static inline int tcp_probe_used(void)
 
 static inline int tcp_probe_avail(void)
 {
+	if (!(bufsize-tcp_probe_used()))
+		printk(KERN_ERR "No log space anymore\n");
 	return bufsize - tcp_probe_used();
 }
 
@@ -88,33 +116,67 @@ static inline int tcp_probe_avail(void)
  * Note: arguments must match tcp_rcv_established()!
  */
 static int jtcp_rcv_established(struct sock *sk, struct sk_buff *skb,
-			       struct tcphdr *th, unsigned len)
+				struct tcphdr *th, unsigned len)
 {
-	const struct tcp_sock *tp = tcp_sk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
 	const struct inet_sock *inet = inet_sk(sk);
+	struct sock *mpcb_sk=tp->mpcb?(struct sock*)tp->mpcb:sk;
+	struct tcp_sock *mpcb_tp=tcp_sk(mpcb_sk);
+
+	if (!tp->last_rcv_probe)
+		tp->last_rcv_probe=jiffies;
+	else if (jiffies-tp->last_rcv_probe<HZ/10)
+	goto out;
+	
+	tp->last_rcv_probe=jiffies;
 
 	/* Only update if port matches */
-	if ((port == 0 || ntohs(inet->dport) == port || ntohs(inet->sport) == port)
+	if ((port == 0 || ntohs(inet->dport) == port 
+	     || ntohs(inet->sport) == port)
+	    && ((ntohl(inet->saddr) & 0xffff0000)!=0xc0a80000) /*addr != 
+							       192.168/16*/
+	    && ((ntohl(inet->daddr) & 0xffff0000)!=0xc0a80000)
+	    && ntohs(inet->sport) != 9000 && ntohs(inet->dport) != 9000
 	    && (full || tp->snd_cwnd != tcp_probe.lastcwnd)) {
 
 		spin_lock(&tcp_probe.lock);
 		/* If log fills, just silently drop */
 		if (tcp_probe_avail() > 1) {
 			struct tcp_log *p = tcp_probe.log + tcp_probe.head;
-
+			if (tp->mpc) BUG_ON(!tp->mpcb && !tp->pending);
 			p->tstamp = ktime_get();
 			p->saddr = inet->saddr;
 			p->sport = inet->sport;
 			p->daddr = inet->daddr;
 			p->dport = inet->dport;
+			p->path_index = sk?tcp_sk(sk)->path_index:0;
 			p->length = skb->len;
 			p->snd_nxt = tp->snd_nxt;
 			p->snd_una = tp->snd_una;
 			p->snd_cwnd = tp->snd_cwnd;
-			p->snd_wnd = tp->snd_wnd;
+			p->snd_wnd = mpcb_tp->snd_wnd;
 			p->ssthresh = tcp_current_ssthresh(sk);
 			p->srtt = tp->srtt >> 3;
-
+			p->rcv_nxt=mpcb_tp->rcv_nxt;
+			p->copied_seq=mpcb_tp->copied_seq;
+			p->rcv_wnd=mpcb_tp->rcv_wnd;
+			p->rcv_buf=sk->sk_rcvbuf;
+			p->rcv_ssthresh=tp->rcv_ssthresh;
+			p->window_clamp=tp->window_clamp;
+			p->send=0;
+			p->space=tp->rcvq_space.space;
+			p->rtt_est=tp->rcv_rtt_est.rtt;
+			p->in_flight=tp->packets_out;
+			p->mss_cache=tp->mss_cache;
+			p->snd_buf=mpcb_sk->sk_sndbuf;
+			p->wmem_queued=mpcb_sk->sk_wmem_queued;
+			p->rmem_alloc=atomic_read(&mpcb_sk->sk_rmem_alloc);
+			p->dsn=TCP_SKB_CB(skb)->data_seq;
+			p->mtcp_snduna=(tp->mpcb)?tp->mpcb->tp.snd_una:0;
+			p->drs_seq=tp->rcvq_space.seq;
+			p->drs_time=tp->rcvq_space.time;
+			p->bw_est=tp->cur_bw_est;
+			p->mpcb_def=(tp->mpcb!=NULL);
 			tcp_probe.head = (tcp_probe.head + 1) % bufsize;
 		}
 		tcp_probe.lastcwnd = tp->snd_cwnd;
@@ -122,17 +184,161 @@ static int jtcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 
 		wake_up(&tcp_probe.wait);
 	}
+out:
+#ifdef CONFIG_KPROBES
+	jprobe_return();
+#endif
+	return 0;
+}
+
+#ifndef CONFIG_KPROBES
+static int logmsg(struct sock *sk,char *fmt, va_list args)
+{
+	const struct inet_sock *inet = inet_sk(sk);
+	char msg[500];	
+	struct timespec tv
+		= ktime_to_timespec(ktime_sub(ktime_get(), tcp_probe.start));
+	
+	if (sk->sk_state == TCP_ESTABLISHED
+	    && ((ntohl(inet->saddr) & 0xffff0000)!=0xc0a80000) /*addr != 
+								 192.168/16*/
+	    && ((ntohl(inet->daddr) & 0xffff0000)!=0xc0a80000)) {
+		int len;
+		snprintf(msg,500,"LOG:%lu.%09lu ",(unsigned long) tv.tv_sec,
+			(unsigned long) tv.tv_nsec);
+		len=strlen(msg);
+		vsnprintf(msg+len,500-len,fmt,args);
+
+		spin_lock_bh(&tcp_probe.lock);
+		/* If log fills, just silently drop */
+		if (tcp_probe_avail() > 1) {
+			struct tcp_log *p = tcp_probe.log + tcp_probe.head;
+			p->path_index=-1;
+			strncpy((char*)((&p->path_index)+1),msg,
+				sizeof(*p)-sizeof(p->path_index));
+			tcp_probe.head = (tcp_probe.head + 1) % bufsize;
+		}
+		spin_unlock_bh(&tcp_probe.lock);
+		wake_up(&tcp_probe.wait);
+	}
+	return 0;
+}
+#endif
+
+/*
+ * Hook inserted to be called before each packet transmission.
+ * Note: arguments must match tcp_transmit_skb()!
+ */
+static int jtcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
+			     gfp_t gfp_mask)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	const struct inet_sock *inet = inet_sk(sk);
+	struct sock *mpcb_sk=tp->mpcb?(struct sock*)tp->mpcb:sk;
+	struct tcp_sock *mpcb_tp=tcp_sk(mpcb_sk);
+
+	if (!tp->last_snd_probe)
+		tp->last_snd_probe=jiffies;
+	else if (jiffies-tp->last_snd_probe<HZ/10)
+		goto out;
+	
+	tp->last_snd_probe=jiffies;
+
+	/* Only update if port matches and state is established*/
+	if (sk->sk_state == TCP_ESTABLISHED && 
+	    (port == 0 || ntohs(inet->dport) == port || 
+	     ntohs(inet->sport) == port)
+	    && ((ntohl(inet->saddr) & 0xffff0000)!=0xc0a80000) /*addr != 
+							       192.168/16*/
+	    && ((ntohl(inet->daddr) & 0xffff0000)!=0xc0a80000)
+	    && ntohs(inet->sport) != 9000 && ntohs(inet->dport) != 9000
+	    && (full || tp->snd_cwnd != tcp_probe.lastcwnd)) {
+
+#ifdef CONFIG_KPROBES
+		/*kprobes disables irqs before to call this function.
+		  So we cannot use the _bh flavour of spin_lock*/
+		spin_lock(&tcp_probe.lock);
+#else
+		spin_lock_bh(&tcp_probe.lock);
+#endif
+		/* If log fills, just silently drop */
+		if (tcp_probe_avail() > 1) {
+			struct tcp_log *p = tcp_probe.log + tcp_probe.head;
+
+			p->tstamp = ktime_get();
+			p->saddr = inet->saddr;
+			p->sport = inet->sport;
+			p->daddr = inet->daddr;
+			p->dport = inet->dport;
+			p->path_index = tp->path_index;
+			p->length = skb->len;
+			p->snd_nxt = tp->snd_nxt;
+			p->snd_una = tp->snd_una;
+			p->snd_cwnd = tp->snd_cwnd;
+			p->snd_wnd = mpcb_tp->snd_wnd;
+			p->ssthresh = tcp_current_ssthresh(sk);
+			p->srtt = tp->srtt >> 3;
+			p->rcv_nxt=mpcb_tp->rcv_nxt;
+			p->copied_seq=mpcb_tp->copied_seq;
+			p->rcv_wnd=mpcb_tp->rcv_wnd;
+			p->rcv_buf=sk->sk_rcvbuf;
+			p->rcv_ssthresh=tp->rcv_ssthresh;
+			p->window_clamp=tp->window_clamp;		
+			p->send=1;
+			p->space=tp->rcvq_space.space;
+			p->rtt_est=tp->rcv_rtt_est.rtt;
+			p->in_flight=tp->packets_out;
+			p->mss_cache=tp->mss_cache;
+			p->snd_buf=mpcb_sk->sk_sndbuf;
+			p->wmem_queued=mpcb_sk->sk_wmem_queued;
+			p->rmem_alloc=atomic_read(&mpcb_sk->sk_rmem_alloc);
+			p->dsn=TCP_SKB_CB(skb)->data_seq;
+			p->mtcp_snduna=(tp->mpcb)?tp->mpcb->tp.snd_una:0;
+			p->drs_seq=tp->rcvq_space.seq;
+			p->drs_time=tp->rcvq_space.time;
+			p->bw_est=tp->cur_bw_est;
+			p->mpcb_def=(tp->mpcb!=NULL);
+			tcp_probe.head = (tcp_probe.head + 1) % bufsize;
+		}
+		tcp_probe.lastcwnd = tp->snd_cwnd;
+#ifdef CONFIG_KPROBES
+		spin_unlock(&tcp_probe.lock);
+#else
+		spin_unlock_bh(&tcp_probe.lock);
+#endif
+		
+		wake_up(&tcp_probe.wait);
+	}
 
+out:
+#ifdef CONFIG_KPROBES
 	jprobe_return();
+#endif
 	return 0;
 }
 
-static struct jprobe tcp_jprobe = {
+#ifdef CONFIG_KPROBES
+static struct jprobe tcp_jprobe_rcv = {
 	.kp = {
 		.symbol_name	= "tcp_rcv_established",
 	},
 	.entry	= jtcp_rcv_established,
+	};
+static struct jprobe tcp_jprobe_send = {
+	.kp = {
+		.symbol_name	= "tcp_transmit_skb",		
+	},
+	.entry	= jtcp_transmit_skb,
+	};
+#else
+static struct tcpprobe_ops tcpprobe_fcts = {
+	.rcv_established=jtcp_rcv_established,
+	.transmit_skb=jtcp_transmit_skb,
+	.logmsg=logmsg,
 };
+#endif
+
+
 
 static int tcpprobe_open(struct inode * inode, struct file * file)
 {
@@ -151,16 +357,29 @@ static int tcpprobe_sprint(char *tbuf, int n)
 		= tcp_probe.log + tcp_probe.tail % bufsize;
 	struct timespec tv
 		= ktime_to_timespec(ktime_sub(p->tstamp, tcp_probe.start));
-
+	
+	if (p->path_index==-1) {
+		return snprintf(tbuf,n,
+				"%s\n",(char*)((&p->path_index)+1));
+	}
+	
 	return snprintf(tbuf, n,
 			"%lu.%09lu " NIPQUAD_FMT ":%u " NIPQUAD_FMT ":%u"
-			" %d %#x %#x %u %u %u %u\n",
+			" %d %d %#x %#x %u %u %u %u %#x %#x %u %u %u %u %d"
+			" %d %u %u %u %d %d %d %#x %#x %#x %#x %d %d\n",
 			(unsigned long) tv.tv_sec,
 			(unsigned long) tv.tv_nsec,
 			NIPQUAD(p->saddr), ntohs(p->sport),
 			NIPQUAD(p->daddr), ntohs(p->dport),
-			p->length, p->snd_nxt, p->snd_una,
-			p->snd_cwnd, p->ssthresh, p->snd_wnd, p->srtt);
+			p->path_index,p->length, p->snd_nxt, p->snd_una,
+			p->snd_cwnd, p->ssthresh, p->snd_wnd, p->srtt,
+			p->rcv_nxt,p->copied_seq,p->rcv_wnd,p->rcv_buf,
+			p->window_clamp,p->rcv_ssthresh, p->send,
+			p->space,p->rtt_est*1000/HZ,p->in_flight,
+			p->mss_cache,
+			p->snd_buf,p->wmem_queued, p->rmem_alloc, p->dsn,
+			p->mtcp_snduna,p->drs_seq,p->drs_time*1000/HZ,
+			((p->bw_est<<3)/1000)*HZ,p->mpcb_def);
 }
 
 static ssize_t tcpprobe_read(struct file *file, char __user *buf,
@@ -172,7 +391,7 @@ static ssize_t tcpprobe_read(struct file *file, char __user *buf,
 		return -EINVAL;
 
 	while (cnt < len) {
-		char tbuf[128];
+		char tbuf[512];
 		int width;
 
 		/* Wait for data in buffer */
@@ -231,7 +450,12 @@ static __init int tcpprobe_init(void)
 	if (!proc_net_fops_create(&init_net, procname, S_IRUSR, &tcpprobe_fops))
 		goto err0;
 
-	ret = register_jprobe(&tcp_jprobe);
+#ifdef CONFIG_KPROBES
+	ret = register_jprobe(&tcp_jprobe_rcv);
+	if (!ret) ret = register_jprobe(&tcp_jprobe_send);
+#else
+	ret=register_probe(&tcpprobe_fcts, 4);
+#endif
 	if (ret)
 		goto err1;
 
@@ -248,7 +472,12 @@ module_init(tcpprobe_init);
 static __exit void tcpprobe_exit(void)
 {
 	proc_net_remove(&init_net, procname);
-	unregister_jprobe(&tcp_jprobe);
+#ifdef CONFIG_KPROBES
+	unregister_jprobe(&tcp_jprobe_rcv);
+	unregister_jprobe(&tcp_jprobe_send);
+#else
+	unregister_probe(&tcpprobe_fcts,4);
+#endif
 	kfree(tcp_probe.log);
 }
 module_exit(tcpprobe_exit);
diff --git a/net/ipv4/tcp_probe_static.c b/net/ipv4/tcp_probe_static.c
new file mode 100644
index 0000000..41e0079
--- /dev/null
+++ b/net/ipv4/tcp_probe_static.c
@@ -0,0 +1,84 @@
+#include <linux/tcp_probe.h>
+#include <linux/ip.h>
+
+static struct tcpprobe_ops *tcpprobe_fcts=NULL;
+static struct tcpprobe_ops *tcpprobe6_fcts=NULL;
+
+static struct tcpprobe_ops **select_family(unsigned short family)
+{
+	switch (family) {
+	case 4:
+		return &tcpprobe_fcts;
+	case 6:
+		return &tcpprobe6_fcts;
+	default:
+		return NULL;
+	}
+}
+
+/* @ipversion is 4 or 6
+ */
+int register_probe(struct tcpprobe_ops* ops, unsigned char ipversion)
+{
+	struct tcpprobe_ops **vops=select_family(ipversion);
+	/*return -1 if incorrect family, or ops already registered*/
+	if (!vops || *vops) return -1;
+	*vops=ops;
+	return 0;
+}
+EXPORT_SYMBOL(register_probe);
+
+int unregister_probe(struct tcpprobe_ops* ops, unsigned char ipversion)
+{
+	struct tcpprobe_ops **vops=select_family(ipversion);
+	/*return -1 if incorrect family*/
+	if (!vops) return -1;
+	if (*vops!=ops) return -1; /*trying to unregister something 
+				     else, probably a bug*/
+	*vops=NULL;
+	return 0;
+}
+EXPORT_SYMBOL(unregister_probe);
+
+int tcpprobe_rcv_established(struct sock *sk, struct sk_buff *skb,
+			     struct tcphdr *th, unsigned len) 
+{
+	int ipversion=ip_hdr(skb)->version;
+	struct tcpprobe_ops **vops=select_family(ipversion);
+	
+	/*return -1 if incorrect family*/
+	if (!vops) return -1;
+	if (!*vops) return 0;
+	return (*vops)->rcv_established(sk,skb,th,len);
+}
+
+int tcpprobe_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
+			  gfp_t gfp_mask)
+{
+	int ipversion=(sk->sk_family==AF_INET6)?6:4;
+	struct tcpprobe_ops **vops=select_family(ipversion);
+	
+	/*return -1 if incorrect family*/
+	if (!vops) return -1;
+	if (!*vops) return 0;
+	return (*vops)->transmit_skb(sk,skb,clone_it,gfp_mask);
+}
+
+int tcpprobe_logmsg(struct sock *sk,char *fmt,...)
+{
+	int ipversion=(sk->sk_family==AF_INET6)?6:4;
+	struct tcpprobe_ops **vops=select_family(ipversion);
+	va_list args;
+	int i;
+	
+	return 0; /*bypassed at the moment*/
+
+	/*return -1 if incorrect family*/
+
+	if (!vops) return -1;
+	if (!*vops) return 0;
+	va_start(args,fmt);
+	i=(*vops)->logmsg(sk,fmt,args);
+	va_end(args);
+	return i;
+}
diff --git a/net/ipv4/tcp_timer.c b/net/ipv4/tcp_timer.c
index 6b6dff1..0fff8e3 100644
--- a/net/ipv4/tcp_timer.c
+++ b/net/ipv4/tcp_timer.c
@@ -66,6 +66,7 @@ static int tcp_out_of_resources(struct sock *sk, int do_reset)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	int orphans = atomic_read(&tcp_orphan_count);
+	u32 snd_wnd=(tp->mpc && tp->mpcb)?tp->mpcb->tp.snd_wnd:tp->snd_wnd;
 
 	/* If peer does not open window for long time, or did not transmit
 	 * anything for long time, penalize it. */
@@ -84,7 +85,7 @@ static int tcp_out_of_resources(struct sock *sk, int do_reset)
 		 *      1. Last segment was sent recently. */
 		if ((s32)(tcp_time_stamp - tp->lsndtime) <= TCP_TIMEWAIT_LEN ||
 		    /*  2. Window is closed. */
-		    (!tp->snd_wnd && !tp->packets_out))
+		    (!snd_wnd && !tp->packets_out))
 			do_reset = 1;
 		if (do_reset)
 			tcp_send_active_reset(sk, GFP_ATOMIC);
@@ -283,13 +284,18 @@ static void tcp_retransmit_timer(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct inet_connection_sock *icsk = inet_csk(sk);
+	u32 snd_wnd=(tp->mpc && tp->mpcb)?tp->mpcb->tp.snd_wnd:tp->snd_wnd;
+
+	tcpprobe_logmsg(sk,"pi %d, RTO",tp->path_index);
+
 
 	if (!tp->packets_out)
 		goto out;
 
-	WARN_ON(tcp_write_queue_empty(sk));
+	BUG_ON(is_meta_sk(sk));
+	BUG_ON(tcp_write_queue_empty(sk));
 
-	if (!tp->snd_wnd && !sock_flag(sk, SOCK_DEAD) &&
+	if (!snd_wnd && !sock_flag(sk, SOCK_DEAD) &&
 	    !((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))) {
 		/* Receiver dastardly shrinks window. Our retransmits
 		 * become zero probes, but we should not timeout this
@@ -396,17 +402,27 @@ out:;
 
 static void tcp_write_timer(unsigned long data)
 {
-	struct sock *sk = (struct sock*)data;
+	struct sock *sk= (struct sock*)data;
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct multipath_pcb *mpcb=tp->mpcb;
+	struct sock *mpcb_sk=mpcb?((struct sock*)tp->mpcb):NULL;
 	struct inet_connection_sock *icsk = inet_csk(sk);
 	int event;
+	
+	BUG_ON(is_meta_sk(sk));
 
+  	if (mpcb_sk) {
+		kref_get(&tp->mpcb->kref);
+		bh_lock_sock(mpcb_sk);
+	}
 	bh_lock_sock(sk);
-	if (sock_owned_by_user(sk)) {
+	if (sock_owned_by_user(sk) ||
+	    (mpcb_sk && sock_owned_by_user(mpcb_sk))) {
 		/* Try again later */
 		sk_reset_timer(sk, &icsk->icsk_retransmit_timer, jiffies + (HZ / 20));
 		goto out_unlock;
 	}
-
+	
 	if (sk->sk_state == TCP_CLOSE || !icsk->icsk_pending)
 		goto out;
 
@@ -427,12 +443,14 @@ static void tcp_write_timer(unsigned long data)
 		break;
 	}
 	TCP_CHECK_TIMER(sk);
-
+	
 out:
 	sk_mem_reclaim(sk);
 out_unlock:
 	bh_unlock_sock(sk);
+	if (mpcb_sk) bh_unlock_sock(mpcb_sk);
 	sock_put(sk);
+	if (mpcb) kref_put(&mpcb->kref,mpcb_release);
 }
 
 /*
diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index 98c1fd0..088d320 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -709,7 +709,7 @@ out:
 	 * things).  We could add another new stat but at least for now that
 	 * seems like overkill.
 	 */
-	if (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {
+	if (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sock_flags)) {
 		UDP_INC_STATS_USER(sock_net(sk),
 				UDP_MIB_SNDBUFERRORS, is_udplite);
 	}
diff --git a/net/ipv6/Kconfig b/net/ipv6/Kconfig
index ec99215..4fcdbee 100644
--- a/net/ipv6/Kconfig
+++ b/net/ipv6/Kconfig
@@ -104,6 +104,21 @@ config INET6_IPCOMP
 
 	  If unsure, say Y.
 
+config IPV6_TCPPROBE
+	tristate "TCP connection probing over IPv6 (EXPERIMENTAL)"
+	depends on IPV6 && EXPERIMENTAL && PROC_FS
+	---help---
+	  This module allows for capturing the changes to TCP connection
+	  state in response to incoming packets. It is used for debugging
+	  TCP congestion avoidance modules. If you don't understand
+	  what was just said, you don't need it: say N.
+
+	  Documentation on how to use TCP connection probing can be found
+	  at http://linux-net.osdl.org/index.php/TcpProbe
+
+	  To compile this code as a module, choose M here: the
+	  module will be called tcp_probe.
+
 config IPV6_MIP6
 	tristate "IPv6: Mobility (EXPERIMENTAL)"
 	depends on EXPERIMENTAL
diff --git a/net/ipv6/Makefile b/net/ipv6/Makefile
index 686934a..2dff3fd 100644
--- a/net/ipv6/Makefile
+++ b/net/ipv6/Makefile
@@ -13,6 +13,8 @@ ipv6-objs :=	af_inet6.o anycast.o ip6_output.o ip6_input.o addrconf.o \
 ipv6-$(CONFIG_SYSCTL) = sysctl_net_ipv6.o
 ipv6-$(CONFIG_IPV6_MROUTE) += ip6mr.o
 
+obj-$(CONFIG_MTCP) += mtcp_ipv6.o mtcpv6_hashtables.o
+
 ipv6-$(CONFIG_XFRM) += xfrm6_policy.o xfrm6_state.o xfrm6_input.o \
 	xfrm6_output.o
 ipv6-$(CONFIG_NETFILTER) += netfilter.o
@@ -32,6 +34,7 @@ obj-$(CONFIG_INET6_XFRM_MODE_TUNNEL) += xfrm6_mode_tunnel.o
 obj-$(CONFIG_INET6_XFRM_MODE_ROUTEOPTIMIZATION) += xfrm6_mode_ro.o
 obj-$(CONFIG_INET6_XFRM_MODE_BEET) += xfrm6_mode_beet.o
 obj-$(CONFIG_IPV6_MIP6) += mip6.o
+obj-$(CONFIG_IPV6_TCPPROBE) += tcp_probe6.o
 obj-$(CONFIG_NETFILTER)	+= netfilter/
 
 obj-$(CONFIG_IPV6_SIT) += sit.o
diff --git a/net/ipv6/addrconf.c b/net/ipv6/addrconf.c
index d9da5eb..9829a07 100644
--- a/net/ipv6/addrconf.c
+++ b/net/ipv6/addrconf.c
@@ -71,6 +71,7 @@
 #include <net/addrconf.h>
 #include <net/tcp.h>
 #include <net/ip.h>
+
 #include <net/netlink.h>
 #include <net/pkt_sched.h>
 #include <linux/if_tunnel.h>
diff --git a/net/ipv6/af_inet6.c b/net/ipv6/af_inet6.c
index 01edac8..c3ad6ef 100644
--- a/net/ipv6/af_inet6.c
+++ b/net/ipv6/af_inet6.c
@@ -342,6 +342,9 @@ int inet6_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
 	inet->sport = htons(inet->num);
 	inet->dport = 0;
 	inet->daddr = 0;
+#ifdef CONFIG_MTCP
+	if (addr_type != IPV6_ADDR_ANY) mtcp_update_metasocket(sk);
+#endif
 out:
 	release_sock(sk);
 	return err;
@@ -480,7 +483,11 @@ const struct proto_ops inet6_stream_ops = {
 	.shutdown	   = inet_shutdown,		/* ok		*/
 	.setsockopt	   = sock_common_setsockopt,	/* ok		*/
 	.getsockopt	   = sock_common_getsockopt,	/* ok		*/
+#ifdef CONFIG_MTCP
+	.sendmsg	   = mtcp_sendmsg,		/* ok		*/
+#else
 	.sendmsg	   = tcp_sendmsg,		/* ok		*/
+#endif
 	.recvmsg	   = sock_common_recvmsg,	/* ok		*/
 	.mmap		   = sock_no_mmap,
 	.sendpage	   = tcp_sendpage,
@@ -1009,13 +1016,20 @@ static int __init inet6_init(void)
 	if (err)
 		goto sysctl_fail;
 #endif
+#ifdef CONFIG_MTCP
+	err = mtcpv6_init();
+	if (err)
+		goto mtcpsubv6_fail;
+#endif
+
 out:
 	return err;
-
+mtcpsubv6_fail:
 #ifdef CONFIG_SYSCTL
+	ipv6_sysctl_unregister();
+#endif
 sysctl_fail:
 	ipv6_packet_cleanup();
-#endif
 ipv6_packet_fail:
 	tcpv6_exit();
 tcpv6_fail:
diff --git a/net/ipv6/inet6_connection_sock.c b/net/ipv6/inet6_connection_sock.c
index 16d43f2..051e314 100644
--- a/net/ipv6/inet6_connection_sock.c
+++ b/net/ipv6/inet6_connection_sock.c
@@ -167,14 +167,15 @@ struct dst_entry *__inet6_csk_dst_check(struct sock *sk, u32 cookie)
 #ifdef CONFIG_XFRM
 	if (dst) {
 		struct rt6_info *rt = (struct rt6_info *)dst;
-		if (rt->rt6i_flow_cache_genid != atomic_read(&flow_cache_genid)) {
+		if (rt->rt6i_flow_cache_genid != atomic_read(&flow_cache_genid))
+		{
 			sk->sk_dst_cache = NULL;
 			dst_release(dst);
 			dst = NULL;
 		}
 	}
 #endif
-
+	
 	return dst;
 }
 
diff --git a/net/ipv6/ip6_input.c b/net/ipv6/ip6_input.c
index 936f489..a8349c9 100644
--- a/net/ipv6/ip6_input.c
+++ b/net/ipv6/ip6_input.c
@@ -46,6 +46,7 @@
 
 
 
+
 inline int ip6_rcv_finish( struct sk_buff *skb)
 {
 	if (skb->dst == NULL)
@@ -165,6 +166,7 @@ static int ip6_input_finish(struct sk_buff *skb)
 	struct inet6_dev *idev;
 	struct net *net = dev_net(skb->dst->dev);
 
+
 	/*
 	 *	Parse extension headers
 	 */
@@ -176,7 +178,7 @@ resubmit:
 		goto discard;
 	nhoff = IP6CB(skb)->nhoff;
 	nexthdr = skb_network_header(skb)[nhoff];
-
+	
 	raw = raw6_local_deliver(skb, nexthdr);
 
 	hash = nexthdr & (MAX_INET_PROTOS - 1);
@@ -194,6 +196,7 @@ resubmit:
 			skb_postpull_rcsum(skb, skb_network_header(skb),
 					   skb_network_header_len(skb));
 			hdr = ipv6_hdr(skb);
+			
 			if (ipv6_addr_is_multicast(&hdr->daddr) &&
 			    !ipv6_chk_mcast_addr(skb->dev, &hdr->daddr,
 			    &hdr->saddr) &&
@@ -227,6 +230,7 @@ resubmit:
 
 discard:
 	IP6_INC_STATS_BH(net, idev, IPSTATS_MIB_INDISCARDS);
+	printk(KERN_ERR "packet discarded\n");
 	rcu_read_unlock();
 	kfree_skb(skb);
 	return 0;
diff --git a/net/ipv6/ip6_output.c b/net/ipv6/ip6_output.c
index c77db0b..2888b25 100644
--- a/net/ipv6/ip6_output.c
+++ b/net/ipv6/ip6_output.c
@@ -107,7 +107,6 @@ static int ip6_output_finish(struct sk_buff *skb)
 			 ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);
 	kfree_skb(skb);
 	return -EINVAL;
-
 }
 
 /* dev_loopback_xmit for use with netfilter. */
@@ -202,11 +201,27 @@ int ip6_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl,
 	struct in6_addr *first_hop = &fl->fl6_dst;
 	struct dst_entry *dst = skb->dst;
 	struct ipv6hdr *hdr;
-	u8  proto = fl->proto;
+	u8  proto = fl->proto;	
 	int seg_len = skb->len;
 	int hlimit, tclass;
 	u32 mtu;
 
+	/*MTCP hack: We need to distinguish the creation of a TCP 
+	  master subsocket, from TCP slave subsockets created by the
+	  kernel. The only way we found to do that was to define a specific
+	  "MTCPSUB" protocol, so that some of the TCP functions (in particular
+	  socket creation can be made MTCP slave specific, while the majority
+	  of functions are taken from TCP. But The protocol cannot be used
+	  without being registered at the IPv6 layer, so we needed to define
+	  a new unused protocol number for MTCPSUB: IPPROTO_MTCPSUB. This 
+	  protocol number MUST NOT be used to send packets to the network,
+	  and is only used inside the kernel, as a workaround to the socket
+	  system. The consequence is that we need to fix here the proto field 
+	  in case we find it to be IPPROTO_MTCPSUB, and replace it with 
+	  IPPROTO_TCP. Of course it would be nice to find a less intrusive 
+	  design in the future.*/
+	if (proto==IPPROTO_MTCPSUB) proto=IPPROTO_TCP;
+
 	if (opt) {
 		unsigned int head_room;
 
@@ -283,6 +298,16 @@ int ip6_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl,
 	if (net_ratelimit())
 		printk(KERN_DEBUG "IPv6: sending pkt_too_big to self\n");
 	skb->dev = dst->dev;
+
+	/*In case xfrm is used, ICMP pkt too big must be sent with the
+	  iface mtu, not the adjusted mtu that takes xfrm headers into account.
+	  If we don't do this, then we will enter into an infinite 
+	  mtu reduction loop,
+	  since the ICMP handler in xfrm will recompute its MTUs, since it
+	  understands the provided mtu as coming from the network.*/
+	while ((dst=dst->child))
+		mtu=dst_mtu(dst);
+
 	icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
 	IP6_INC_STATS(net, ip6_dst_idev(skb->dst), IPSTATS_MIB_FRAGFAILS);
 	kfree_skb(skb);
diff --git a/net/ipv6/ipv6_sockglue.c b/net/ipv6/ipv6_sockglue.c
index 2aa294b..92a9691 100644
--- a/net/ipv6/ipv6_sockglue.c
+++ b/net/ipv6/ipv6_sockglue.c
@@ -1120,7 +1120,7 @@ int ipv6_getsockopt(struct sock *sk, int level, int optname,
 
 		lock_sock(sk);
 		err = nf_getsockopt(sk, PF_INET6, optname, optval,
-				&len);
+				    &len);
 		release_sock(sk);
 		if (err >= 0)
 			err = put_user(len, optlen);
diff --git a/net/ipv6/mtcp_ipv6.c b/net/ipv6/mtcp_ipv6.c
new file mode 100644
index 0000000..9f74c45
--- /dev/null
+++ b/net/ipv6/mtcp_ipv6.c
@@ -0,0 +1,153 @@
+/*
+ *	MTCP implementation
+ *
+ *	Author:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *
+ *      date : June 09
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <net/sock.h>
+#include <net/mtcp.h>
+#include <net/tcp.h>
+#include <net/protocol.h>
+#include <net/ipv6.h>
+#include <net/transp_v6.h>
+#include <net/addrconf.h>
+
+/*Functions and structures defined in tcp_ipv6.c*/
+extern struct inet_connection_sock_af_ops ipv6_specific;
+extern int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
+			  int addr_len);
+extern int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);
+extern void tcp_v6_hash(struct sock *sk);
+extern void tcp_v6_destroy_sock(struct sock *sk);
+extern struct timewait_sock_ops tcp6_timewait_sock_ops;
+extern struct request_sock_ops tcp6_request_sock_ops;
+
+
+/* NOTE: A lot of things set to zero explicitly by call to
+ *       sk_alloc() so need not be done here.
+ */
+static int mtcpsub_v6_init_sock(struct sock *sk)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	skb_queue_head_init(&tp->out_of_order_queue);
+	tcp_init_xmit_timers(sk);
+	tcp_prequeue_init(tp);
+
+	icsk->icsk_rto = TCP_TIMEOUT_INIT;
+	tp->mdev = TCP_TIMEOUT_INIT;
+
+	/* So many TCP implementations out there (incorrectly) count the
+	 * initial SYN frame in their delayed-ACK and congestion control
+	 * algorithms that we must have the following bandaid to talk
+	 * efficiently to them.  -DaveM
+	 */
+	tp->snd_cwnd = 2;
+
+	/* See draft-stevens-tcpca-spec-01 for discussion of the
+	 * initialization of these values.
+	 */
+	tp->snd_ssthresh = 0x7fffffff;
+	tp->snd_cwnd_clamp = ~0;
+	tp->mss_cache = 536;
+
+	tp->reordering = sysctl_tcp_reordering;
+
+	sk->sk_state = TCP_CLOSE;
+
+	icsk->icsk_af_ops = &ipv6_specific;
+	icsk->icsk_ca_ops = &tcp_init_congestion_ops;
+	icsk->icsk_sync_mss = tcp_sync_mss;
+	sk->sk_write_space = sk_stream_write_space;
+	sock_set_flag(sk, SOCK_USE_WRITE_QUEUE);
+
+#ifdef CONFIG_TCP_MD5SIG
+	tp->af_specific = &tcp_sock_ipv6_specific;
+#endif
+
+	sk->sk_sndbuf = sysctl_tcp_wmem[1];
+	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
+
+	atomic_inc(&tcp_sockets_allocated);
+
+	return 0;
+}
+
+
+
+struct proto mtcpsubv6_prot = {
+	.name			= "MTCPSUBv6",
+	.owner			= THIS_MODULE,
+	.close			= tcp_close,
+	.connect		= tcp_v6_connect,
+	.disconnect		= tcp_disconnect,
+	.accept			= inet_csk_accept,
+	.ioctl			= tcp_ioctl,
+	.init			= mtcpsub_v6_init_sock,
+	.destroy		= tcp_v6_destroy_sock,
+	.shutdown		= tcp_shutdown,
+	.setsockopt		= tcp_setsockopt,
+	.getsockopt		= tcp_getsockopt,
+	.recvmsg		= tcp_recvmsg,
+	.backlog_rcv		= tcp_v6_do_rcv,
+	.hash			= tcp_v6_hash,
+	.unhash			= inet_unhash,
+	.get_port		= mtcpsub_get_port,
+	.enter_memory_pressure	= tcp_enter_memory_pressure,
+	.sockets_allocated	= &tcp_sockets_allocated,
+	.memory_allocated	= &tcp_memory_allocated,
+	.memory_pressure	= &tcp_memory_pressure,
+	.orphan_count		= &tcp_orphan_count,
+	.sysctl_mem		= sysctl_tcp_mem,
+	.sysctl_wmem		= sysctl_tcp_wmem,
+	.sysctl_rmem		= sysctl_tcp_rmem,
+	.max_header		= MAX_TCP_HEADER,
+	.obj_size		= sizeof(struct tcp6_sock),
+	.twsk_prot		= &tcp6_timewait_sock_ops,
+	.rsk_prot		= &tcp6_request_sock_ops,
+	.h.hashinfo		= &tcp_hashinfo,
+#ifdef CONFIG_COMPAT
+	.compat_setsockopt	= compat_tcp_setsockopt,
+	.compat_getsockopt	= compat_tcp_getsockopt,
+#endif
+};
+
+static struct inet_protosw mtcpsubv6_protosw = {
+	.type		=	SOCK_STREAM,
+	.protocol	=	IPPROTO_MTCPSUB,
+	.prot		=	&mtcpsubv6_prot,
+	.ops		=	&inet6_stream_ops,
+	.capability	=	-1,
+	.no_check	=	0,
+	.flags		=	INET_PROTOSW_PERMANENT |
+				INET_PROTOSW_ICSK,
+};
+
+int __init mtcpv6_init(void)
+{
+	int ret;
+	/* register inet6 protocol */
+	ret = inet6_register_protosw(&mtcpsubv6_protosw);
+	
+	/*Although the protocol is not used as such, it is necessary to register
+	  it, so that slab memory is allocated for it.*/
+	if (ret==0) 
+		ret=proto_register(&mtcpsubv6_prot, 1);
+	return ret;
+}
+
+void mtcpv6_exit(void)
+{
+	inet6_unregister_protosw(&mtcpsubv6_protosw);
+}
diff --git a/net/ipv6/mtcpv6_hashtables.c b/net/ipv6/mtcpv6_hashtables.c
new file mode 100644
index 0000000..68f8cae
--- /dev/null
+++ b/net/ipv6/mtcpv6_hashtables.c
@@ -0,0 +1,166 @@
+/*
+ *	MTCP implementation
+ *
+ *      Specific hashtables for MTCP
+ *      Essentially copied from inet6_hashtables.c
+ *
+ *	Author:
+ *      Sébastien Barré		<sebastien.barre@uclouvain.be>
+ *
+ *
+ *      date : June 09
+ *
+ *
+ *	This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/types.h>
+#include <linux/in6.h>
+
+#include <net/inet_hashtables.h>
+#include <net/inet6_hashtables.h>
+#include <net/mtcp_v6.h>
+
+
+/**
+ * Returns true if the given path index matched with the tp
+ * a match is considered in the following cases:
+ * -either the path_index or the tp path index is zero. (which means
+ *  that the path index is just bypassed). However, if only one of those
+ *  is zero, the other one can only be 1. This is to avoid the following
+ *  scenario. We start a connection, then we stop it. The PM has path indices 
+ *  one and two. Then we start a second connection, with one subsocket and pi 0
+ *  at the beginning. If the other end sends us a SYN on pi 2 before we 
+ *  receive the PM notification, that SYN will arrive on the already
+ *  established socket, which is incorrect. 
+ *  Enforcing equivalence between pi 0 and pi 1 avoids this problem.
+ *  It is still useful, to keep those two numbers (0 and 1) to differentiate
+ *  between a blind segment (path 0), and a segment voluntarily sent to path 1.
+ * -or the path index is equal to that of the tp
+ */
+inline int check_path_index(int path_index, struct tcp_sock *tp)
+{
+	/*1 and 0 can be used interchangeably*/
+	if (path_index==1 || path_index==0)
+		return (tp->path_index==1 || tp->path_index==0);
+	
+	/*Other path indices must be strictly equal*/
+	return (tp->path_index==path_index);
+}
+
+/*
+ * Sockets in TCP_CLOSE state are _always_ taken out of the hash, so
+ * we need not check it for TCP lookups anymore, thanks Alexey. -DaveM
+ *
+ * The sockhash lock must be held as a reader here.
+ */
+struct sock *__mtcpv6_lookup_established(struct net *net,
+					 struct inet_hashinfo *hashinfo,
+					 const struct in6_addr *saddr,
+					 const __be16 sport,
+					 const struct in6_addr *daddr,
+					 const u16 hnum,
+					 const int dif, const int path_index)
+{
+	struct sock *sk;
+	struct tcp_sock *tp;
+	const struct hlist_node *node;
+	const __portpair ports = INET_COMBINED_PORTS(sport, hnum);
+	/* Optimize here for direct hit, only listening connections can
+	 * have wildcards anyways.
+	 */
+	unsigned int hash = inet6_ehashfn(net, daddr, hnum, saddr, sport);
+	struct inet_ehash_bucket *head = inet_ehash_bucket(hashinfo, hash);
+	rwlock_t *lock = inet_ehash_lockp(hashinfo, hash);
+
+	prefetch(head->chain.first);
+	read_lock(lock);
+	sk_for_each(sk, node, &head->chain) {
+		tp=tcp_sk(sk);
+		/* For IPV6 do the cheaper port and family tests first. */
+		if (INET6_MATCH(sk, net, hash, saddr, daddr, ports, dif) &&
+		    check_path_index(path_index,tp))
+			goto hit; /* You sunk my battleship! */
+	}
+	/* Must check for a TIME_WAIT'er before going to listener hash. */
+	sk_for_each(sk, node, &head->twchain) {
+		tp=tcp_sk(sk);
+		if (INET6_TW_MATCH(sk, net, hash, saddr, daddr, ports, dif) &&
+		    check_path_index(path_index,tp))
+			goto hit;
+	}
+	read_unlock(lock);
+	return NULL;
+
+hit:
+	sock_hold(sk);
+	read_unlock(lock);
+	return sk;
+}
+
+struct sock *mtcpv6_lookup_listener(struct net *net,
+				    struct inet_hashinfo *hashinfo, 
+				    const struct in6_addr *daddr,
+				    const unsigned short hnum, const int dif, 
+				    const int path_index)
+{
+	struct sock *sk;
+	struct tcp_sock *tp;
+	const struct hlist_node *node;
+	struct sock *result = NULL;
+	int score, hiscore = 0;
+
+	read_lock(&hashinfo->lhash_lock);
+	sk_for_each(sk, node,
+		    &hashinfo->listening_hash[inet_lhashfn(net, hnum)]) {
+		tp=tcp_sk(sk);
+		
+		if (net_eq(sock_net(sk), net) && inet_sk(sk)->num == hnum &&
+		    sk->sk_family == PF_INET6 && 
+		    check_path_index(path_index,tp)) {
+			const struct ipv6_pinfo *np = inet6_sk(sk);
+			
+			score = 1;
+			if (!ipv6_addr_any(&np->rcv_saddr)) {
+				if (!ipv6_addr_equal(&np->rcv_saddr, daddr))
+					continue;
+				score++;
+			}
+			if (sk->sk_bound_dev_if) {
+				if (sk->sk_bound_dev_if != dif)
+					continue;
+				score++;
+			}
+			if (score == 3) {
+				result = sk;
+				break;
+			}
+			if (score > hiscore) {
+				hiscore = score;
+				result = sk;
+			}
+		}
+	}
+	if (result)
+		sock_hold(result);
+	read_unlock(&hashinfo->lhash_lock);
+	return result;
+}
+
+struct sock *mtcpv6_lookup(struct net *net, struct inet_hashinfo *hashinfo,
+			   const struct in6_addr *saddr, const __be16 sport,
+			   const struct in6_addr *daddr, const __be16 dport,
+			   const int dif, const int path_index)
+{
+	struct sock *sk;
+
+	local_bh_disable();
+	sk = __mtcpv6_lookup(net, hashinfo, saddr, sport, daddr, ntohs(dport), 
+			     dif, path_index);
+	local_bh_enable();
+
+	return sk;
+}
diff --git a/net/ipv6/raw.c b/net/ipv6/raw.c
index 2ba04d4..ae3307d 100644
--- a/net/ipv6/raw.c
+++ b/net/ipv6/raw.c
@@ -52,7 +52,6 @@
 #include <net/mip6.h>
 #endif
 #include <linux/mroute6.h>
-
 #include <net/raw.h>
 #include <net/rawv6.h>
 #include <net/xfrm.h>
diff --git a/net/ipv6/route.c b/net/ipv6/route.c
index 89dc699..44e11a0 100644
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@ -893,6 +893,8 @@ static void ip6_rt_update_pmtu(struct dst_entry *dst, u32 mtu)
 {
 	struct rt6_info *rt6 = (struct rt6_info*)dst;
 
+	printk(KERN_ERR "%s:new mtu:%d\n",__FUNCTION__,mtu);
+
 	if (mtu < dst_mtu(dst) && rt6->rt6i_dst.plen == 128) {
 		rt6->rt6i_flags |= RTF_MODIFIED;
 		if (mtu < IPV6_MIN_MTU) {
@@ -1534,6 +1536,8 @@ void rt6_pmtu_discovery(struct in6_addr *daddr, struct in6_addr *saddr,
 	int allfrag = 0;
 
 	rt = rt6_lookup(net, daddr, saddr, dev->ifindex, 0);
+	printk(KERN_ERR "%s:Received new pmtu:%d\n",__FUNCTION__,pmtu);
+
 	if (rt == NULL)
 		return;
 
@@ -2009,6 +2013,8 @@ void rt6_mtu_change(struct net_device *dev, unsigned mtu)
 		.mtu = mtu,
 	};
 
+	printk(KERN_ERR "%s:new mtu:%d\n",__FUNCTION__,mtu);
+
 	fib6_clean_all(dev_net(dev), rt6_mtu_change_route, 0, &arg);
 }
 
diff --git a/net/ipv6/tcp_ipv6.c b/net/ipv6/tcp_ipv6.c
index b6b356b..28222a3 100644
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@ -59,6 +59,7 @@
 #include <net/timewait_sock.h>
 #include <net/netdma.h>
 #include <net/inet_common.h>
+#include <net/mtcp_v6.h>
 
 #include <asm/uaccess.h>
 
@@ -72,10 +73,10 @@ static void	tcp_v6_send_reset(struct sock *sk, struct sk_buff *skb);
 static void	tcp_v6_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,
 				      struct request_sock *req);
 
-static int	tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);
+int	tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);
 
 static struct inet_connection_sock_af_ops ipv6_mapped;
-static struct inet_connection_sock_af_ops ipv6_specific;
+struct inet_connection_sock_af_ops ipv6_specific;
 #ifdef CONFIG_TCP_MD5SIG
 static struct tcp_sock_af_ops tcp_sock_ipv6_specific;
 static struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific;
@@ -87,7 +88,7 @@ static struct tcp_md5sig_key *tcp_v6_md5_do_lookup(struct sock *sk,
 }
 #endif
 
-static void tcp_v6_hash(struct sock *sk)
+void tcp_v6_hash(struct sock *sk)
 {
 	if (sk->sk_state != TCP_CLOSE) {
 		if (inet_csk(sk)->icsk_af_ops == &ipv6_mapped) {
@@ -116,8 +117,8 @@ static __u32 tcp_v6_init_sequence(struct sk_buff *skb)
 					    tcp_hdr(skb)->source);
 }
 
-static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
-			  int addr_len)
+int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
+		   int addr_len)
 {
 	struct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;
 	struct inet_sock *inet = inet_sk(sk);
@@ -303,6 +304,10 @@ static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
 	if (err)
 		goto late_failure;
 
+#ifdef CONFIG_MTCP
+	mtcp_update_metasocket(sk);
+#endif
+
 	return 0;
 
 late_failure:
@@ -315,7 +320,7 @@ failure:
 }
 
 static void tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,
-		int type, int code, int offset, __be32 info)
+		       int type, int code, int offset, __be32 info)
 {
 	struct ipv6hdr *hdr = (struct ipv6hdr*)skb->data;
 	const struct tcphdr *th = (struct tcphdr *)(skb->data+offset);
@@ -326,8 +331,9 @@ static void tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,
 	__u32 seq;
 	struct net *net = dev_net(skb->dev);
 
-	sk = inet6_lookup(net, &tcp_hashinfo, &hdr->daddr,
-			th->dest, &hdr->saddr, th->source, skb->dev->ifindex);
+	sk = mtcpv6_lookup(net, &tcp_hashinfo, &hdr->daddr,
+			   th->dest, &hdr->saddr, th->source, 
+			   skb->dev->ifindex,skb->path_index);
 
 	if (sk == NULL) {
 		ICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),
@@ -900,7 +906,7 @@ static struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {
 };
 #endif
 
-static struct timewait_sock_ops tcp6_timewait_sock_ops = {
+struct timewait_sock_ops tcp6_timewait_sock_ops = {
 	.twsk_obj_size	= sizeof(struct tcp6_timewait_sock),
 	.twsk_unique	= tcp_twsk_unique,
 	.twsk_destructor= tcp_twsk_destructor,
@@ -1097,9 +1103,11 @@ static struct sock *tcp_v6_hnd_req(struct sock *sk,struct sk_buff *skb)
 	if (req)
 		return tcp_check_req(sk, skb, req, prev);
 
-	nsk = __inet6_lookup_established(sock_net(sk), &tcp_hashinfo,
-			&ipv6_hdr(skb)->saddr, th->source,
-			&ipv6_hdr(skb)->daddr, ntohs(th->dest), inet6_iif(skb));
+	nsk = __mtcpv6_lookup_established(sock_net(sk), &tcp_hashinfo,
+					  &ipv6_hdr(skb)->saddr, th->source,
+					  &ipv6_hdr(skb)->daddr, 
+					  ntohs(th->dest), inet6_iif(skb),
+					  skb->path_index);
 
 	if (nsk) {
 		if (nsk->sk_state != TCP_TIME_WAIT) {
@@ -1166,7 +1174,7 @@ static int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)
 	tmp_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);
 	tmp_opt.user_mss = tp->rx_opt.user_mss;
 
-	tcp_parse_options(skb, &tmp_opt, 0);
+	tcp_parse_options(skb, &tmp_opt, NULL, 0);
 
 	if (want_cookie && !tmp_opt.saw_tstamp)
 		tcp_clear_options(&tmp_opt);
@@ -1454,11 +1462,11 @@ static __sum16 tcp_v6_checksum_init(struct sk_buff *skb)
  * This is because we cannot sleep with the original spinlock
  * held.
  */
-static int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)
+int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)
 {
 	struct ipv6_pinfo *np = inet6_sk(sk);
 	struct tcp_sock *tp;
-	struct sk_buff *opt_skb = NULL;
+	struct sk_buff *opt_skb = NULL;       
 
 	/* Imagine: socket is IPv6. IPv4 packet arrives,
 	   goes to IPv4 receive handler and backlogged.
@@ -1497,19 +1505,23 @@ static int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)
 	   by tcp. Feel free to propose better solution.
 					       --ANK (980728)
 	 */
-	if (np->rxopt.all)
+	if (np->rxopt.all) {
+		printk(KERN_ERR "cloning inc segment\n");
 		opt_skb = skb_clone(skb, GFP_ATOMIC);
+	}
 
 	if (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */
 		TCP_CHECK_TIMER(sk);
+
 		if (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len))
 			goto reset;
+		
 		TCP_CHECK_TIMER(sk);
 		if (opt_skb)
 			goto ipv6_pktoptions;
 		return 0;
 	}
-
+	
 	if (skb->len < tcp_hdrlen(skb) || tcp_checksum_complete(skb))
 		goto csum_err;
 
@@ -1542,7 +1554,7 @@ static int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)
 
 reset:
 	tcp_v6_send_reset(sk, skb);
-discard:
+discard:	
 	if (opt_skb)
 		__kfree_skb(opt_skb);
 	kfree_skb(skb);
@@ -1587,7 +1599,7 @@ static int tcp_v6_rcv(struct sk_buff *skb)
 	struct sock *sk;
 	int ret;
 	struct net *net = dev_net(skb->dev);
-
+	
 	if (skb->pkt_type != PACKET_HOST)
 		goto discard_it;
 
@@ -1599,7 +1611,7 @@ static int tcp_v6_rcv(struct sk_buff *skb)
 	if (!pskb_may_pull(skb, sizeof(struct tcphdr)))
 		goto discard_it;
 
-	th = tcp_hdr(skb);
+	th = tcp_hdr(skb);	
 
 	if (th->doff < sizeof(struct tcphdr)/4)
 		goto bad_packet;
@@ -1614,14 +1626,35 @@ static int tcp_v6_rcv(struct sk_buff *skb)
 	TCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +
 				    skb->len - th->doff*4);
 	TCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);
+#ifdef CONFIG_MTCP
+	/*Init to zero, will be set upon option parsing.*/
+	TCP_SKB_CB(skb)->data_seq = 0;
+	TCP_SKB_CB(skb)->end_data_seq = 0;
+#endif
 	TCP_SKB_CB(skb)->when = 0;
 	TCP_SKB_CB(skb)->flags = ipv6_get_dsfield(ipv6_hdr(skb));
 	TCP_SKB_CB(skb)->sacked = 0;
+	
+#ifdef CONFIG_MTCP_PM
+	/*We must absolutely check for subflow related segments
+	  before the normal sock lookup, because otherwise subflow
+	  segments could be understood as associated to some listening
+	  socket.*/
+
+	/*Is there a pending request sock for this segment ?*/
+	if (mtcp_syn_recv_sock(skb)) return 0;
+	/*Is this a new syn+join ?*/
+	if (th->syn && mtcp_lookup_join(skb)) return 0;
+
+	/*OK, this segment is not related to subflow initiation,
+	  we can proceed to normal lookup*/
+#endif
 
 	sk = __inet6_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest);
+	
 	if (!sk)
 		goto no_tcp_socket;
-
+	
 process:
 	if (sk->sk_state == TCP_TIME_WAIT)
 		goto do_time_wait;
@@ -1645,14 +1678,12 @@ process:
 			ret = tcp_v6_do_rcv(sk, skb);
 		else
 #endif
-		{
 			if (!tcp_prequeue(sk, skb))
 				ret = tcp_v6_do_rcv(sk, skb);
-		}
-	} else
+	} else 
 		sk_add_backlog(sk, skb);
 	bh_unlock_sock(sk);
-
+	
 	sock_put(sk);
 	return ret ? -1 : 0;
 
@@ -1725,7 +1756,7 @@ static int tcp_v6_remember_stamp(struct sock *sk)
 	return 0;
 }
 
-static struct inet_connection_sock_af_ops ipv6_specific = {
+struct inet_connection_sock_af_ops ipv6_specific = {
 	.queue_xmit	   = inet6_csk_xmit,
 	.send_check	   = tcp_v6_send_check,
 	.rebuild_header	   = inet6_sk_rebuild_header,
@@ -1831,12 +1862,22 @@ static int tcp_v6_init_sock(struct sock *sk)
 	sk->sk_sndbuf = sysctl_tcp_wmem[1];
 	sk->sk_rcvbuf = sysctl_tcp_rmem[1];
 
+#ifdef CONFIG_MTCP
+	/*Init the MTCP mpcb*/
+	{
+		struct multipath_pcb *mpcb;		
+		mpcb=mtcp_alloc_mpcb(sk);
+		tp->path_index=0;
+		mtcp_add_sock(mpcb,tp);
+	}
+#endif
+
 	atomic_inc(&tcp_sockets_allocated);
 
 	return 0;
 }
 
-static void tcp_v6_destroy_sock(struct sock *sk)
+void tcp_v6_destroy_sock(struct sock *sk)
 {
 #ifdef CONFIG_TCP_MD5SIG
 	/* Clean up the MD5 key list */
@@ -2020,7 +2061,11 @@ void tcp6_proc_exit(struct net *net)
 struct proto tcpv6_prot = {
 	.name			= "TCPv6",
 	.owner			= THIS_MODULE,
+#ifdef CONFIG_MTCP
+	.close			= mtcp_close,
+#else
 	.close			= tcp_close,
+#endif
 	.connect		= tcp_v6_connect,
 	.disconnect		= tcp_disconnect,
 	.accept			= inet_csk_accept,
@@ -2102,13 +2147,14 @@ int __init tcpv6_init(void)
 	ret = inet6_register_protosw(&tcpv6_protosw);
 	if (ret)
 		goto out_tcpv6_protocol;
-
+	
 	ret = register_pernet_subsys(&tcpv6_net_ops);
 	if (ret)
 		goto out_tcpv6_protosw;
+	
 out:
 	return ret;
-
+	
 out_tcpv6_protocol:
 	inet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);
 out_tcpv6_protosw:
@@ -2120,5 +2166,5 @@ void tcpv6_exit(void)
 {
 	unregister_pernet_subsys(&tcpv6_net_ops);
 	inet6_unregister_protosw(&tcpv6_protosw);
-	inet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);
+	inet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);	
 }
diff --git a/net/ipv6/tcp_probe6.c b/net/ipv6/tcp_probe6.c
new file mode 100644
index 0000000..e95f4b1
--- /dev/null
+++ b/net/ipv6/tcp_probe6.c
@@ -0,0 +1,340 @@
+
+/*
+ * tcpprobe - Observe the TCP flow with kprobes.
+ *
+ * The idea for this came from Werner Almesberger's umlsim
+ * Copyright (C) 2004, Stephen Hemminger <shemminger@osdl.org>
+ *
+ * Adapted from IPv4 to IPv6 by Amine Dhraief.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/kernel.h>
+#include <linux/socket.h>
+#include <linux/tcp.h>
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+#include <linux/ktime.h>
+#include <linux/time.h>
+#include <linux/tcp_probe.h>
+#include <net/net_namespace.h>
+#include <net/ipv6.h>
+#include <net/tcp.h>
+
+MODULE_AUTHOR("Stephen Hemminger <shemminger@linux-foundation.org>");
+MODULE_DESCRIPTION("TCP cwnd snooper");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("1.1");
+
+static int port __read_mostly = 0;
+MODULE_PARM_DESC(port, "Port to match (0=all)");
+module_param(port, int, 0);
+
+static int bufsize __read_mostly = 4096;
+MODULE_PARM_DESC(bufsize, "Log buffer size in packets (4096)");
+module_param(bufsize, int, 0);
+
+static int full __read_mostly = 1;
+MODULE_PARM_DESC(full, "Full log (1=every ack packet received,  0=only cwnd changes)");
+module_param(full, int, 0);
+
+static const char procname[] = "tcpprobe6";
+
+struct tcp_log {
+	ktime_t tstamp;
+	struct in6_addr daddr, saddr;
+	__be16	sport, dport;
+	int     path_index;
+	u16	length;
+	u32	snd_nxt;
+	u32	snd_una;
+	u32	snd_wnd;
+	u32	snd_cwnd;
+	u32	ssthresh;
+	u32	srtt;
+	u32     rcv_nxt;
+	u32     copied_seq;
+	u32     rcv_wnd;
+	u32     rcv_buf;
+	u32     rcv_ssthresh;
+	u32     window_clamp;
+	char    send; /*1 if sending side, 0 if receive*/
+	int     space;
+	u32     rtt_est;
+	u32     in_flight;
+	u32     mss_cache;
+};
+
+static struct {
+	spinlock_t	lock;
+	wait_queue_head_t wait;
+	ktime_t		start;
+	u32		lastcwnd;
+
+	unsigned long	head, tail;
+	struct tcp_log	*log;
+} tcp_probe;
+
+
+static inline int tcp_probe_used(void)
+{
+	return (tcp_probe.head - tcp_probe.tail) % bufsize;
+}
+
+static inline int tcp_probe_avail(void)
+{
+	return bufsize - tcp_probe_used();
+}
+
+/*
+ * Hook inserted to be called before each receive packet.
+ * Note: arguments must match tcp_rcv_established()!
+ */
+static int rcv_established(struct sock *sk, struct sk_buff *skb,
+			   struct tcphdr *th, unsigned len)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	const struct inet_sock *inet = inet_sk(sk);
+ 	const struct ipv6_pinfo *np=inet6_sk(sk);
+
+	/* Only update if port matches */
+	if ((skb->protocol == htons(ETH_P_IPV6)) && (port == 0 || ntohs(inet->dport) == port || ntohs(inet->sport) == port)
+	    && (full || tp->snd_cwnd != tcp_probe.lastcwnd)) {
+		spin_lock(&tcp_probe.lock);
+		/* If log fills, just silently drop */
+		if (tcp_probe_avail() > 1) {
+			struct tcp_log *p = tcp_probe.log + tcp_probe.head;
+
+			p->tstamp = ktime_get();
+			ipv6_addr_copy(&p->saddr,&np->saddr);
+			p->sport = inet->sport;
+			ipv6_addr_copy(&p->daddr,&np->daddr);
+			p->dport = inet->dport;
+			p->path_index = skb->path_index;
+			p->length = skb->len;
+			p->snd_nxt = tp->snd_nxt;
+			p->snd_una = tp->snd_una;
+			p->snd_cwnd = tp->snd_cwnd;
+			p->snd_wnd = tp->snd_wnd;
+			p->ssthresh = tcp_current_ssthresh(sk);
+			p->srtt = tp->srtt >> 3;
+			p->rcv_nxt=tp->rcv_nxt;
+			p->copied_seq=tp->copied_seq;
+			p->rcv_wnd=tp->rcv_wnd;
+			p->rcv_buf=sk->sk_rcvbuf;
+			p->rcv_ssthresh=tp->rcv_ssthresh;
+			p->window_clamp=tp->window_clamp;
+			p->send=0;
+			p->space=tp->rcvq_space.space;
+			p->rtt_est=tp->rcv_rtt_est.rtt;
+			p->in_flight=tp->packets_out;
+			p->mss_cache=tp->mss_cache;
+			
+			tcp_probe.head = (tcp_probe.head + 1) % bufsize;
+		}
+		tcp_probe.lastcwnd = tp->snd_cwnd;
+		spin_unlock(&tcp_probe.lock);
+
+		wake_up(&tcp_probe.wait);
+	}
+
+	return 0;
+}
+
+/*
+ * Hook inserted to be called before each transmit_skb
+ * Note: arguments must match tcp_transmit_skb()!
+ */
+static int transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
+			gfp_t gfp_mask)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	const struct inet_sock *inet = inet_sk(sk);
+ 	const struct ipv6_pinfo *np=inet6_sk(sk);
+
+	/* Only update if port matches */
+	spin_lock_bh(&tcp_probe.lock);
+	/* If log fills, just silently drop */
+	if (tcp_probe_avail() > 1) {
+		struct tcp_log *p = tcp_probe.log + tcp_probe.head;
+		
+		p->tstamp = ktime_get();
+		ipv6_addr_copy(&p->saddr,&np->saddr);
+		p->sport = inet->sport;
+		ipv6_addr_copy(&p->daddr,&np->daddr);
+		p->dport = inet->dport;
+		p->path_index = tp->path_index;
+		p->length = skb->len;
+		p->snd_nxt = tp->snd_nxt;
+		p->snd_una = tp->snd_una;
+		p->snd_cwnd = tp->snd_cwnd;
+		p->snd_wnd = tp->snd_wnd;
+		p->ssthresh = tcp_current_ssthresh(sk);
+		p->srtt = tp->srtt >> 3;
+		p->rcv_nxt=tp->rcv_nxt;
+		p->copied_seq=tp->copied_seq;
+		p->rcv_wnd=tp->rcv_wnd;
+		p->rcv_buf=sk->sk_rcvbuf;
+		p->rcv_ssthresh=tp->rcv_ssthresh;
+		p->window_clamp=tp->window_clamp;
+		p->send=1;
+		p->space=tp->rcvq_space.space;
+		p->rtt_est=tp->rcv_rtt_est.rtt;
+		p->in_flight=tp->packets_out;
+		p->mss_cache=tp->mss_cache;
+		
+		tcp_probe.head = (tcp_probe.head + 1) % bufsize;
+	}
+	tcp_probe.lastcwnd = tp->snd_cwnd;
+	spin_unlock_bh(&tcp_probe.lock);
+	
+	wake_up(&tcp_probe.wait);
+	
+	return 0;
+}
+
+static int tcpprobe_open(struct inode * inode, struct file * file)
+{
+	/* Reset (empty) log */
+	spin_lock_bh(&tcp_probe.lock);
+	tcp_probe.head = tcp_probe.tail = 0;
+	spin_unlock_bh(&tcp_probe.lock);
+
+	return 0;
+}
+
+static int tcpprobe_sprint(char *tbuf, int n)
+{
+	const struct tcp_log *p
+		= tcp_probe.log + tcp_probe.tail % bufsize;
+	struct timespec tv
+		= ktime_to_timespec(ktime_sub(p->tstamp, tcp_probe.start));
+
+	return snprintf(tbuf, n,
+			"%lu.%09lu " NIP6_FMT ":%u " NIP6_FMT ":%u"
+			" %d %d %#x %#x %u %u %u %u %#x %#x %u %u %u %u %d"
+			" %d %u %u %u\n",
+			(unsigned long) tv.tv_sec,
+			(unsigned long) tv.tv_nsec,
+			NIP6(p->saddr), ntohs(p->sport),
+			NIP6(p->daddr), ntohs(p->dport),
+			p->path_index, p->length, p->snd_nxt, p->snd_una,
+			p->snd_cwnd, p->ssthresh, p->snd_wnd, p->srtt,
+			p->rcv_nxt,p->copied_seq,p->rcv_wnd,p->rcv_buf,
+			p->window_clamp,p->rcv_ssthresh, p->send,
+			p->space,p->rtt_est*1000/HZ,p->in_flight,p->mss_cache);
+}
+
+static ssize_t tcpprobe_read(struct file *file, char __user *buf,
+			     size_t len, loff_t *ppos)
+{
+	int error = 0, cnt = 0;
+
+	if (!buf || len < 0)
+		return -EINVAL;
+
+	while (cnt < len) {
+		char tbuf[1024];
+		int width;
+
+		/* Wait for data in buffer */
+		error = wait_event_interruptible(tcp_probe.wait,
+						 tcp_probe_used() > 0);
+		if (error)
+			break;
+
+		spin_lock_bh(&tcp_probe.lock);
+		if (tcp_probe.head == tcp_probe.tail) {
+			/* multiple readers race? */
+			spin_unlock_bh(&tcp_probe.lock);
+			continue;
+		}
+
+		width = tcpprobe_sprint(tbuf, sizeof(tbuf));
+
+		if (width < len)
+			tcp_probe.tail = (tcp_probe.tail + 1) % bufsize;
+
+		spin_unlock_bh(&tcp_probe.lock);
+
+		/* if record greater than space available
+		   return partial buffer (so far) */
+		if (width >= len)
+			break;
+
+		error = copy_to_user(buf + cnt, tbuf, width);
+		if (error)
+			break;
+		cnt += width;
+	}
+
+	return cnt == 0 ? error : cnt;
+}
+
+static const struct file_operations tcpprobe_fops = {
+	.owner	 = THIS_MODULE,
+	.open	 = tcpprobe_open,
+	.read    = tcpprobe_read,
+};
+
+static struct tcpprobe_ops tcpprobe_fcts = {
+	.rcv_established=rcv_established,
+	.transmit_skb=transmit_skb,
+};
+
+static __init int tcpprobe_init(void)
+{
+	int ret = -ENOMEM;
+
+	init_waitqueue_head(&tcp_probe.wait);
+	spin_lock_init(&tcp_probe.lock);
+
+	if (bufsize < 0)
+		return -EINVAL;
+
+	tcp_probe.log = kcalloc(sizeof(struct tcp_log), bufsize, GFP_KERNEL);
+	if (!tcp_probe.log)
+		goto err0;
+
+	if (!proc_net_fops_create(&init_net, procname, S_IRUSR, &tcpprobe_fops))
+		goto err0;
+
+	ret = register_probe(&tcpprobe_fcts,6);
+	if (ret)
+		goto err1;
+
+	pr_info("TCP probe registered (port=%d)\n", port);
+	
+	spin_lock_bh(&tcp_probe.lock);
+        tcp_probe.head = tcp_probe.tail = 0;
+        tcp_probe.start = ktime_get();
+        spin_unlock_bh(&tcp_probe.lock);
+
+	return 0;
+ err1:
+	proc_net_remove(&init_net, procname);
+ err0:
+	kfree(tcp_probe.log);
+	return ret;
+}
+module_init(tcpprobe_init);
+
+static __exit void tcpprobe_exit(void)
+{
+	proc_net_remove(&init_net, procname);
+	unregister_probe(&tcpprobe_fcts,6);
+	kfree(tcp_probe.log);
+}
+module_exit(tcpprobe_exit);
diff --git a/net/ipv6/udp.c b/net/ipv6/udp.c
index 8b48512..bcdaaaa 100644
--- a/net/ipv6/udp.c
+++ b/net/ipv6/udp.c
@@ -903,7 +903,7 @@ out:
 	 * things).  We could add another new stat but at least for now that
 	 * seems like overkill.
 	 */
-	if (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {
+	if (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sock_flags)) {
 		UDP6_INC_STATS_USER(sock_net(sk),
 				UDP_MIB_SNDBUFERRORS, is_udplite);
 	}
diff --git a/net/ipv6/xfrm6_policy.c b/net/ipv6/xfrm6_policy.c
index 08e4cbb..386fc3c 100644
--- a/net/ipv6/xfrm6_policy.c
+++ b/net/ipv6/xfrm6_policy.c
@@ -83,8 +83,12 @@ __xfrm6_find_bundle(struct flowi *fl, struct xfrm_policy *policy)
 		ipv6_addr_prefix(&fl_src_prefix,
 				 &fl->fl6_src,
 				 xdst->u.rt6.rt6i_src.plen);
-		if (ipv6_addr_equal(&xdst->u.rt6.rt6i_dst.addr, &fl_dst_prefix) &&
-		    ipv6_addr_equal(&xdst->u.rt6.rt6i_src.addr, &fl_src_prefix) &&
+		if (ipv6_addr_equal(&xdst->u.rt6.rt6i_dst.addr, &fl_dst_prefix)
+		    &&
+		    ipv6_addr_equal(&xdst->u.rt6.rt6i_src.addr, &fl_src_prefix)
+		    &&
+		    dst->path_index==fl->path_index
+		    &&
 		    xfrm_bundle_ok(policy, xdst, fl, AF_INET6,
 				   (xdst->u.rt6.rt6i_dst.plen != 128 ||
 				    xdst->u.rt6.rt6i_src.plen != 128))) {
@@ -226,6 +230,8 @@ static void xfrm6_update_pmtu(struct dst_entry *dst, u32 mtu)
 	struct xfrm_dst *xdst = (struct xfrm_dst *)dst;
 	struct dst_entry *path = xdst->route;
 
+	printk(KERN_ERR "new mtu:%d\n",mtu);
+
 	path->ops->update_pmtu(path, mtu);
 }
 
diff --git a/net/socket.c b/net/socket.c
index 76ba80a..ccd67f1 100644
--- a/net/socket.c
+++ b/net/socket.c
@@ -1679,6 +1679,8 @@ asmlinkage long sys_send(int fd, void __user *buff, size_t len, unsigned flags)
  *	sender address from kernel to user space.
  */
 
+#include <linux/tcp.h>
+#include <linux/tcp_probe.h>
 asmlinkage long sys_recvfrom(int fd, void __user *ubuf, size_t size,
 			     unsigned flags, struct sockaddr __user *addr,
 			     int __user *addr_len)
@@ -1693,7 +1695,6 @@ asmlinkage long sys_recvfrom(int fd, void __user *ubuf, size_t size,
 	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 	if (!sock)
 		goto out;
-
 	msg.msg_control = NULL;
 	msg.msg_controllen = 0;
 	msg.msg_iovlen = 1;
diff --git a/net/xfrm/xfrm_policy.c b/net/xfrm/xfrm_policy.c
index fb216c9..9f99131 100644
--- a/net/xfrm/xfrm_policy.c
+++ b/net/xfrm/xfrm_policy.c
@@ -119,7 +119,7 @@ static inline struct dst_entry *__xfrm_dst_lookup(int tos,
 static inline struct dst_entry *xfrm_dst_lookup(struct xfrm_state *x, int tos,
 						xfrm_address_t *prev_saddr,
 						xfrm_address_t *prev_daddr,
-						int family)
+						int family, int path_index)
 {
 	xfrm_address_t *saddr = &x->props.saddr;
 	xfrm_address_t *daddr = &x->id.daddr;
@@ -599,6 +599,7 @@ int xfrm_policy_insert(int dir, struct xfrm_policy *policy, int excl)
 		hlist_add_after(newpos, &policy->bydst);
 	else
 		hlist_add_head(&policy->bydst, chain);
+
 	xfrm_pol_hold(policy);
 	xfrm_policy_count[dir]++;
 	atomic_inc(&flow_cache_genid);
@@ -1391,7 +1392,8 @@ static inline int xfrm_fill_dst(struct xfrm_dst *xdst, struct net_device *dev)
 static struct dst_entry *xfrm_bundle_create(struct xfrm_policy *policy,
 					    struct xfrm_state **xfrm, int nx,
 					    struct flowi *fl,
-					    struct dst_entry *dst)
+					    struct dst_entry *dst, 
+					    int path_index)
 {
 	unsigned long now = jiffies;
 	struct net_device *dev;
@@ -1432,19 +1434,19 @@ static struct dst_entry *xfrm_bundle_create(struct xfrm_policy *policy,
 			dst1->flags |= DST_NOHASH;
 		}
 
-		xdst->route = dst;
-		memcpy(&dst1->metrics, &dst->metrics, sizeof(dst->metrics));
-
 		if (xfrm[i]->props.mode != XFRM_MODE_TRANSPORT) {
 			family = xfrm[i]->props.family;
 			dst = xfrm_dst_lookup(xfrm[i], tos, &saddr, &daddr,
-					      family);
+					      family, path_index);
 			err = PTR_ERR(dst);
 			if (IS_ERR(dst))
 				goto put_states;
 		} else
 			dst_hold(dst);
 
+  		xdst->route = dst;
+		memcpy(&dst1->metrics, &dst->metrics, sizeof(dst->metrics));
+
 		dst1->xfrm = xfrm[i];
 		xdst->genid = xfrm[i]->genid;
 
@@ -1472,7 +1474,7 @@ static struct dst_entry *xfrm_bundle_create(struct xfrm_policy *policy,
 	if (!dev)
 		goto free_dst;
 
-	/* Copy neighbout for reachability confirmation */
+	/* Copy neighbour for reachability confirmation */
 	dst0->neighbour = neigh_clone(dst->neighbour);
 
 	xfrm_init_path((struct xfrm_dst *)dst0, dst, nfheader_len);
@@ -1562,6 +1564,14 @@ int __xfrm_lookup(struct dst_entry **dst_p, struct flowi *fl,
 	u32 genid;
 	u16 family;
 	u8 dir = policy_to_flow_dir(XFRM_POLICY_OUT);
+	int path_index=0;
+
+	/*Currently, path indices are used only with TCP*/
+	if (sk && (sk->sk_protocol==IPPROTO_TCP || 
+		   sk->sk_protocol==IPPROTO_MTCPSUB)) {
+		path_index= tcp_sk(sk)->path_index;
+		fl->path_index=path_index;
+	}
 
 restart:
 	genid = atomic_read(&flow_cache_genid);
@@ -1723,7 +1733,8 @@ restart:
 			return 0;
 		}
 
-		dst = xfrm_bundle_create(policy, xfrm, nx, fl, dst_orig);
+		dst = xfrm_bundle_create(policy, xfrm, nx, fl, dst_orig,
+					 path_index);
 		err = PTR_ERR(dst);
 		if (IS_ERR(dst)) {
 			XFRM_INC_STATS(LINUX_MIB_XFRMOUTBUNDLEGENERROR);
@@ -1767,6 +1778,7 @@ restart:
 
 		dst->next = policy->bundles;
 		policy->bundles = dst;
+		dst->path_index=fl->path_index;
 		dst_hold(dst);
 		write_unlock_bh(&policy->lock);
 	}
@@ -2205,7 +2217,6 @@ static void xfrm_init_pmtu(struct dst_entry *dst)
 
 		if (pmtu > route_mtu_cached)
 			pmtu = route_mtu_cached;
-
 		dst->metrics[RTAX_MTU-1] = pmtu;
 	} while ((dst = dst->next));
 }
@@ -2256,6 +2267,8 @@ int xfrm_bundle_ok(struct xfrm_policy *pol, struct xfrm_dst *first,
 
 		mtu = dst_mtu(dst->child);
 		if (xdst->child_mtu_cached != mtu) {
+			printk(KERN_ERR "child_mtu_cached:%d,mtu:%d\n",
+			       xdst->child_mtu_cached,mtu);
 			last = xdst;
 			xdst->child_mtu_cached = mtu;
 		}
@@ -2286,10 +2299,12 @@ int xfrm_bundle_ok(struct xfrm_policy *pol, struct xfrm_dst *first,
 		if (last == first)
 			break;
 
+		printk(KERN_ERR "should not arrive here\n");
 		last = (struct xfrm_dst *)last->u.dst.next;
 		last->child_mtu_cached = mtu;
 	}
 
+	printk(KERN_ERR "%s:mtu set to %d\n", __FUNCTION__,mtu);
 	return 1;
 }
 
diff --git a/net/xfrm/xfrm_state.c b/net/xfrm/xfrm_state.c
index 508337f..3e5c416 100644
--- a/net/xfrm/xfrm_state.c
+++ b/net/xfrm/xfrm_state.c
@@ -1091,24 +1091,24 @@ int xfrm_state_add(struct xfrm_state *x)
 	if (use_spi && x->km.seq) {
 		x1 = __xfrm_find_acq_byseq(x->km.seq);
 		if (x1 && ((x1->id.proto != x->id.proto) ||
-		    xfrm_addr_cmp(&x1->id.daddr, &x->id.daddr, family))) {
+			   xfrm_addr_cmp(&x1->id.daddr, &x->id.daddr, family))) {
 			to_put = x1;
 			x1 = NULL;
 		}
 	}
-
+	
 	if (use_spi && !x1)
 		x1 = __find_acq_core(family, x->props.mode, x->props.reqid,
 				     x->id.proto,
 				     &x->id.daddr, &x->props.saddr, 0);
-
+	
 	__xfrm_state_bump_genids(x);
 	__xfrm_state_insert(x);
 	err = 0;
-
+	
 out:
 	spin_unlock_bh(&xfrm_state_lock);
-
+	
 	if (x1) {
 		xfrm_state_delete(x1);
 		xfrm_state_put(x1);
diff --git a/net/xfrm/xfrm_user.c b/net/xfrm/xfrm_user.c
index a278a6f..bdfbb32 100644
--- a/net/xfrm/xfrm_user.c
+++ b/net/xfrm/xfrm_user.c
@@ -26,6 +26,7 @@
 #include <net/sock.h>
 #include <net/xfrm.h>
 #include <net/netlink.h>
+#include <net/ip6_route.h>
 #include <asm/uaccess.h>
 #if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
 #include <linux/in6.h>
@@ -92,7 +93,7 @@ static inline int verify_sec_ctx_len(struct nlattr **attrs)
 	struct nlattr *rt = attrs[XFRMA_SEC_CTX];
 	struct xfrm_user_sec_ctx *uctx;
 
-	if (!rt)
+	if (!rt) 
 		return 0;
 
 	uctx = nla_data(rt);
@@ -169,7 +170,7 @@ static int verify_newsa_info(struct xfrm_usersa_info *p,
 			goto out;
 		break;
 #endif
-
+		
 	default:
 		goto out;
 	}
@@ -184,7 +185,7 @@ static int verify_newsa_info(struct xfrm_usersa_info *p,
 		goto out;
 	if ((err = verify_sec_ctx_len(attrs)))
 		goto out;
-
+	
 	err = -EINVAL;
 	switch (p->mode) {
 	case XFRM_MODE_TRANSPORT:
@@ -443,7 +444,7 @@ static struct xfrm_state *xfrm_user_state_lookup(struct xfrm_usersa_id *p,
 		x = xfrm_state_lookup(&p->daddr, p->spi, p->proto, p->family);
 	} else {
 		xfrm_address_t *saddr = NULL;
-
+		
 		verify_one_addr(attrs, XFRMA_SRCADDR, &saddr);
 		if (!saddr) {
 			err = -EINVAL;
@@ -1119,6 +1120,7 @@ static int xfrm_add_policy(struct sk_buff *skb, struct nlmsghdr *nlh,
 	u32 sessionid = NETLINK_CB(skb).sessionid;
 	u32 sid = NETLINK_CB(skb).sid;
 
+
 	err = verify_newpolicy_info(p);
 	if (err)
 		return err;
@@ -1138,6 +1140,7 @@ static int xfrm_add_policy(struct sk_buff *skb, struct nlmsghdr *nlh,
 	err = xfrm_policy_insert(p->dir, xp, excl);
 	xfrm_audit_policy_add(xp, err ? 0 : 1, loginuid, sessionid, sid);
 
+
 	if (err) {
 		security_xfrm_policy_free(xp->security);
 		kfree(xp);
@@ -2029,6 +2032,7 @@ static int build_expire(struct sk_buff *skb, struct xfrm_state *x, struct km_eve
 	ue->hard = (c->data.hard != 0) ? 1 : 0;
 
 	return nlmsg_end(skb, nlh);
+
 }
 
 static int xfrm_exp_state_notify(struct xfrm_state *x, struct km_event *c)
